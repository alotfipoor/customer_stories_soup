[
    {
        "id": 1,
        "url": "https://www.knime.com/success-story/how-knime-helped-alexion-shorten-time-disease-diagnosis-accelerate-time-treatment",
        "title": "How KNIME helped Alexion shorten time to disease diagnosis & accelerate time to treatment | KNIME",
        "company": "Alexion",
        "content": "The impact of lacking a central, trusted source of rare disease definitions\nWhile there is more medical data available than ever before, much of it remains untapped, disorganized, and unusable. For instance, there is no single authority providing a comprehensive list of rare disease definitions. In fact, there isn\u2019t even a consensus on the number of rare diseases that exist. Depending on the source consulted, the answer varies between 7,000 and 9,000 rare conditions. Meanwhile, Alexion\u2019s dataset contains over 12,000 conditions with 7,230 from Orphanet \u2013 and many more are in the process of being defined.\nWithout a trusted and accurate source of rare disease definitions to refer to, it is difficult for physicians to correctly and quickly identify a diagnosis. Most rare diseases lead to considerable disability and early mortality, so a rapid and accurate diagnosis is crucial. Even after receiving a diagnosis, close to 90% of people with a rare disease do not have an approved treatment or therapy. The grim reality is that, of the thousands of rare conditions, only 161 have approved drugs available. The lack of a recognized inventory of rare disease definitions leads to under-representation in healthcare planning and resource allocation and prevents clinical research from being carried out.\nWhat\u2019s more, the lack of a rare disease master dataset also has an impact on medical insurance coverage. The healthcare system in the United States uses ICD-10 codes for billing. There are barely 128 ICD-10 codes for over 7,000 rare conditions, which means that only a minuscule number of rare diseases are recognized by insurance providers and get coverage.\nUltimately, a rare disease definition is the backbone to which all clinically relevant information is mapped, and a lack of it leads to delays in every aspect of disease management from drug discovery, clinical trial design, and drug approval to diagnosis, treatment plan, and insurance coverage. Alexion wanted to address these challenges by building a rare disease data master to shorten the diagnostic journey and time to initiate treatment for those living with rare conditions.\nIngesting constantly changing medical data\nAlexion adopted KNIME as its core analytics tool to accelerate the rare disease diagnostic odyssey. With KNIME, they were able to easily ingest data from countless biomedical knowledge bases such as\u00a0Orphanet,\u00a0PubMed,\u00a0DrugBank,\u00a0Reactome, and\u00a0GTex\u00a0and get it into the right shape. Before KNIME, if Orphanet or any other data source changed its data format, the team at Alexion would end up spending up to three weeks modifying their Java code, indices, and APIs to accommodate the new format. With KNIME, they were able to seamlessly integrate and transform the data in one uniform, visual environment in no time, regardless of any changes within the data source.\nOnce the transformed data was loaded, KNIME enabled Alexion to build all-inclusive, master datasets such as the disease master, drug master, gene master, and trial master that helped close major gaps in rare disease information.\nAccurate and actionable rare disease definition dataset\nThis data was sent for validation to clinicians and subject matter experts through an interactive dashboard on a KNIME-built data app. These stakeholders - typically PhDs with over 20 years of experience - often insisted on seeing the data lineage and the business logic that was followed for data mapping before they provided feedback. KNIME\u2019s ability to automatically document each step of the data wrangling process enabled Alexion to provide stakeholders the complete visibility they needed into how data pipelines transitioned from raw data all the way to analysis. The feedback from clinicians has led to the creation of a master dataset that is 80% actionable and its quality continues to improve with time.\nShortened rare disease diagnostic journey\nToday, this rare disease data master serves as an accurate and ready reference tool for physicians to deliver a precise diagnosis quickly. In some cases, the diagnostic time has been brought down to a few hours from several years by matching against the rare disease master.\nThe rare disease data master also equips physicians with the information they need to provide early and effective treatment for patients after diagnosis. It reduces dependence on unreliable Google searches and addresses the clinical knowledge gap.\nAdditionally, the rare disease data master forms the center of a flywheel of patient-centric communities around genetics, symptoms, and treatments that work, and it holds the potential to fast-track discovery and approval of new therapies.\nImproved rare disease recognition and medical coverage\nA single, trusted inventory of rare disease data bolsters the chances of higher rare disease representation in healthcare coding systems such as ICD. This would lead to increased interest in clinical research from the healthcare community as well as the much-needed recognition of rare conditions by insurance providers.\nWhy KNIME?\nKNIME\u2019s open-source nature was a key factor in it becoming the tool of choice for Alexion. Multiple research institutes that Alexion partners with, were able to start using KNIME without any barriers on scaling. The ability to share complex data with clinicians and subject matter experts through an easy-to-use, interactive data app coupled with the ability to track data lineage was invaluable for Alexion.With KNIME, Alexion and its partners were able to work on the same analytical pipeline while executing it independently on their own datasets. In other words, it helped them bring the analytics to the data and not the other way round.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 2,
        "url": "https://www.knime.com/success-story/how-knime-helped-alexion-shorten-time-disease-diagnosis-accelerate-time-treatment",
        "title": "How KNIME helped Alexion shorten time to disease diagnosis & accelerate time to treatment | KNIME",
        "company": "Alexion",
        "content": "The impact of lacking a central, trusted source of rare disease definitions\nWhile there is more medical data available than ever before, much of it remains untapped, disorganized, and unusable. For instance, there is no single authority providing a comprehensive list of rare disease definitions. In fact, there isn\u2019t even a consensus on the number of rare diseases that exist. Depending on the source consulted, the answer varies between 7,000 and 9,000 rare conditions. Meanwhile, Alexion\u2019s dataset contains over 12,000 conditions with 7,230 from Orphanet \u2013 and many more are in the process of being defined.\nWithout a trusted and accurate source of rare disease definitions to refer to, it is difficult for physicians to correctly and quickly identify a diagnosis. Most rare diseases lead to considerable disability and early mortality, so a rapid and accurate diagnosis is crucial. Even after receiving a diagnosis, close to 90% of people with a rare disease do not have an approved treatment or therapy. The grim reality is that, of the thousands of rare conditions, only 161 have approved drugs available. The lack of a recognized inventory of rare disease definitions leads to under-representation in healthcare planning and resource allocation and prevents clinical research from being carried out.\nWhat\u2019s more, the lack of a rare disease master dataset also has an impact on medical insurance coverage. The healthcare system in the United States uses ICD-10 codes for billing. There are barely 128 ICD-10 codes for over 7,000 rare conditions, which means that only a minuscule number of rare diseases are recognized by insurance providers and get coverage.\nUltimately, a rare disease definition is the backbone to which all clinically relevant information is mapped, and a lack of it leads to delays in every aspect of disease management from drug discovery, clinical trial design, and drug approval to diagnosis, treatment plan, and insurance coverage. Alexion wanted to address these challenges by building a rare disease data master to shorten the diagnostic journey and time to initiate treatment for those living with rare conditions.\nIngesting constantly changing medical data\nAlexion adopted KNIME as its core analytics tool to accelerate the rare disease diagnostic odyssey. With KNIME, they were able to easily ingest data from countless biomedical knowledge bases such as\u00a0Orphanet,\u00a0PubMed,\u00a0DrugBank,\u00a0Reactome, and\u00a0GTex\u00a0and get it into the right shape. Before KNIME, if Orphanet or any other data source changed its data format, the team at Alexion would end up spending up to three weeks modifying their Java code, indices, and APIs to accommodate the new format. With KNIME, they were able to seamlessly integrate and transform the data in one uniform, visual environment in no time, regardless of any changes within the data source.\nOnce the transformed data was loaded, KNIME enabled Alexion to build all-inclusive, master datasets such as the disease master, drug master, gene master, and trial master that helped close major gaps in rare disease information.\nAccurate and actionable rare disease definition dataset\nThis data was sent for validation to clinicians and subject matter experts through an interactive dashboard on a KNIME-built data app. These stakeholders - typically PhDs with over 20 years of experience - often insisted on seeing the data lineage and the business logic that was followed for data mapping before they provided feedback. KNIME\u2019s ability to automatically document each step of the data wrangling process enabled Alexion to provide stakeholders the complete visibility they needed into how data pipelines transitioned from raw data all the way to analysis. The feedback from clinicians has led to the creation of a master dataset that is 80% actionable and its quality continues to improve with time.\nShortened rare disease diagnostic journey\nToday, this rare disease data master serves as an accurate and ready reference tool for physicians to deliver a precise diagnosis quickly. In some cases, the diagnostic time has been brought down to a few hours from several years by matching against the rare disease master.\nThe rare disease data master also equips physicians with the information they need to provide early and effective treatment for patients after diagnosis. It reduces dependence on unreliable Google searches and addresses the clinical knowledge gap.\nAdditionally, the rare disease data master forms the center of a flywheel of patient-centric communities around genetics, symptoms, and treatments that work, and it holds the potential to fast-track discovery and approval of new therapies.\nImproved rare disease recognition and medical coverage\nA single, trusted inventory of rare disease data bolsters the chances of higher rare disease representation in healthcare coding systems such as ICD. This would lead to increased interest in clinical research from the healthcare community as well as the much-needed recognition of rare conditions by insurance providers.\nWhy KNIME?\nKNIME\u2019s open-source nature was a key factor in it becoming the tool of choice for Alexion. Multiple research institutes that Alexion partners with, were able to start using KNIME without any barriers on scaling. The ability to share complex data with clinicians and subject matter experts through an easy-to-use, interactive data app coupled with the ability to track data lineage was invaluable for Alexion.With KNIME, Alexion and its partners were able to work on the same analytical pipeline while executing it independently on their own datasets. In other words, it helped them bring the analytics to the data and not the other way round.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 3,
        "url": "https://www.knime.com/success-story/how-allente-built-recommendation-engine-deliver-personalized-content",
        "title": "How Allente built a recommendation engine to deliver personalized content | KNIME",
        "company": "Allente",
        "content": "Identifying users' viewing preferences\nAllente knew that to achieve this as fast as possible, they would need a data science platform that offered simplicity of setup and use while delivering the analytic depth to explore data, build models, and deploy them within the same platform.\nAs an end-to-end data science platform, KNIME perfectly met these requirements. It was an easy decision for\u00a0Allente\u00a0to embrace KNIME as its central data science platform to deliver highly relevant, machine learning-powered recommendations to its viewers.\nTogether with\u00a0Redfield, a KNIME partner, Allente set out to build a recommendation engine using KNIME. The first step in building the recommendation engine was to connect to Allente\u2019s main data sources in the cloud and on-premises and make sense of the troves of data they held such as event data, video-on-demand catalog, and sample tables from its legacy Oracle database. With KNIME, they built an automated data pipeline that let them connect to these varied data sources, get the data into the right shape, and categorize it into key customer segments easily in one intuitive, visual workflow environment.\nOnce the key customer segments were defined, Redfield and Allente built and deployed the content recommendation model in the same visual environment offered by KNIME. This involved having the flexibility to code customer segments using\u00a0Python, optimizing and refining them to improve the accuracy of recommendations, and finally, testing and production, all without switching tools.\nThe model can predict content that viewers might find engaging, even for new users who do not have a viewing history. Allente is now able to deliver extremely relevant recommendations with the added upshot of improving content distribution efficiency. With KNIME, the team at Allente can easily tune and adapt the model when new consumption data surfaces, ensuring that recommendations remain relevant in the future as well. What\u2019s more, Allente can anticipate customer behavior and predict demand such as which shows might be popular with which customer segments a couple of years from now.\n\nReducing churn and improving LTV\nKNIME\u2019s low-code capabilities have also made machine learning accessible to non-technical teams at Allente. For instance, the customer service team at Allente uses KNIME to predict the likelihood of customers churning and lowers it with targeted retention measures using behavioral insights. They also carry out customer lifetime value analysis to understand return on investment and calculate expected revenue. Additionally, they are able to improve the conversion rate of their upselling activities with personalized offers.\nKNIME has accelerated the adoption of a data-driven culture and data-driven decisions at Allente. One of the biggest advantages of using KNIME has been the speed and flexibility of both ETL and spinning up models. \nIn the words of an executive at Allente, \u201cFor us it has been key to have one tool that could do a little bit of everything. Easy to use \u2013 yet advanced and powerful. Having one hub where we can do both data-crunching, machine learning models and automated pipelines in a simple manner has transformed our life.\u201d\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 4,
        "url": "https://www.knime.com/success-story/how-allente-built-recommendation-engine-deliver-personalized-content",
        "title": "How Allente built a recommendation engine to deliver personalized content | KNIME",
        "company": "Allente",
        "content": "Identifying users' viewing preferences\nAllente knew that to achieve this as fast as possible, they would need a data science platform that offered simplicity of setup and use while delivering the analytic depth to explore data, build models, and deploy them within the same platform.\nAs an end-to-end data science platform, KNIME perfectly met these requirements. It was an easy decision for\u00a0Allente\u00a0to embrace KNIME as its central data science platform to deliver highly relevant, machine learning-powered recommendations to its viewers.\nTogether with\u00a0Redfield, a KNIME partner, Allente set out to build a recommendation engine using KNIME. The first step in building the recommendation engine was to connect to Allente\u2019s main data sources in the cloud and on-premises and make sense of the troves of data they held such as event data, video-on-demand catalog, and sample tables from its legacy Oracle database. With KNIME, they built an automated data pipeline that let them connect to these varied data sources, get the data into the right shape, and categorize it into key customer segments easily in one intuitive, visual workflow environment.\nOnce the key customer segments were defined, Redfield and Allente built and deployed the content recommendation model in the same visual environment offered by KNIME. This involved having the flexibility to code customer segments using\u00a0Python, optimizing and refining them to improve the accuracy of recommendations, and finally, testing and production, all without switching tools.\nThe model can predict content that viewers might find engaging, even for new users who do not have a viewing history. Allente is now able to deliver extremely relevant recommendations with the added upshot of improving content distribution efficiency. With KNIME, the team at Allente can easily tune and adapt the model when new consumption data surfaces, ensuring that recommendations remain relevant in the future as well. What\u2019s more, Allente can anticipate customer behavior and predict demand such as which shows might be popular with which customer segments a couple of years from now.\n\nReducing churn and improving LTV\nKNIME\u2019s low-code capabilities have also made machine learning accessible to non-technical teams at Allente. For instance, the customer service team at Allente uses KNIME to predict the likelihood of customers churning and lowers it with targeted retention measures using behavioral insights. They also carry out customer lifetime value analysis to understand return on investment and calculate expected revenue. Additionally, they are able to improve the conversion rate of their upselling activities with personalized offers.\nKNIME has accelerated the adoption of a data-driven culture and data-driven decisions at Allente. One of the biggest advantages of using KNIME has been the speed and flexibility of both ETL and spinning up models. \nIn the words of an executive at Allente, \u201cFor us it has been key to have one tool that could do a little bit of everything. Easy to use \u2013 yet advanced and powerful. Having one hub where we can do both data-crunching, machine learning models and automated pipelines in a simple manner has transformed our life.\u201d\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 5,
        "url": "https://www.knime.com/success-story/how-anadolu-sigorta-improved-their-fraud-detection-rate-51-knime",
        "title": "How Anadolu Sigorta improved their fraud detection rate by 51% with KNIME | KNIME",
        "company": "Anadolu Sigorta",
        "content": "The growing need for better fraud detection\nFraud is becoming more prevalent across many different industries today, taking place in insurance more than others. Due to the large number of services included in different policy coverages, detecting fraud has become more challenging (and more necessary) for Anadolu Sigorta and others in the business.\u00a0\u00a0\u00a0\nInsurance fraud can happen in two ways: a report of false or deceptive accident conditions by the policyholder themself or an organized fraudster submitting false reports using the car owner\u2019s information (third-party offense). Anadolu Sigorta processes approximately 3,500 of these transactions a day. A transaction could be a net new claim or claim that needs reprocessing. Each claim must go through several phases or \u201ctouch points\u201d (a very complex and time-consuming process) and might need to be processed more than once.\nFraudulent claims that relate to property damage or personal injury are also extremely difficult to prove, take a lot of time and research, and could be identified as a false positive. Detecting an organized fraud ring in motor insurance is also done by manual research and can take two to three months to fully investigate.\u00a0\nDue to the large amount of services included in insurance coverage, the complexity of processing a claim, and trying to prove the claim, Anadolu Sigorta needed an analytics approach that could streamline, expedite, and simplify the process.\n\nBuilding a real-time, tailored fraud detection platform from the ground up\nBefore KNIME, Anadolu Sigorta was using a third-party solution to detect and monitor fraud. This specific tool was costly, difficult to maintain, and couldn\u2019t handle in-house updates or easily implement new features. Due to the flexibility of KNIME Analytics Platform, the team was able to build their own in-house fraud detection solution (and entire business process) called The SOBE Platform.\nThe input data Anadolu Sigorta uses to analyze claims comes from the company's source systems, like their internal claim processing system, and external sources like accident report files and data from the Insurance Information and Monitoring Center (over 20 operational systems in total). In the first phase of the project, they used KNIME to collect, transform, organize, and stabilize this data.\u00a0\nIn addition to data preparation, KNIME Analytics Platform is also used to assist with machine learning and social network analysis (SNA). Using KNIME, the data team was able to quickly build and implement two machine learning models: one to detect and predict fraudulent claim files and another to prioritize the files based on the probability of most risky to least. Prioritizing the claims by risk probability meant business units could spend less time on false positives or low risk claims and instead focus time catching the fraudulent ones.\u00a0\nThe company also used KNIME to build a rule engine for the next step in the claim review process. The workflow includes experience-based business standards. The platform assigns the claim a score based on the results of each stage (workflow), and the results are returned to the source system via the web service configuration. Results for each claim are provided in five to 10 seconds, and they expect it to happen even faster with future integrations.\u00a0\nBy moving data analysis and audit work to KNIME, they eliminated errors and saved time on analysis \u2014 adding value to the company. What\u2019s more, costs related to maintenance, development, and subscriptions have decreased because they can now perform internal health checks themself without any vendor dependency.\nEssentially, KNIME was used to build The SOBE Platform from the ground up. They capitalized on the flexibility of KNIME Analytics Platform to build automated workflows that could process claim files instantaneously and forward them for fraud inquiry during any point in the claim review lifecycle. The SOBE Platform calculates claim file scores in real time, allowing business units to investigate and detect fraudulent activity in real time too. Beyond fraud detection, Anadolu Sigorta was also able to use KNIME to create insurance renewal probability workflows, price optimization analysis, and a customer analytics platform.\nSaving 146 million in payments from fraudulent claims\nAnadolu Sigorta can process claims and detect fraud faster and easier since implementing KNIME. They\u2019ve achieved a 51% greater detection rate and a 54% increase in financial gain by preventing approximately 146M Turkish Liras in payments from fraudulent claims. What\u2019s more, false positives have decreased by 31% and implementation and maintenance is done in house, costs less, and is much easier.\u00a0\nThe SNA module significantly improved the organized fraud detection processes and reduced the effort and identification period from three to four months down to only two hours. What\u2019s more, they were able to successfully discover eight new organized crime rings via this module. Their fraud platform is now a one-of-a-kind when it comes to configurations and structure compared to other tools on the market.\u00a0\nIn addition to operational efficiency gains, Anadolu Sigorta was able to train business users on data science applications using KNIME. Because KNIME doesn\u2019t require any coding, non-experts in data can create and run their own processes to solve technology issues. As an organization, Anadolu Sigorta has upskilled over 200 people in IT and other business units on how to apply data science.\n\n\nWhy KNIME\nAnadolu Sigorta chose KNIME software due to its ease and speed of implementation. Building models with KNIME can be done very quickly, and their team can use existing nodes already built and available via the KNIME Hub. The team also loves KNIME for auditability and transparency of the process due to its ability to log steps while building a workflow. KNIME also integrates with different programming languages and algorithms that the team uses, like Python, R, Java Script, etc. Lastly, they like that end users can make transactions through a data app and change, alter, or define new rules to that app, if needed.\u00a0\nKNIME was chosen in terms of business considerations because of its simplicity. The API integration was easy to implement and the AI features for machine learning models were easily explainable to others. Both of these features allow the team to manage and trace outcomes accordingly.\n\u21d2\u00a0Download this Innovation Note as a PDF\n\nInforA has been Anadolu Sigorta's Solution Partner for KNIME technologies and data science since 2018. During this time, InforA's certified trainers have provided many qualified trainings on KNIME and data science to Anadolu Sigorta teams. In the last few years, these trainings have been systematically conducted as part of Anadolu Sigorta's Citizen Data Scientist Training Program. InforA's expert consultants also provide consulting support for Anadolu Sigorta's data science projects using KNIME.\u00a0\nTo contact\u00a0InforA,\u00a0click here.\n"
    },
    {
        "id": 6,
        "url": "https://www.knime.com/success-story/how-anadolu-sigorta-improved-their-fraud-detection-rate-51-knime",
        "title": "How Anadolu Sigorta improved their fraud detection rate by 51% with KNIME | KNIME",
        "company": "Anadolu Sigorta",
        "content": "The growing need for better fraud detection\nFraud is becoming more prevalent across many different industries today, taking place in insurance more than others. Due to the large number of services included in different policy coverages, detecting fraud has become more challenging (and more necessary) for Anadolu Sigorta and others in the business.\u00a0\u00a0\u00a0\nInsurance fraud can happen in two ways: a report of false or deceptive accident conditions by the policyholder themself or an organized fraudster submitting false reports using the car owner\u2019s information (third-party offense). Anadolu Sigorta processes approximately 3,500 of these transactions a day. A transaction could be a net new claim or claim that needs reprocessing. Each claim must go through several phases or \u201ctouch points\u201d (a very complex and time-consuming process) and might need to be processed more than once.\nFraudulent claims that relate to property damage or personal injury are also extremely difficult to prove, take a lot of time and research, and could be identified as a false positive. Detecting an organized fraud ring in motor insurance is also done by manual research and can take two to three months to fully investigate.\u00a0\nDue to the large amount of services included in insurance coverage, the complexity of processing a claim, and trying to prove the claim, Anadolu Sigorta needed an analytics approach that could streamline, expedite, and simplify the process.\n\nBuilding a real-time, tailored fraud detection platform from the ground up\nBefore KNIME, Anadolu Sigorta was using a third-party solution to detect and monitor fraud. This specific tool was costly, difficult to maintain, and couldn\u2019t handle in-house updates or easily implement new features. Due to the flexibility of KNIME Analytics Platform, the team was able to build their own in-house fraud detection solution (and entire business process) called The SOBE Platform.\nThe input data Anadolu Sigorta uses to analyze claims comes from the company's source systems, like their internal claim processing system, and external sources like accident report files and data from the Insurance Information and Monitoring Center (over 20 operational systems in total). In the first phase of the project, they used KNIME to collect, transform, organize, and stabilize this data.\u00a0\nIn addition to data preparation, KNIME Analytics Platform is also used to assist with machine learning and social network analysis (SNA). Using KNIME, the data team was able to quickly build and implement two machine learning models: one to detect and predict fraudulent claim files and another to prioritize the files based on the probability of most risky to least. Prioritizing the claims by risk probability meant business units could spend less time on false positives or low risk claims and instead focus time catching the fraudulent ones.\u00a0\nThe company also used KNIME to build a rule engine for the next step in the claim review process. The workflow includes experience-based business standards. The platform assigns the claim a score based on the results of each stage (workflow), and the results are returned to the source system via the web service configuration. Results for each claim are provided in five to 10 seconds, and they expect it to happen even faster with future integrations.\u00a0\nBy moving data analysis and audit work to KNIME, they eliminated errors and saved time on analysis \u2014 adding value to the company. What\u2019s more, costs related to maintenance, development, and subscriptions have decreased because they can now perform internal health checks themself without any vendor dependency.\nEssentially, KNIME was used to build The SOBE Platform from the ground up. They capitalized on the flexibility of KNIME Analytics Platform to build automated workflows that could process claim files instantaneously and forward them for fraud inquiry during any point in the claim review lifecycle. The SOBE Platform calculates claim file scores in real time, allowing business units to investigate and detect fraudulent activity in real time too. Beyond fraud detection, Anadolu Sigorta was also able to use KNIME to create insurance renewal probability workflows, price optimization analysis, and a customer analytics platform.\nSaving 146 million in payments from fraudulent claims\nAnadolu Sigorta can process claims and detect fraud faster and easier since implementing KNIME. They\u2019ve achieved a 51% greater detection rate and a 54% increase in financial gain by preventing approximately 146M Turkish Liras in payments from fraudulent claims. What\u2019s more, false positives have decreased by 31% and implementation and maintenance is done in house, costs less, and is much easier.\u00a0\nThe SNA module significantly improved the organized fraud detection processes and reduced the effort and identification period from three to four months down to only two hours. What\u2019s more, they were able to successfully discover eight new organized crime rings via this module. Their fraud platform is now a one-of-a-kind when it comes to configurations and structure compared to other tools on the market.\u00a0\nIn addition to operational efficiency gains, Anadolu Sigorta was able to train business users on data science applications using KNIME. Because KNIME doesn\u2019t require any coding, non-experts in data can create and run their own processes to solve technology issues. As an organization, Anadolu Sigorta has upskilled over 200 people in IT and other business units on how to apply data science.\n\n\nWhy KNIME\nAnadolu Sigorta chose KNIME software due to its ease and speed of implementation. Building models with KNIME can be done very quickly, and their team can use existing nodes already built and available via the KNIME Hub. The team also loves KNIME for auditability and transparency of the process due to its ability to log steps while building a workflow. KNIME also integrates with different programming languages and algorithms that the team uses, like Python, R, Java Script, etc. Lastly, they like that end users can make transactions through a data app and change, alter, or define new rules to that app, if needed.\u00a0\nKNIME was chosen in terms of business considerations because of its simplicity. The API integration was easy to implement and the AI features for machine learning models were easily explainable to others. Both of these features allow the team to manage and trace outcomes accordingly.\n\u21d2\u00a0Download this Innovation Note as a PDF\n\nInforA has been Anadolu Sigorta's Solution Partner for KNIME technologies and data science since 2018. During this time, InforA's certified trainers have provided many qualified trainings on KNIME and data science to Anadolu Sigorta teams. In the last few years, these trainings have been systematically conducted as part of Anadolu Sigorta's Citizen Data Scientist Training Program. InforA's expert consultants also provide consulting support for Anadolu Sigorta's data science projects using KNIME.\u00a0\nTo contact\u00a0InforA,\u00a0click here.\n"
    },
    {
        "id": 7,
        "url": "https://www.knime.com/success-story/how-bgis-saved-400k-annually-automating-work-order-parsing-knime",
        "title": "How BGIS saved $400k annually by automating work order parsing with KNIME | KNIME",
        "company": "BGIS",
        "content": "Eliminating the need to manually read through 30,000 work order descriptions\nIn order to understand this information, the traditional approach would have been for someone to manually read through work order problem descriptions \u2013 30,000 to be precise \u2013 and subjectively categorize them. Furthermore, not all sites participated in the retrofit project, and as the project spanned multiple years, sites were not all retrofit at the same time. The typical approach of analyzing such a large volume of data would be to aggregate information at the client level, which would result in any savings benefits at test sites being diluted by control sites.\nIn a nutshell: while the data existed in the database, extracting information from large quantities of work orders, and the corresponding textual fields was complex. It simply wasn\u2019t as easy to prove retrofit savings as it may have appeared. The client was relying on a detailed analysis to help inform future lighting benefits, so the analysis had to be spot on to ensure a correct decision.\nUsing data science to deep dive into topics and ensure accuracy\nThe first step in defining an objective and efficient way to quantify savings was to identify a baseline: when did the retrofit happen, and which sites were retrofit? Looking at the costs on either side of the baseline for test sites, for both retrofit sites and non-retrofit sites, it\u2019s possible to identify the impact of the retrofit.\nProving that the drop was in fact driven by the fluorescent tube light change from type A1 to A2 was the next challenge - as that level of detail is not kept as a tabular record and is difficult to extract. The solution for this was through application of data science. Topic modeling, an unsupervised natural language processing (NLP) technique, was used to read through all work orders\u2019 descriptions and resolutions - to understand what issue occurred, and what work was performed at the site. This technique categorized the service calls in an objective fashion, providing statistics to deep dive into the topics to ensure accuracy. In one case, topic modelling detected a category of work orders (\u201cceil, height, standard, fluoresce\u201d) where service calls had been initiated to change a fluorescent tube light at ceiling height. This was clearly an activity which was within the scope of the retrofit project objective i.e. the project aimed to reduce such types of work orders. Several other in-scope themes of service calls were also identified which occurred prior to the retrofit.\nTopic modelling was conducted in both the pre-retrofit and post-retrofit phases to identify (1) the types and counts of work orders created, (2) whether the underlying issues were those which the retrofit was designed to address, and (3) the associated costs within each type of topic.\nThe issues that the retrofit was designed to address \u2013 like in the example above \u2013 reduced dramatically in quantity and in the overall priority post-retrofit. When compared to non-retrofit sites, it became further apparent that the retrofit resulted in cost reductions. Topic modelling allowed BGIS to attribute these savings to the particular type of light bulb that was replaced under the retrofit.\nResults\nAt a high level, the lighting retrofit project was budgeted at approximately $4M. The savings were a combination of reductions in both electricity consumption and maintenance and repair costs (M & R work orders). Energy savings were measurable from the bills, while the M & R savings required the topic modeling approach. Therefore the savings just on M & R work orders of $420K (annualized) is substantial as a proportion of the overall project budget and helped to justify the overall project costs. Specific results were:\n$420K worth of annual cost savings$35K reduction in average monthly work order costsPotential for even more savingsAs a service provider, this analysis demonstrated that savings could potentially have been higher had all sites been retrofit \u2013 the sites which did not go through the retrofit in fact increased in cost over time over the baseline. Additionally, future retrofits may be condensed over shorter time frames to reap cost avoidance benefits, leading to stronger business cases and savings for clients. From an analysis perspective, modern technology can quickly address common business problems in an objective, transparent, and repeatable fashion. A risk with analyses conducted at aggregate levels is that such analyses can easily provide an incorrect direction (Simpson\u2019s Paradox); in this case, had an aggregated method been used, it could have incorrectly led to an understatement of the savings delivered for the client.\nWhy KNIME?\nA review of independent research papers identified KNIME as a Leader in Gartner\u2019s 2018 Magic Quadrant for Data Science and Machine Learning Platforms: a position KNIME had retained for four years prior. Additionally, the total cost of ownership for KNIME was dramatically lower than other software providers in the same quadrant.\nAs brand new tool to the organization,\u00a0KNIME Analytics Platform\u00a0was simple to learn thanks to an extensive example library (KNIME Hub), several\u00a0free and paid online courses, a buzzing online community on\u00a0KNIME Forum\u00a0(as is typical with many open source tools), and a responsive support team. A key reason for selecting KNIME was the no-fee,\u00a0one-click download. Other data science tools were considered, but the high licensing fees quickly made the total cost of ownership unpalatable. Also, because KNIME doesn\u2019t work in competition with existing tools, but rather alongside them, it provided peace of mind that the tools the business is familiar with could still be used \u2013 including\u00a0Access,\u00a0SQL\u00a0and\u00a0Tableau.\nGetting started with KNIME\u00a0was also very easy \u2013 thanks to all the free resources available online. Paras Gupta, Director, BI & Advanced Analytics at BGIS went from\u00a0\"having zero experience to being an advanced user in under two weeks.\u201d\nFurthermore, in this case specifically, the client was able to go back and justify further business cases - helping BGIS to prove value and to continue delivering value to clients.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 8,
        "url": "https://www.knime.com/success-story/how-bgis-saved-400k-annually-automating-work-order-parsing-knime",
        "title": "How BGIS saved $400k annually by automating work order parsing with KNIME | KNIME",
        "company": "BGIS",
        "content": "Eliminating the need to manually read through 30,000 work order descriptions\nIn order to understand this information, the traditional approach would have been for someone to manually read through work order problem descriptions \u2013 30,000 to be precise \u2013 and subjectively categorize them. Furthermore, not all sites participated in the retrofit project, and as the project spanned multiple years, sites were not all retrofit at the same time. The typical approach of analyzing such a large volume of data would be to aggregate information at the client level, which would result in any savings benefits at test sites being diluted by control sites.\nIn a nutshell: while the data existed in the database, extracting information from large quantities of work orders, and the corresponding textual fields was complex. It simply wasn\u2019t as easy to prove retrofit savings as it may have appeared. The client was relying on a detailed analysis to help inform future lighting benefits, so the analysis had to be spot on to ensure a correct decision.\nUsing data science to deep dive into topics and ensure accuracy\nThe first step in defining an objective and efficient way to quantify savings was to identify a baseline: when did the retrofit happen, and which sites were retrofit? Looking at the costs on either side of the baseline for test sites, for both retrofit sites and non-retrofit sites, it\u2019s possible to identify the impact of the retrofit.\nProving that the drop was in fact driven by the fluorescent tube light change from type A1 to A2 was the next challenge - as that level of detail is not kept as a tabular record and is difficult to extract. The solution for this was through application of data science. Topic modeling, an unsupervised natural language processing (NLP) technique, was used to read through all work orders\u2019 descriptions and resolutions - to understand what issue occurred, and what work was performed at the site. This technique categorized the service calls in an objective fashion, providing statistics to deep dive into the topics to ensure accuracy. In one case, topic modelling detected a category of work orders (\u201cceil, height, standard, fluoresce\u201d) where service calls had been initiated to change a fluorescent tube light at ceiling height. This was clearly an activity which was within the scope of the retrofit project objective i.e. the project aimed to reduce such types of work orders. Several other in-scope themes of service calls were also identified which occurred prior to the retrofit.\nTopic modelling was conducted in both the pre-retrofit and post-retrofit phases to identify (1) the types and counts of work orders created, (2) whether the underlying issues were those which the retrofit was designed to address, and (3) the associated costs within each type of topic.\nThe issues that the retrofit was designed to address \u2013 like in the example above \u2013 reduced dramatically in quantity and in the overall priority post-retrofit. When compared to non-retrofit sites, it became further apparent that the retrofit resulted in cost reductions. Topic modelling allowed BGIS to attribute these savings to the particular type of light bulb that was replaced under the retrofit.\nResults\nAt a high level, the lighting retrofit project was budgeted at approximately $4M. The savings were a combination of reductions in both electricity consumption and maintenance and repair costs (M & R work orders). Energy savings were measurable from the bills, while the M & R savings required the topic modeling approach. Therefore the savings just on M & R work orders of $420K (annualized) is substantial as a proportion of the overall project budget and helped to justify the overall project costs. Specific results were:\n$420K worth of annual cost savings$35K reduction in average monthly work order costsPotential for even more savingsAs a service provider, this analysis demonstrated that savings could potentially have been higher had all sites been retrofit \u2013 the sites which did not go through the retrofit in fact increased in cost over time over the baseline. Additionally, future retrofits may be condensed over shorter time frames to reap cost avoidance benefits, leading to stronger business cases and savings for clients. From an analysis perspective, modern technology can quickly address common business problems in an objective, transparent, and repeatable fashion. A risk with analyses conducted at aggregate levels is that such analyses can easily provide an incorrect direction (Simpson\u2019s Paradox); in this case, had an aggregated method been used, it could have incorrectly led to an understatement of the savings delivered for the client.\nWhy KNIME?\nA review of independent research papers identified KNIME as a Leader in Gartner\u2019s 2018 Magic Quadrant for Data Science and Machine Learning Platforms: a position KNIME had retained for four years prior. Additionally, the total cost of ownership for KNIME was dramatically lower than other software providers in the same quadrant.\nAs brand new tool to the organization,\u00a0KNIME Analytics Platform\u00a0was simple to learn thanks to an extensive example library (KNIME Hub), several\u00a0free and paid online courses, a buzzing online community on\u00a0KNIME Forum\u00a0(as is typical with many open source tools), and a responsive support team. A key reason for selecting KNIME was the no-fee,\u00a0one-click download. Other data science tools were considered, but the high licensing fees quickly made the total cost of ownership unpalatable. Also, because KNIME doesn\u2019t work in competition with existing tools, but rather alongside them, it provided peace of mind that the tools the business is familiar with could still be used \u2013 including\u00a0Access,\u00a0SQL\u00a0and\u00a0Tableau.\nGetting started with KNIME\u00a0was also very easy \u2013 thanks to all the free resources available online. Paras Gupta, Director, BI & Advanced Analytics at BGIS went from\u00a0\"having zero experience to being an advanced user in under two weeks.\u201d\nFurthermore, in this case specifically, the client was able to go back and justify further business cases - helping BGIS to prove value and to continue delivering value to clients.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 9,
        "url": "https://www.knime.com/success-story/how-knime-helped-centogene-identify-biomarkers-and-improve-accuracy-patient",
        "title": "How KNIME helped Centogene identify biomarkers and improve accuracy of patient diagnostics | KNIME",
        "company": "Centogene",
        "content": "Access to data and algorithms for non-AI experts\nThe main business problem was to push forward the frontier of metabolomics: The biomarker department was enabled to use novel and highly advanced methods for metabolomic analysis and biomarker discovery, i.e., AI-based algorithms that allow discovery of new insights on big data sets. A software environment was required in which the biomarker experts had access to the data and algorithms without having to be IT or AI experts. The environment also had to be flexible enough to enable innovation by seamlessly integrating new or improved algorithms.\nA collaborative and interactive infrastructure was crucial to enabling everyone to work seamlessly together. Given the nature of the work, and the speed at which CENTOGENE operates, the platform needed to be reliable, offer reproducible workflows, provide a way to standardize operations (including tracking and auditing), as well as automate certain steps.\nStrong interdepartmental collaboration with Guided Analytics\nKNIME Analytics Platform\u00a0provides the ideal environment for a team of data scientists to build several large, automated workflows and visualizations. These workflows, deployed to the\u00a0KNIME WebPortal\u00a0via\u00a0KNIME Server, create web-based applications that users can access and interact with. The intuitive and easy-to-use nature of the solution enables strong interdepartmental collaboration. With Guided Analytics, users can interact with and explore the data through interactive web pages where a KNIME workflow is running under the hood. This provides the team with access to the data, which is pre-processed and offered in a more usable form. It also provides the ability to interact with it at pre-determined points and dive deeper when needed. The seamless integration with databases, processes, and other software means KNIME is used as a hub for the optimization and linkage of a bigger software architecture. This is made possible by features such as the native\u00a0Python integration\u00a0and integration with\u00a0Jupyter notebooks.\nDue to this central, pivotal role, KNIME is also used as an early detection point for variations in the infrastructure to which it is connected to. This was made possible as a side benefit of a feature that was implemented primarily to increase reliability during workflow development:\u00a0automated workflow testing. Both the company and project goals intersect at the level of automation, reproducibility, and security related aspects.\nSecurity and control is addressed by native KNIME functions, such as versioning and extensive logging for auditing. However, what allows the services and implementations to exceed expectations in this case, is the flexibility with which newly developed\u00a0components\u00a0enable the sharing and reusing of KNIME workflow snippets \u2013 either in other workflows or by other teams \u2013 which ensures reproducibility and standardization of different processes\nResults\nKNIME has enabled CENTOGENE to provide their rare disease patients with better medical solutions more quickly and cost effectively. By optimizing automation, reproducibility, and security, KNIME has provided the CENTOGENE team with the tools needed to minimize time spent on non-essential training of the technology, and fewer processes with regards to how the domain expert interacts with the Bio/Databank.\nUsing KNIME has resulted in enhanced collaboration and interactive infrastructures, which are now the core elements in creating machine learning models for biomarkers. KNIME has also enabled the automation of workflows, which has reduced the amount of manual work required by data scientists, as well as enabled users to interact with intuitive visualizations to get even more out of the data than was previously possible. Being able to meaningfully integrate data coming from different resources and experiments is what gives users the edge to address complex scientific scenarios. The overall result: the ability to identify biomarkers and improve workflows for screening and diagnosis for patients more quickly.\nWhy KNIME?\nKNIME Analytics Platform\u00a0is intuitive and makes creating data science workflows easy due to\u00a0visual programming, the drag and drop workflow building method, and shallow learning curve. However, the power of KNIME lies in the self-documenting nature and reproducibility of these workflows. This ensures that knowledge and expertise is captured and saved automatically and enables others to understand what is going on. Parts of the workflow can be packaged up into\u00a0components\u00a0and shared among colleagues or added to other workflows. This guarantees standardization as well as compliance of certain steps or processes, such as data processing rules. Tracking and auditing of KNIME workflows is automatically captured via the workflow metadata.\nKNIME Server\u00a0plays a pivotal role in this solution because it offers workflow automation, enables collaboration among team members in remote locations, and provides users with access to the\u00a0KNIME WebPortal. It also functions as hub for the bigger infrastructure in which it is embedded. Moreover,\u00a0KNIME Server on AWS\u00a0enables users to adapt resources once the data process becomes intense, without compromising existing infrastructure and without burdening other connected resources. From a business perspective, KNIME addresses all concerns around risk, security, and auditing. Expertise from\u00a0KNIME Partner Discngine\u00a0was a strong contributing factor to the success of this project due to both their sound technical knowledge and understanding of KNIME, as well as their\u00a0life science\u00a0background.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 10,
        "url": "https://www.knime.com/success-story/how-knime-helped-centogene-identify-biomarkers-and-improve-accuracy-patient",
        "title": "How KNIME helped Centogene identify biomarkers and improve accuracy of patient diagnostics | KNIME",
        "company": "Centogene",
        "content": "Access to data and algorithms for non-AI experts\nThe main business problem was to push forward the frontier of metabolomics: The biomarker department was enabled to use novel and highly advanced methods for metabolomic analysis and biomarker discovery, i.e., AI-based algorithms that allow discovery of new insights on big data sets. A software environment was required in which the biomarker experts had access to the data and algorithms without having to be IT or AI experts. The environment also had to be flexible enough to enable innovation by seamlessly integrating new or improved algorithms.\nA collaborative and interactive infrastructure was crucial to enabling everyone to work seamlessly together. Given the nature of the work, and the speed at which CENTOGENE operates, the platform needed to be reliable, offer reproducible workflows, provide a way to standardize operations (including tracking and auditing), as well as automate certain steps.\nStrong interdepartmental collaboration with Guided Analytics\nKNIME Analytics Platform\u00a0provides the ideal environment for a team of data scientists to build several large, automated workflows and visualizations. These workflows, deployed to the\u00a0KNIME WebPortal\u00a0via\u00a0KNIME Server, create web-based applications that users can access and interact with. The intuitive and easy-to-use nature of the solution enables strong interdepartmental collaboration. With Guided Analytics, users can interact with and explore the data through interactive web pages where a KNIME workflow is running under the hood. This provides the team with access to the data, which is pre-processed and offered in a more usable form. It also provides the ability to interact with it at pre-determined points and dive deeper when needed. The seamless integration with databases, processes, and other software means KNIME is used as a hub for the optimization and linkage of a bigger software architecture. This is made possible by features such as the native\u00a0Python integration\u00a0and integration with\u00a0Jupyter notebooks.\nDue to this central, pivotal role, KNIME is also used as an early detection point for variations in the infrastructure to which it is connected to. This was made possible as a side benefit of a feature that was implemented primarily to increase reliability during workflow development:\u00a0automated workflow testing. Both the company and project goals intersect at the level of automation, reproducibility, and security related aspects.\nSecurity and control is addressed by native KNIME functions, such as versioning and extensive logging for auditing. However, what allows the services and implementations to exceed expectations in this case, is the flexibility with which newly developed\u00a0components\u00a0enable the sharing and reusing of KNIME workflow snippets \u2013 either in other workflows or by other teams \u2013 which ensures reproducibility and standardization of different processes\nResults\nKNIME has enabled CENTOGENE to provide their rare disease patients with better medical solutions more quickly and cost effectively. By optimizing automation, reproducibility, and security, KNIME has provided the CENTOGENE team with the tools needed to minimize time spent on non-essential training of the technology, and fewer processes with regards to how the domain expert interacts with the Bio/Databank.\nUsing KNIME has resulted in enhanced collaboration and interactive infrastructures, which are now the core elements in creating machine learning models for biomarkers. KNIME has also enabled the automation of workflows, which has reduced the amount of manual work required by data scientists, as well as enabled users to interact with intuitive visualizations to get even more out of the data than was previously possible. Being able to meaningfully integrate data coming from different resources and experiments is what gives users the edge to address complex scientific scenarios. The overall result: the ability to identify biomarkers and improve workflows for screening and diagnosis for patients more quickly.\nWhy KNIME?\nKNIME Analytics Platform\u00a0is intuitive and makes creating data science workflows easy due to\u00a0visual programming, the drag and drop workflow building method, and shallow learning curve. However, the power of KNIME lies in the self-documenting nature and reproducibility of these workflows. This ensures that knowledge and expertise is captured and saved automatically and enables others to understand what is going on. Parts of the workflow can be packaged up into\u00a0components\u00a0and shared among colleagues or added to other workflows. This guarantees standardization as well as compliance of certain steps or processes, such as data processing rules. Tracking and auditing of KNIME workflows is automatically captured via the workflow metadata.\nKNIME Server\u00a0plays a pivotal role in this solution because it offers workflow automation, enables collaboration among team members in remote locations, and provides users with access to the\u00a0KNIME WebPortal. It also functions as hub for the bigger infrastructure in which it is embedded. Moreover,\u00a0KNIME Server on AWS\u00a0enables users to adapt resources once the data process becomes intense, without compromising existing infrastructure and without burdening other connected resources. From a business perspective, KNIME addresses all concerns around risk, security, and auditing. Expertise from\u00a0KNIME Partner Discngine\u00a0was a strong contributing factor to the success of this project due to both their sound technical knowledge and understanding of KNIME, as well as their\u00a0life science\u00a0background.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 11,
        "url": "https://www.knime.com/success-story/how-chiesi-automated-physico-chemical-property-calculation-50000-compounds",
        "title": "How Chiesi automated the physico-chemical property calculation of 50,000 + compounds | KNIME",
        "company": "Chiesi",
        "content": "Getting information to medicinal chemists in a timely manner\nThe development of drug candidates that combine an acceptable biological activity and an appropriate physico-chemical profile is a key challenge. Therefore, in the drug discovery process, physico-chemical properties are important parameters for the characterization of compounds. In the search for new drug candidates, medicinal chemists routinely evaluate data such as biological activities and physico-chemical properties associated to numerous compounds. This is to prioritize the most promising ones for further optimization or study and discard the others. The present workflow helps chemists evaluate whether the compounds possess desirable physico-chemical properties such as solubility, pKa, and Lipinski criteria.\nThe goal of this project was to provide essential information to all medicinal chemists in a timely manner. All scientists, about one hundred people \u2013 and not only MedChem or computational chemists - all benefit from these calculated properties at various stages of the discovery process.\nThe specific requirements of this project included:\nIntegrate physico-chemical properties (calculated by ACD/Labs Percepta) to the corporate database.Implement a user-friendly data visualization format for pKa values to simplify association of calculated property to compound functional group.Automate the entire process without any human intervention (e-mail if workflow ends with success).Run the workflow once a day, during night.Extract newly registered (i.e. without calculated physico-chemical properties) molecules from dedicated ORACLE\u2122 view.Interact with ACD/Labs Percepta in order to calculate physico-chemical properties.Parse the calculated results and upload to the corporate database.Render the annotated structures in PNG image and upload to the corporate database.Fully automating the evaluation of drug compoundsTo aid chemists in evaluating whether compounds have desirable physico-chemical properties, a KNIME workflow was developed that that routinely updates compounds registered in the proprietary database, with the corresponding predicted physico-chemical properties (LogP, LogD, LogS and pKa). A commercial program for property calculation (ACD/Labs Percepta) has been coupled with KNIME Analytics Platform and KNIME Server to fully automate this procedure for all new chemical entities registered in the company database. The KNIME workflow, deployed on KNIME Server, is executed automatically at a given time, and results are stored in the Chiesi proprietary corporate DB.\nThe project started with verifying that ACD/Labs Percepta (batch module) could calculate all the needed properties via command-line and that the results were compatible with standard KNIME nodes \u2013 specifically SDF Reader and CSV Reader. A set of molecule structures was then received from a public structure database to use in setting up a properties calculation different enough to cover most calculation problems. Then the structure\u2019s format of the input table coming from the ORACLE\u2122 view was defined (i.e. SDF or SMILES format of molecules, identifier, primary keys, other fields), as well as the output format to write to ORACLE\u2122 tables (table names, fields name and type, accessory columns).\nThe construction of the workflow looked like this:\nBuild the ORACLE\u2122 table similar to production environment, for the read and write/update.Export SMILES structures, transform and write an SDF file, and instruct external applications to read the last one using the External Tool node.Gather the output files (CSV and SDF).Split data between a single-type property (LogP, LogD and LogS at pH 7.4) to be registered in the molecule table, multiple-type property (pKa values) to be registered in its proper table and warning message to be archived as logfile.txt \u2013 all using the CSV Reader Output.Annotate the pKa values in the SDF structure, render as PNG image and load as binary object in a third table \u2013 all using SDF Reader Output.Send email to administrator (assuming all is fine) automatically using KNIME Server.\n\nResults\nThis project has resulted in approximately 50,000 compounds with calculated properties over a timeframe of more than 4 years - without any problem or intervention. The biggest impact that this solution has had, is the time saved by scientists who no longer need to calculate properties on demand \u2013 as a result, customer satisfaction has also increased considerably. The biggest lesson learned: solving a real-life business case using integration and automation increase productivity and user experience.\nWhy KNIME?\nBefore the project began, two key features were required. The first being the ability to interact (read, write and update permissions) with ORACLE\u2122 Database. The second: the ability to interact via command line with third party software (ACD/Labs Percepta Batch). KNIME Analytics Platform enabled us to do these two things.\nTo start with, the free and open source KNIME Analytics Platform played an important role \u2013 largely due to the significant cost advantages that an open source software has, as well as the number of internal KNIME advocates who had already been using KNIME . Once acceptance for KNIME Analytics Platform grew, getting a license for KNIME Server was much simpler. Furthermore, adoption of KNIME Server was driven by the possibility to solve other use cases across different departments.\nThis Success Story can be downloaded here as a PDF.\n"
    },
    {
        "id": 12,
        "url": "https://www.knime.com/success-story/how-chiesi-automated-physico-chemical-property-calculation-50000-compounds",
        "title": "How Chiesi automated the physico-chemical property calculation of 50,000 + compounds | KNIME",
        "company": "Chiesi",
        "content": "Getting information to medicinal chemists in a timely manner\nThe development of drug candidates that combine an acceptable biological activity and an appropriate physico-chemical profile is a key challenge. Therefore, in the drug discovery process, physico-chemical properties are important parameters for the characterization of compounds. In the search for new drug candidates, medicinal chemists routinely evaluate data such as biological activities and physico-chemical properties associated to numerous compounds. This is to prioritize the most promising ones for further optimization or study and discard the others. The present workflow helps chemists evaluate whether the compounds possess desirable physico-chemical properties such as solubility, pKa, and Lipinski criteria.\nThe goal of this project was to provide essential information to all medicinal chemists in a timely manner. All scientists, about one hundred people \u2013 and not only MedChem or computational chemists - all benefit from these calculated properties at various stages of the discovery process.\nThe specific requirements of this project included:\nIntegrate physico-chemical properties (calculated by ACD/Labs Percepta) to the corporate database.Implement a user-friendly data visualization format for pKa values to simplify association of calculated property to compound functional group.Automate the entire process without any human intervention (e-mail if workflow ends with success).Run the workflow once a day, during night.Extract newly registered (i.e. without calculated physico-chemical properties) molecules from dedicated ORACLE\u2122 view.Interact with ACD/Labs Percepta in order to calculate physico-chemical properties.Parse the calculated results and upload to the corporate database.Render the annotated structures in PNG image and upload to the corporate database.Fully automating the evaluation of drug compoundsTo aid chemists in evaluating whether compounds have desirable physico-chemical properties, a KNIME workflow was developed that that routinely updates compounds registered in the proprietary database, with the corresponding predicted physico-chemical properties (LogP, LogD, LogS and pKa). A commercial program for property calculation (ACD/Labs Percepta) has been coupled with KNIME Analytics Platform and KNIME Server to fully automate this procedure for all new chemical entities registered in the company database. The KNIME workflow, deployed on KNIME Server, is executed automatically at a given time, and results are stored in the Chiesi proprietary corporate DB.\nThe project started with verifying that ACD/Labs Percepta (batch module) could calculate all the needed properties via command-line and that the results were compatible with standard KNIME nodes \u2013 specifically SDF Reader and CSV Reader. A set of molecule structures was then received from a public structure database to use in setting up a properties calculation different enough to cover most calculation problems. Then the structure\u2019s format of the input table coming from the ORACLE\u2122 view was defined (i.e. SDF or SMILES format of molecules, identifier, primary keys, other fields), as well as the output format to write to ORACLE\u2122 tables (table names, fields name and type, accessory columns).\nThe construction of the workflow looked like this:\nBuild the ORACLE\u2122 table similar to production environment, for the read and write/update.Export SMILES structures, transform and write an SDF file, and instruct external applications to read the last one using the External Tool node.Gather the output files (CSV and SDF).Split data between a single-type property (LogP, LogD and LogS at pH 7.4) to be registered in the molecule table, multiple-type property (pKa values) to be registered in its proper table and warning message to be archived as logfile.txt \u2013 all using the CSV Reader Output.Annotate the pKa values in the SDF structure, render as PNG image and load as binary object in a third table \u2013 all using SDF Reader Output.Send email to administrator (assuming all is fine) automatically using KNIME Server.\n\nResults\nThis project has resulted in approximately 50,000 compounds with calculated properties over a timeframe of more than 4 years - without any problem or intervention. The biggest impact that this solution has had, is the time saved by scientists who no longer need to calculate properties on demand \u2013 as a result, customer satisfaction has also increased considerably. The biggest lesson learned: solving a real-life business case using integration and automation increase productivity and user experience.\nWhy KNIME?\nBefore the project began, two key features were required. The first being the ability to interact (read, write and update permissions) with ORACLE\u2122 Database. The second: the ability to interact via command line with third party software (ACD/Labs Percepta Batch). KNIME Analytics Platform enabled us to do these two things.\nTo start with, the free and open source KNIME Analytics Platform played an important role \u2013 largely due to the significant cost advantages that an open source software has, as well as the number of internal KNIME advocates who had already been using KNIME . Once acceptance for KNIME Analytics Platform grew, getting a license for KNIME Server was much simpler. Furthermore, adoption of KNIME Server was driven by the possibility to solve other use cases across different departments.\nThis Success Story can be downloaded here as a PDF.\n"
    },
    {
        "id": 13,
        "url": "https://www.knime.com/success-story/how-knime-helps-hr-teams-predict-attrition-low-code-machine-learning",
        "title": "How KNIME helps HR teams predict attrition with low-code machine learning | KNIME",
        "company": "ClearPeaks",
        "content": "The challenge: Use attrition analysis to design effective talent retention strategies\nEmployee attrition is a significant cost to an organization. A high attrition rate can lead to increased tangible costs such as training, recruitment, and on-boarding, as well as intangible costs such as project management and customer relationships. Attrition analysis and support in defining an optimal talent retention strategy. However, this requires a deep understanding of employee behavior. If HR directors designed talent retention strategies based on data and machine learning, companies could significantly reduce attrition and see an increase in employee productivity and company profitability. Together with attrition, the quantification of employee value and attrition cost is key to making optimal HR decisions.\nThe solution: A KNIME Guided Analytics application\nA KNIME workflow collects employee data provided by the HR department and estimates the probability of each employee leaving the company. The dataset is then cleaned up for outliers, erroneous values, and/or representational structure. Employee attrition datasets are usually imbalanced for the attrition category; hence a rebalancing exercise is performed. Several models such as Random Forest, Logistic Regression, Na\u00efve Bayes, and Gradient Boosted are trained, with the best performing model being chosen to score current employees. The workflow is deployed on\u00a0KNIME Server\u00a0and can be executed on demand or using the scheduling option, to update attrition probabilities. A dataset is generated with the model output, which the HR department uses for analysis on the profiles of the employees likely to leave.\nWith the native\u00a0Tableau Integration\u00a0in\u00a0KNIME Analytics Platform, the results are exported directly to a Tableau dashboard. This provides business users with access to informative and useful visualizations quickly and easily. A\u00a0Guided Analytics\u00a0application, made accessible via the KNIME WebPortal, enables business users to make parametric changes to understand their impact on the attrition process. Business users can also simulate scenarios, which empowers them and promotes a data-driven decision-making culture.\nBoth workflows are available for download on\u00a0KNIME Hub:\nTraining a Churn Predictor WorkflowDeploying a Churn Predictor WorkflowWhy KNIME?KNIME provides the ideal environment for building a classification machine learning model, and conducting an attrition analysis. It\u2019s possible to build workflows using the appropriate connector for different data sources, transform data, and train machine learning models.\u00a0KNIME Server\u00a0enables these workflows to be executed at predefined times. The\u00a0KNIME WebPortal\u00a0allows business users to interact with these workflows by uploading their own data or updating the attrition probabilities on demand. The seamless integration with\u00a0Tableau, enables users to create dashboards in order to dive deeper into the results.\nThis Innovation Note is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 14,
        "url": "https://www.knime.com/success-story/how-knime-helps-hr-teams-predict-attrition-low-code-machine-learning",
        "title": "How KNIME helps HR teams predict attrition with low-code machine learning | KNIME",
        "company": "ClearPeaks",
        "content": "The challenge: Use attrition analysis to design effective talent retention strategies\nEmployee attrition is a significant cost to an organization. A high attrition rate can lead to increased tangible costs such as training, recruitment, and on-boarding, as well as intangible costs such as project management and customer relationships. Attrition analysis and support in defining an optimal talent retention strategy. However, this requires a deep understanding of employee behavior. If HR directors designed talent retention strategies based on data and machine learning, companies could significantly reduce attrition and see an increase in employee productivity and company profitability. Together with attrition, the quantification of employee value and attrition cost is key to making optimal HR decisions.\nThe solution: A KNIME Guided Analytics application\nA KNIME workflow collects employee data provided by the HR department and estimates the probability of each employee leaving the company. The dataset is then cleaned up for outliers, erroneous values, and/or representational structure. Employee attrition datasets are usually imbalanced for the attrition category; hence a rebalancing exercise is performed. Several models such as Random Forest, Logistic Regression, Na\u00efve Bayes, and Gradient Boosted are trained, with the best performing model being chosen to score current employees. The workflow is deployed on\u00a0KNIME Server\u00a0and can be executed on demand or using the scheduling option, to update attrition probabilities. A dataset is generated with the model output, which the HR department uses for analysis on the profiles of the employees likely to leave.\nWith the native\u00a0Tableau Integration\u00a0in\u00a0KNIME Analytics Platform, the results are exported directly to a Tableau dashboard. This provides business users with access to informative and useful visualizations quickly and easily. A\u00a0Guided Analytics\u00a0application, made accessible via the KNIME WebPortal, enables business users to make parametric changes to understand their impact on the attrition process. Business users can also simulate scenarios, which empowers them and promotes a data-driven decision-making culture.\nBoth workflows are available for download on\u00a0KNIME Hub:\nTraining a Churn Predictor WorkflowDeploying a Churn Predictor WorkflowWhy KNIME?KNIME provides the ideal environment for building a classification machine learning model, and conducting an attrition analysis. It\u2019s possible to build workflows using the appropriate connector for different data sources, transform data, and train machine learning models.\u00a0KNIME Server\u00a0enables these workflows to be executed at predefined times. The\u00a0KNIME WebPortal\u00a0allows business users to interact with these workflows by uploading their own data or updating the attrition probabilities on demand. The seamless integration with\u00a0Tableau, enables users to create dashboards in order to dive deeper into the results.\nThis Innovation Note is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 15,
        "url": "https://www.knime.com/success-story/how-combating-terrorism-office-detects-irregular-conflict",
        "title": "How the Combating Terrorism Office detects irregular conflict | KNIME",
        "company": "Combating Terrorism Technical Support Office",
        "content": "Inferring gray-zone activity\nKNIME Partner BigBear.ai built VANE \u2013 the Virtual Anticipatory Network \u2013 which detects gray-zone activity from open-source data by reading in between the lines of directly reported phenomena. This allows decision makers to understand how US activities influence complex systems to mitigate gray-zone influences. It\u2019s not designed to predict the precise date that an anticipated event is going to take place. VANE monitors data streams of known drivers that are predictive of the event of interest \u2013 and models courses of action to reduce the likelihood of the event\u2019s occurrence. \nVANE helps give decision makers quantitative insights to do this. It\u2019s a data-driven platform answering questions such as what does the future hold for X? Publicly available data such as demographic data, econometric data, news, social media, web information, and more is plugged into the system and provides the insights that decision makers need to achieve the desired end-state.\n\nDetermining signals of fake news\nVANE currently leverages 14 open-source databases, tracking 660 independent metrics. The model spans many different topic areas \u2013 from economies to weather to cyber \u2013 of which many are not pristine. For example, one of the event sources is GDELT, which tracks 17,000 different news sources in 70 different languages. However, it struggles to maintain a clean dataset. Every time an Air Jordan sneaker is sold, an event in Jordan (the country) occurs. GDELT also doesn\u2019t determine between fake news, bad reports, and so on. Usually to compensate for low-quality data, data scientists must impute metrics or clean data-streams manually, which is time intensive and can introduce errors in the model.\nTherefore, tensor completion is used. Specifically matrix factorization, which is a two-degree form of tensor completion. In this case, this has been extended into multiple domains because the real world isn\u2019t just users and ratings. It\u2019s countries and sensors and different time dimensions, meaning there is a lot more going into the tensor. The power of tensor completion is it finds relationships not just between two entities, but between the features that are being used to model the relationship between entities.\nAn example of how this works for image recognition: feed in some images that have pixel error, obfuscation, or where entire chunks of the image are missing entirely. The tensor completion algorithm will learn what it can about how pixels, edges, and colors relate within the image and provide a reconstructed view of what it thinks the data should look like. The other benefit is that a matrix is provided, which states where the suspected error is inside the data.\nWhy KNIME?\nKNIME Analytics Platform\u00a0is great for many reasons. One of the greatest things about KNIME is that it\u2019s a no-coding workflow building environment. However, it\u2019s possible to add code when and where needed, which is what was done in this case. There are few developers and even less technical specialists working in the government. Therefore, giving them a platform that lowers the barrier to entry so that they can engage productively and communicate with the actual developers is important.\nMost projects that the CTTSO works on require out of the box solutions, which means creative thinking is needed and in many cases, the need to extend the software - which is possible with KNIME. As a result, over 15 custom nodes have been written. This makes the projects easier to manage, because they are still being completed within a single software environment. \u201cWe\u2019ve got a lot of great things that help us take the value that KNIME gives us and fit it right in the little square peg, round hole that we\u2019re working in. And we can do it very, very quickly, plus integrate with all the other platforms that the government likes to use\u201d says Brian Frutchey, VANE Technical Lead at BigBear.ai.\nThis Success Story can be downloaded\u00a0here\u00a0as a PDF.\n\n\n"
    },
    {
        "id": 16,
        "url": "https://www.knime.com/success-story/how-combating-terrorism-office-detects-irregular-conflict",
        "title": "How the Combating Terrorism Office detects irregular conflict | KNIME",
        "company": "Combating Terrorism Technical Support Office",
        "content": "Inferring gray-zone activity\nKNIME Partner BigBear.ai built VANE \u2013 the Virtual Anticipatory Network \u2013 which detects gray-zone activity from open-source data by reading in between the lines of directly reported phenomena. This allows decision makers to understand how US activities influence complex systems to mitigate gray-zone influences. It\u2019s not designed to predict the precise date that an anticipated event is going to take place. VANE monitors data streams of known drivers that are predictive of the event of interest \u2013 and models courses of action to reduce the likelihood of the event\u2019s occurrence. \nVANE helps give decision makers quantitative insights to do this. It\u2019s a data-driven platform answering questions such as what does the future hold for X? Publicly available data such as demographic data, econometric data, news, social media, web information, and more is plugged into the system and provides the insights that decision makers need to achieve the desired end-state.\n\nDetermining signals of fake news\nVANE currently leverages 14 open-source databases, tracking 660 independent metrics. The model spans many different topic areas \u2013 from economies to weather to cyber \u2013 of which many are not pristine. For example, one of the event sources is GDELT, which tracks 17,000 different news sources in 70 different languages. However, it struggles to maintain a clean dataset. Every time an Air Jordan sneaker is sold, an event in Jordan (the country) occurs. GDELT also doesn\u2019t determine between fake news, bad reports, and so on. Usually to compensate for low-quality data, data scientists must impute metrics or clean data-streams manually, which is time intensive and can introduce errors in the model.\nTherefore, tensor completion is used. Specifically matrix factorization, which is a two-degree form of tensor completion. In this case, this has been extended into multiple domains because the real world isn\u2019t just users and ratings. It\u2019s countries and sensors and different time dimensions, meaning there is a lot more going into the tensor. The power of tensor completion is it finds relationships not just between two entities, but between the features that are being used to model the relationship between entities.\nAn example of how this works for image recognition: feed in some images that have pixel error, obfuscation, or where entire chunks of the image are missing entirely. The tensor completion algorithm will learn what it can about how pixels, edges, and colors relate within the image and provide a reconstructed view of what it thinks the data should look like. The other benefit is that a matrix is provided, which states where the suspected error is inside the data.\nWhy KNIME?\nKNIME Analytics Platform\u00a0is great for many reasons. One of the greatest things about KNIME is that it\u2019s a no-coding workflow building environment. However, it\u2019s possible to add code when and where needed, which is what was done in this case. There are few developers and even less technical specialists working in the government. Therefore, giving them a platform that lowers the barrier to entry so that they can engage productively and communicate with the actual developers is important.\nMost projects that the CTTSO works on require out of the box solutions, which means creative thinking is needed and in many cases, the need to extend the software - which is possible with KNIME. As a result, over 15 custom nodes have been written. This makes the projects easier to manage, because they are still being completed within a single software environment. \u201cWe\u2019ve got a lot of great things that help us take the value that KNIME gives us and fit it right in the little square peg, round hole that we\u2019re working in. And we can do it very, very quickly, plus integrate with all the other platforms that the government likes to use\u201d says Brian Frutchey, VANE Technical Lead at BigBear.ai.\nThis Success Story can be downloaded\u00a0here\u00a0as a PDF.\n\n\n"
    },
    {
        "id": 17,
        "url": "https://www.knime.com/success-story/how-continental-delivered-global-analytics-upskilling-program-knime",
        "title": "How Continental delivered a global analytics upskilling program with KNIME | KNIME",
        "company": "Continental",
        "content": "Starting a global roll-out\nThe roll-out of KNIME at Continental started out as a pilot project in Chassis & Safety (C&S), a \u20ac9.6bn sales division of the corporation, and was led by Dr. Arne Beckhaus - Head of Big Data and Digital Transformation C&S at Continental. The very first use case emerged in the Brake System R&D Department, where KNIME was used to generate Kanban boards based on data exported from the issue tracking system.\nAn agile setup of starting small and expanding quickly had a positive response. It was quickly clear just how much business users loved using KNIME and how much data processing power was gained. The roll-out was made easier by positive word of mouth and a high, bottom-up demand for trainings on how to use KNIME.\nMany processes across the entire organization have now achieved higher levels of automation. For example, production planning, reporting (controlling, logistics, etc.), various project management tasks such as Kanban boards and deadline tracking, supply chain warnings, and so on. Data integration and data wrangling are now mostly done by business users in KNIME, and, in many cases, this step alone yields the promised data insight or process automation. Sometimes KNIME is used for an intermediate data preparation step before the data can be exported to visualization tools such as\u00a0PowerBI,\u00a0Tableau, and MicroStrategy. Continental also uses Amazon Web Services for long-running workflows (or workflows which are RAM or CPU hungry).\nBetter decisions mean huge resource savings\nSignificant\u00a0time savings of over 80% in pilot projectsLead time reduction of month-end controlling tasks\u00a0from two days to thirty minutesAbility to run an entire plant budgeting process\u00a0at a tenfold precision levelSerial number traceability to R&D, plant, and quality users, providing answers in minutes rather than in daysAnd there\u2019s a lot more potential for using KNIME throughout the organization. \u201cOne indicator for the high adoption of KNIME is the rising demand for in-house trainings\u201d he continues. \u201cIt is great to see that we can serve this huge bottom up momentum with a very small central team\u201d. The next steps will entail creating more user-specific training content for users in different departments, enabling a more institutionalized roll-out, and realizing and standardizing knowledge sharing across teams and departments. Due to high interest by external parties who want to learn from Continental\u2019s data transformation journey, the decision was even made to offer data services to external clients using KNIME.\u00a0\nContinental Engineering Services\u00a0is an official\u00a0KNIME Partner\u00a0and able to offer these services.\nWhy KNIME?\nKNIME was chosen primarily for its business user friendliness but also due to speed, data volume, breadth of functionality, and state-of-the art data wrangling and data science features. \u201cFrom a business perspective, the open source model was the only commercially feasible option to rapidly spread data analytics competence throughout the organization\u201d says Arne Beckhaus. A long tail of infrequent users would have made the license fees too expensive and a flat, company-wide license would have slowed everything down due to the initial investment. Typical commercial business models simply would not have enabled such rapid dissemination.\u00a0KNIME Server\u00a0is now also used on selected projects at Continental, enabling teams to collaborate on KNIME workflows and share amongst colleagues.\nKNIME is easy to maintain long term and can be enhanced with custom extensions, which are built and developed by Continental. Examples of the\u00a0Continental Extensions for KNIME\u00a0include extensions to enable custom formatting in Excel reports, or to parse third party PDF documents with text position information for automatically analyzing contract information. From an IT perspective, KNIME is easily installed on client PCs worldwide. Continental can pre-package a customized installation and deliver it as a standalone software application - making it easy for KNIME to be included in the official software distribution process.\nLastly, rolling out KNIME to business departments has helped raise the awareness of analytics and artificial intelligence (AI) at the management level. They now understand the advantage of finding answers to problems by effective data wrangling and the necessity of good quality data for applying data science algorithms in KNIME. The result? Increased data literacy across the organization, more efficient, automated processes, new insights from data without blind application of AI, and better decisions everywhere.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n\n\n"
    },
    {
        "id": 18,
        "url": "https://www.knime.com/success-story/how-continental-delivered-global-analytics-upskilling-program-knime",
        "title": "How Continental delivered a global analytics upskilling program with KNIME | KNIME",
        "company": "Continental",
        "content": "Starting a global roll-out\nThe roll-out of KNIME at Continental started out as a pilot project in Chassis & Safety (C&S), a \u20ac9.6bn sales division of the corporation, and was led by Dr. Arne Beckhaus - Head of Big Data and Digital Transformation C&S at Continental. The very first use case emerged in the Brake System R&D Department, where KNIME was used to generate Kanban boards based on data exported from the issue tracking system.\nAn agile setup of starting small and expanding quickly had a positive response. It was quickly clear just how much business users loved using KNIME and how much data processing power was gained. The roll-out was made easier by positive word of mouth and a high, bottom-up demand for trainings on how to use KNIME.\nMany processes across the entire organization have now achieved higher levels of automation. For example, production planning, reporting (controlling, logistics, etc.), various project management tasks such as Kanban boards and deadline tracking, supply chain warnings, and so on. Data integration and data wrangling are now mostly done by business users in KNIME, and, in many cases, this step alone yields the promised data insight or process automation. Sometimes KNIME is used for an intermediate data preparation step before the data can be exported to visualization tools such as\u00a0PowerBI,\u00a0Tableau, and MicroStrategy. Continental also uses Amazon Web Services for long-running workflows (or workflows which are RAM or CPU hungry).\nBetter decisions mean huge resource savings\nSignificant\u00a0time savings of over 80% in pilot projectsLead time reduction of month-end controlling tasks\u00a0from two days to thirty minutesAbility to run an entire plant budgeting process\u00a0at a tenfold precision levelSerial number traceability to R&D, plant, and quality users, providing answers in minutes rather than in daysAnd there\u2019s a lot more potential for using KNIME throughout the organization. \u201cOne indicator for the high adoption of KNIME is the rising demand for in-house trainings\u201d he continues. \u201cIt is great to see that we can serve this huge bottom up momentum with a very small central team\u201d. The next steps will entail creating more user-specific training content for users in different departments, enabling a more institutionalized roll-out, and realizing and standardizing knowledge sharing across teams and departments. Due to high interest by external parties who want to learn from Continental\u2019s data transformation journey, the decision was even made to offer data services to external clients using KNIME.\u00a0\nContinental Engineering Services\u00a0is an official\u00a0KNIME Partner\u00a0and able to offer these services.\nWhy KNIME?\nKNIME was chosen primarily for its business user friendliness but also due to speed, data volume, breadth of functionality, and state-of-the art data wrangling and data science features. \u201cFrom a business perspective, the open source model was the only commercially feasible option to rapidly spread data analytics competence throughout the organization\u201d says Arne Beckhaus. A long tail of infrequent users would have made the license fees too expensive and a flat, company-wide license would have slowed everything down due to the initial investment. Typical commercial business models simply would not have enabled such rapid dissemination.\u00a0KNIME Server\u00a0is now also used on selected projects at Continental, enabling teams to collaborate on KNIME workflows and share amongst colleagues.\nKNIME is easy to maintain long term and can be enhanced with custom extensions, which are built and developed by Continental. Examples of the\u00a0Continental Extensions for KNIME\u00a0include extensions to enable custom formatting in Excel reports, or to parse third party PDF documents with text position information for automatically analyzing contract information. From an IT perspective, KNIME is easily installed on client PCs worldwide. Continental can pre-package a customized installation and deliver it as a standalone software application - making it easy for KNIME to be included in the official software distribution process.\nLastly, rolling out KNIME to business departments has helped raise the awareness of analytics and artificial intelligence (AI) at the management level. They now understand the advantage of finding answers to problems by effective data wrangling and the necessity of good quality data for applying data science algorithms in KNIME. The result? Increased data literacy across the organization, more efficient, automated processes, new insights from data without blind application of AI, and better decisions everywhere.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n\n\n"
    },
    {
        "id": 19,
        "url": "https://www.knime.com/success-story/how-crystalloids-predicted-stock-value-changes-knime",
        "title": "How Crystalloids predicted stock value changes with KNIME | KNIME",
        "company": "Crystalloids",
        "content": "The challenge: Keep track of changes in stock markets\nThe global stock market is expansive and volatile. With thousands of stocks whose values change constantly due to (macro) economic events, it\u2019s almost impossible for investors to keep track. Furthermore, reliable and current data are not available to everyone. If traders were able to collect real-time stock information, they would be able to not only monitor these stocks of interest but also predict changes in value over time and react accordingly.\nThe solution: automated reports delivered straight to inboxes\nData scientists build a workflow in\u00a0KNIME Analytics Platform\u00a0to estimate the percentage change in stock value for the following day. The workflow uses the native\u00a0KNIME Python Integration\u00a0to collect stock information via the\u00a0pandas-datareader library. The workflow is then deployed on\u00a0KNIME Server\u00a0and, using the built-in scheduling feature, is executed at the start of each working day. A report is generated, which is delivered to the trader\u2019s inbox as soon as it\u2019s finished executing and is ready for them to read the minute they open their mail.\nWhy KNIME?\nKNIME Analytics Platform\u00a0enables data scientists to seamlessly integrate other technologies within one familiar environment. In this case, data is read in from Yahoo Finance using the native\u00a0KNIME Python Integration.\u00a0KNIME Server\u00a0makes it possible to schedule and execute the workflow daily, making important stock information available to traders and other decision makers.\nThis Success Story can be downloaded\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 20,
        "url": "https://www.knime.com/success-story/how-crystalloids-predicted-stock-value-changes-knime",
        "title": "How Crystalloids predicted stock value changes with KNIME | KNIME",
        "company": "Crystalloids",
        "content": "The challenge: Keep track of changes in stock markets\nThe global stock market is expansive and volatile. With thousands of stocks whose values change constantly due to (macro) economic events, it\u2019s almost impossible for investors to keep track. Furthermore, reliable and current data are not available to everyone. If traders were able to collect real-time stock information, they would be able to not only monitor these stocks of interest but also predict changes in value over time and react accordingly.\nThe solution: automated reports delivered straight to inboxes\nData scientists build a workflow in\u00a0KNIME Analytics Platform\u00a0to estimate the percentage change in stock value for the following day. The workflow uses the native\u00a0KNIME Python Integration\u00a0to collect stock information via the\u00a0pandas-datareader library. The workflow is then deployed on\u00a0KNIME Server\u00a0and, using the built-in scheduling feature, is executed at the start of each working day. A report is generated, which is delivered to the trader\u2019s inbox as soon as it\u2019s finished executing and is ready for them to read the minute they open their mail.\nWhy KNIME?\nKNIME Analytics Platform\u00a0enables data scientists to seamlessly integrate other technologies within one familiar environment. In this case, data is read in from Yahoo Finance using the native\u00a0KNIME Python Integration.\u00a0KNIME Server\u00a0makes it possible to schedule and execute the workflow daily, making important stock information available to traders and other decision makers.\nThis Success Story can be downloaded\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 21,
        "url": "https://www.knime.com/solutions/success-story/advanced-job-analytics-at-daimler",
        "title": "Advanced Job Analytics at Daimler | KNIME",
        "company": "Daimler",
        "content": ""
    },
    {
        "id": 22,
        "url": "https://www.knime.com/solutions/success-story/advanced-job-analytics-at-daimler",
        "title": "Advanced Job Analytics at Daimler | KNIME",
        "company": "Daimler",
        "content": ""
    },
    {
        "id": 23,
        "url": "https://www.knime.com/success-story/how-day5-analytics-centralized-and-automated-500-financial-processes",
        "title": "How Day5 Analytics centralized and automated 500 financial processes | KNIME",
        "company": "Day5 Analytics",
        "content": "Scaling the business without a proportional cost increase\nFor many finance organizations, significant amounts of time are spent each month performing repetitive, manual tasks in spreadsheets. These tasks are time consuming and prone to human error. Moreover, most Excel processes are inherently not well documented, cannot be scaled easily, and many are not easy to centralize due to business rules and nuances. They are also not easily codified by IT as they involve complex financial methods built by Finance professionals. Centralizing financial processes across multiple account teams presented an opportunity to optimize and drive efficiencies, allowing the business to scale without a proportional increase in cost.\nAutomating financial cost accruals based on multiple business rules\nCost accruals form a significant component of monthly costs for most companies. Accurate accruals result in accurate monthly financial statements, which CRE clients use to monitor spend and make strategic budget decisions. Previously, several account teams would complete similar accrual calculations each month. This manual process involved downloading CSV files from the Financial ERP, converting to Excel, and performing several sequential calculations to determine the accrual amount. Each account team would apply its own specific business logic to ensure accruals were relevant to the specific CRE client. These processes were carried out in standalone Excel models and were disconnected from one another. Consequently, the methods deviated substantially from a standard process. Once these individual account accruals were generated by Finance, they would be reviewed and approved by the Operations teams, and uploaded to the Financial system.\nThis process was an ideal candidate for centralization and automation due to its well-understood, predicable nature. As multiple account teams were repeating the same procedures each month, albeit with slight differences, this process happened hundreds of times a year. Therefore, any improvement in efficiency to the core process could be delivered across each account.\nThe CAS team consolidated the cost accrual process across 20 accounts by exploring customized accrual processes, defining global and account-specific business rules, and automating the accrual generation process with KNIME. Finance ERP reports are now automatically downloaded into a single folder. KNIME reads through each of the files and applies business rules, tailoring accruals to each CRE client\u2019s nuances. The generated accrual is then reviewed by Operations staff, who ensure the cost aligns with the actual work delivered. KNIME then consolidates the input received from Operations and creates upload-ready templates for the Financial ERP system.\nSaving 5 Weeks Per Year with a New, Nimble Business Process\nPreviously, an analyst would use Excel to perform the end-to-end process, taking around 2.5 days each month. By centralizing this task with KNIME, the process is now completed in under half a day each month - resulting in time-saving of 2 days per month, or almost 5 weeks a year.\u00a0The time saved by centralization is recovered by each account team at the most stressful time of the month: month-end close. As this process scales to support more account teams without a proportional increase in headcount, it will result in additional value-added work for the IFM customer and its clients.\nA hard-coded ETL solution would not work for such a process, simply due to constant changes in CRE clients\u2019 business rules. As this process was built independently by Finance experts as KNIME workflows, teams can nimbly respond to changes in business processes which impact cost accruals. The project increased the transparency of the accrual process and unveiled numerous improvement opportunities during the conversion from Excel to KNIME. The standardized, built-in documentation within KNIME allowed for more focused testing and efficiency in the annual audit process. As a primary venture into self-service automation, this project sparked interest in further automation and drove increased centralization across the department.\n\nManaging 600 + processes, automatically, adding value where it matters\nWith Finance centralization comes the challenge of handling peak workloads. Certain manual processes, such as ensuring accounting entries are uploaded correctly in the system, need to be independently automated to ensure any centralized solutions are scalable. Every month there are over 600 individual processes managed for various account teams \u2013 including accrual uploads, account reconciliations, expense card reviews, budget uploads, and more. Keeping track of such a large volume of critical deliverables is not feasible using spreadsheets. To ensure delivery and accuracy in client commitments, the CAS team created a transaction scheduling solution in KNIME. This involved creating a single calendar encompassing each process for each account team. Working backwards from the final deadline for each process, milestones were assigned and tracked for each step of every task. The tool provides visibility into upcoming deadlines, highlights potential delays, and supports staff scheduling by considering projected deliverable timelines, weekends, statutory holidays, and staff vacations.\nWorkload redistribution drives team engagement\nThe KNIME solution enables these individual processes to be completed routinely through the month, rather than accumulating for month-end. This relieves staff workload at peak periods, reducing stress and burn-out. Built into the KNIME solution are important validation checks, which identify and correct mistakes in manual sheets, such as those requiring foreign exchange conversions. KNIME automatically accesses completed reports generated from the system, ensuring the approved uploads have been successfully posted in the ERP - which prevents omissions from occurring. Furthermore, as these validations are built and embedded into the model, the model can be run independently by junior analysts, allowing senior staff to focus on more engaging value-add analyses.\nWhy KNIME?\nKNIME has been embraced by Finance staff due to its simplicity and relevance. Finance tasks rely on structured data from official Finance systems, and well-defined logical sequences of calculations. As an open-source tool, there are no costs for licenses, the community is engaged and supportive, data is secured, and learning resources are abundant.\nWith its no-code visual interface, logical flow, and Excel-synonymous functions, professionals can easily leverage KNIME to automate their spreadsheet processes. KNIME provides an opportunity for Finance staff to build analytical automation tools without IT support, while remaining nimble to changes in business.\nKNIME was introduced to the IFM customer in 2019 through a formal training program, available commercially through Day5 Analytics. The approach was to introduce staff to the tool, challenge them to tackle small-scale Finance problems, and then to demonstrate their solutions across the department \u2013 inspiring others to give it a fair chance. This gradual pressure-testing by Finance staff themselves proved to be a strong motivator for others to learn and apply the tool. Today, a second generation of trainees teaches others, forming an internal community that enables KNIME expertise to flourish.\nWith KNIME deployed as a core tool across the department\u2019s workstations, many traditional Finance and Accounting professionals have learned and applied KNIME to ease their day-to-day roles. With over 50 automation projects delivered, each saving between 2 and 4 weeks of effort on an annualized basis, the customer has realized long-term efficiencies across the board. New projects are now rolled out faster than ever before, and KNIME has driven engagement across the department by shifting the work focus to value-add.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 24,
        "url": "https://www.knime.com/success-story/how-day5-analytics-centralized-and-automated-500-financial-processes",
        "title": "How Day5 Analytics centralized and automated 500 financial processes | KNIME",
        "company": "Day5 Analytics",
        "content": "Scaling the business without a proportional cost increase\nFor many finance organizations, significant amounts of time are spent each month performing repetitive, manual tasks in spreadsheets. These tasks are time consuming and prone to human error. Moreover, most Excel processes are inherently not well documented, cannot be scaled easily, and many are not easy to centralize due to business rules and nuances. They are also not easily codified by IT as they involve complex financial methods built by Finance professionals. Centralizing financial processes across multiple account teams presented an opportunity to optimize and drive efficiencies, allowing the business to scale without a proportional increase in cost.\nAutomating financial cost accruals based on multiple business rules\nCost accruals form a significant component of monthly costs for most companies. Accurate accruals result in accurate monthly financial statements, which CRE clients use to monitor spend and make strategic budget decisions. Previously, several account teams would complete similar accrual calculations each month. This manual process involved downloading CSV files from the Financial ERP, converting to Excel, and performing several sequential calculations to determine the accrual amount. Each account team would apply its own specific business logic to ensure accruals were relevant to the specific CRE client. These processes were carried out in standalone Excel models and were disconnected from one another. Consequently, the methods deviated substantially from a standard process. Once these individual account accruals were generated by Finance, they would be reviewed and approved by the Operations teams, and uploaded to the Financial system.\nThis process was an ideal candidate for centralization and automation due to its well-understood, predicable nature. As multiple account teams were repeating the same procedures each month, albeit with slight differences, this process happened hundreds of times a year. Therefore, any improvement in efficiency to the core process could be delivered across each account.\nThe CAS team consolidated the cost accrual process across 20 accounts by exploring customized accrual processes, defining global and account-specific business rules, and automating the accrual generation process with KNIME. Finance ERP reports are now automatically downloaded into a single folder. KNIME reads through each of the files and applies business rules, tailoring accruals to each CRE client\u2019s nuances. The generated accrual is then reviewed by Operations staff, who ensure the cost aligns with the actual work delivered. KNIME then consolidates the input received from Operations and creates upload-ready templates for the Financial ERP system.\nSaving 5 Weeks Per Year with a New, Nimble Business Process\nPreviously, an analyst would use Excel to perform the end-to-end process, taking around 2.5 days each month. By centralizing this task with KNIME, the process is now completed in under half a day each month - resulting in time-saving of 2 days per month, or almost 5 weeks a year.\u00a0The time saved by centralization is recovered by each account team at the most stressful time of the month: month-end close. As this process scales to support more account teams without a proportional increase in headcount, it will result in additional value-added work for the IFM customer and its clients.\nA hard-coded ETL solution would not work for such a process, simply due to constant changes in CRE clients\u2019 business rules. As this process was built independently by Finance experts as KNIME workflows, teams can nimbly respond to changes in business processes which impact cost accruals. The project increased the transparency of the accrual process and unveiled numerous improvement opportunities during the conversion from Excel to KNIME. The standardized, built-in documentation within KNIME allowed for more focused testing and efficiency in the annual audit process. As a primary venture into self-service automation, this project sparked interest in further automation and drove increased centralization across the department.\n\nManaging 600 + processes, automatically, adding value where it matters\nWith Finance centralization comes the challenge of handling peak workloads. Certain manual processes, such as ensuring accounting entries are uploaded correctly in the system, need to be independently automated to ensure any centralized solutions are scalable. Every month there are over 600 individual processes managed for various account teams \u2013 including accrual uploads, account reconciliations, expense card reviews, budget uploads, and more. Keeping track of such a large volume of critical deliverables is not feasible using spreadsheets. To ensure delivery and accuracy in client commitments, the CAS team created a transaction scheduling solution in KNIME. This involved creating a single calendar encompassing each process for each account team. Working backwards from the final deadline for each process, milestones were assigned and tracked for each step of every task. The tool provides visibility into upcoming deadlines, highlights potential delays, and supports staff scheduling by considering projected deliverable timelines, weekends, statutory holidays, and staff vacations.\nWorkload redistribution drives team engagement\nThe KNIME solution enables these individual processes to be completed routinely through the month, rather than accumulating for month-end. This relieves staff workload at peak periods, reducing stress and burn-out. Built into the KNIME solution are important validation checks, which identify and correct mistakes in manual sheets, such as those requiring foreign exchange conversions. KNIME automatically accesses completed reports generated from the system, ensuring the approved uploads have been successfully posted in the ERP - which prevents omissions from occurring. Furthermore, as these validations are built and embedded into the model, the model can be run independently by junior analysts, allowing senior staff to focus on more engaging value-add analyses.\nWhy KNIME?\nKNIME has been embraced by Finance staff due to its simplicity and relevance. Finance tasks rely on structured data from official Finance systems, and well-defined logical sequences of calculations. As an open-source tool, there are no costs for licenses, the community is engaged and supportive, data is secured, and learning resources are abundant.\nWith its no-code visual interface, logical flow, and Excel-synonymous functions, professionals can easily leverage KNIME to automate their spreadsheet processes. KNIME provides an opportunity for Finance staff to build analytical automation tools without IT support, while remaining nimble to changes in business.\nKNIME was introduced to the IFM customer in 2019 through a formal training program, available commercially through Day5 Analytics. The approach was to introduce staff to the tool, challenge them to tackle small-scale Finance problems, and then to demonstrate their solutions across the department \u2013 inspiring others to give it a fair chance. This gradual pressure-testing by Finance staff themselves proved to be a strong motivator for others to learn and apply the tool. Today, a second generation of trainees teaches others, forming an internal community that enables KNIME expertise to flourish.\nWith KNIME deployed as a core tool across the department\u2019s workstations, many traditional Finance and Accounting professionals have learned and applied KNIME to ease their day-to-day roles. With over 50 automation projects delivered, each saving between 2 and 4 weeks of effort on an annualized basis, the customer has realized long-term efficiencies across the board. New projects are now rolled out faster than ever before, and KNIME has driven engagement across the department by shifting the work focus to value-add.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 25,
        "url": "https://www.knime.com/success-story/how-deutsche-telekom-automated-month-end-closing-procedures",
        "title": "How Deutsche Telekom automated month-end closing procedures | KNIME",
        "company": "Deutsche Telekom",
        "content": "Grappling with manual processes during peak workload\nThe digitalization team, headed by Gerrit Lillig, was brought in to evaluate the situation. The problems with the previous process were twofold: On the one hand, collecting the information for the reports from multiple sources was time consuming and potentially error prone. The other problem was the slow pace of the process. Controllers need quick access to meaningful figures and don\u2019t want to waste time sifting through figures manually - particularly during the busy end-of-month period.\nLillig\u2019s team ascertained that both collection of the data as well as access to report comments can be more effectively handled by a workflow that automates the provision of comments and analyses for the monthly closing reports.\n\nDeviations analysis at the click of a button\nControllers can now obtain the underlying meaningful reports quickly without having to search through figures manually. The new process produces technical comments on the compiled figures automatically in a template. After reviewing the comments in the template, controllers can elaborate on this information, thus entering further expertise to the database.\nWith this new process, controllers can access basic, commented reports literally at the click of a button. And now focus on using their time to elaborate on these, adding more insight to the automatically provided comments in the reports, and enhancing the master database with their expertise. This provides Controlling not just with an automated process for comments but also a deviations analysis tool.\nA three-step process using KNIME Analytics Platform\nIn the first step, KNIME is used to read in, aggregate, and create time series from the data. The data consists of monthly actuals compared to forecast and budget figures as well as prior year figures. The hierarchies between individual positions (profit and loss vs parent profit and loss positions) are consolidated and then mapped to the relevant entities in the respective management report. \nThe second step is to take the prepared data from SAP BW, add missing totals from the management report, other residual positions, as well as accumulated values e.g., YTD, QTD.\n In the third step, deviations (absolute and relative) are calculated, filtered, and components for creating comments are created with particular reference to positive and negative deviations. Finally the individual positions are merged and the results are automatically commented. These comments are produced in text form and transferred by a bot (blue prism) to the closing team who review them and elaborate as required. Figure 2, below shows how much of the output i.e., the final commenting is provided by the workflow. Only the comments highlighted in yellow were added afterwards by the closing team. \nWhy KNIME?\nThe availability of KNIME Analytics Platform as a free and open source tool meant that the project could start immediately. There was no need for getting approval for an upfront investment. Due to KNIME\u2019s visual programming environment, it\u2019s not only easy to use but it also enabled the department to develop this solution independently of IT teams. Even with little prior experience in using KNIME, a team of two - together with the closing team - was able to develop a production-ready solution within three months. With the workflow in place, its graphical visualization enables easy handovers and understanding of the process within and across teams.\n\u201cIt was great to see how, with the right approach and tool, we were able to generate a valuable benefit for our closing team in such a short time.\u201d Gerrit Lillig, VP PK Steuerungsinstrumente, Telekom Deutschland GmbH.\nThis solution is currently in operation in controlling for Telekom Deutschland\u2019s private customer business (B2C) closing department. Ruben Flemming from the digitalization team is now about to scale up the solution to run it not only on the financial closing data but also for deviation analysis on sales performance data.\n\u21d2\u00a0Download this Success Story here as a PDF.\n"
    },
    {
        "id": 26,
        "url": "https://www.knime.com/success-story/how-deutsche-telekom-automated-month-end-closing-procedures",
        "title": "How Deutsche Telekom automated month-end closing procedures | KNIME",
        "company": "Deutsche Telekom",
        "content": "Grappling with manual processes during peak workload\nThe digitalization team, headed by Gerrit Lillig, was brought in to evaluate the situation. The problems with the previous process were twofold: On the one hand, collecting the information for the reports from multiple sources was time consuming and potentially error prone. The other problem was the slow pace of the process. Controllers need quick access to meaningful figures and don\u2019t want to waste time sifting through figures manually - particularly during the busy end-of-month period.\nLillig\u2019s team ascertained that both collection of the data as well as access to report comments can be more effectively handled by a workflow that automates the provision of comments and analyses for the monthly closing reports.\n\nDeviations analysis at the click of a button\nControllers can now obtain the underlying meaningful reports quickly without having to search through figures manually. The new process produces technical comments on the compiled figures automatically in a template. After reviewing the comments in the template, controllers can elaborate on this information, thus entering further expertise to the database.\nWith this new process, controllers can access basic, commented reports literally at the click of a button. And now focus on using their time to elaborate on these, adding more insight to the automatically provided comments in the reports, and enhancing the master database with their expertise. This provides Controlling not just with an automated process for comments but also a deviations analysis tool.\nA three-step process using KNIME Analytics Platform\nIn the first step, KNIME is used to read in, aggregate, and create time series from the data. The data consists of monthly actuals compared to forecast and budget figures as well as prior year figures. The hierarchies between individual positions (profit and loss vs parent profit and loss positions) are consolidated and then mapped to the relevant entities in the respective management report. \nThe second step is to take the prepared data from SAP BW, add missing totals from the management report, other residual positions, as well as accumulated values e.g., YTD, QTD.\n In the third step, deviations (absolute and relative) are calculated, filtered, and components for creating comments are created with particular reference to positive and negative deviations. Finally the individual positions are merged and the results are automatically commented. These comments are produced in text form and transferred by a bot (blue prism) to the closing team who review them and elaborate as required. Figure 2, below shows how much of the output i.e., the final commenting is provided by the workflow. Only the comments highlighted in yellow were added afterwards by the closing team. \nWhy KNIME?\nThe availability of KNIME Analytics Platform as a free and open source tool meant that the project could start immediately. There was no need for getting approval for an upfront investment. Due to KNIME\u2019s visual programming environment, it\u2019s not only easy to use but it also enabled the department to develop this solution independently of IT teams. Even with little prior experience in using KNIME, a team of two - together with the closing team - was able to develop a production-ready solution within three months. With the workflow in place, its graphical visualization enables easy handovers and understanding of the process within and across teams.\n\u201cIt was great to see how, with the right approach and tool, we were able to generate a valuable benefit for our closing team in such a short time.\u201d Gerrit Lillig, VP PK Steuerungsinstrumente, Telekom Deutschland GmbH.\nThis solution is currently in operation in controlling for Telekom Deutschland\u2019s private customer business (B2C) closing department. Ruben Flemming from the digitalization team is now about to scale up the solution to run it not only on the financial closing data but also for deviation analysis on sales performance data.\n\u21d2\u00a0Download this Success Story here as a PDF.\n"
    },
    {
        "id": 27,
        "url": "https://www.knime.com/success-story/how-diaceutics-automated-and-streamlined-data-labeling-increase-speed-insights",
        "title": "How Diaceutics automated and streamlined data labeling to increase speed to insights | KNIME",
        "company": "Diaceutics",
        "content": "Streamlining data analysis and enabling domain expert input\nWith so much data available and so many details within the data itself, an approach was needed to streamline the analysis of this data and empower analysts to add their medical knowledge to further enrich it.\nBy using a\u00a0data analytics platform\u00a0like KNIME and taking advantage of all the tools, it's possible to cleanse and label the data and then it in a standard workflow for project-specific analysts to easily use. This saves them time and improves project quality.\nThis specific example shows how Diaceutics implemented logic as well as business rules to label data, and how a\u00a0standard workflow for project use was\u00a0created.\u00a0\nLabeling of clinical data allows data-driven insights\u00a0\nThere are several reasons why patient data needs to be labeled. Primarily, healthcare data is transactional. In this raw form, it offers little insight, from which no data-driven insight can be made. Labeling data appropriately allows insights to be uncovered and provides a cleaner and easier dataset to work with. It also allows the creation of groupings and filters. Not only for project-specific internal analysts to work with, but also for the Diaeceutics DXRX platform, which clients can directly interact with themselves. The data that needs to be labeled varies and includes time point, disease, disease stage, patient history, biomarker tested, and test method.\nFor some of these, the task is to standardize or group the existing data. For example, with time points it\u2019s possible to group a specific data field by year, quarter, and/or month using a simple SQL statement. However, many parts of the data require a new label, created using a combination of logic and business rules - for example, disease stage.\nIn terms of labeling the data, straight-forward data can be hard-coded in SQL. However, for most data, control files and flexible SQL coding is used. In KNIME, linked components are used, which are files that contain all the logic for diseases: stage, biomarkers, methodologies, and business rules. A Build SQL Component builds out the SQL for all combinations\u00a0specified in the control files, or the options that are chosen by the business analyst or on DXRX.\nA project\u00a0typically consists of taking patient-level data and analyzing and aggregating it as appropriate for the client. The initial process involved approaching these on a project-by-project basis. With this method, workflows very quickly got complex and difficult to quality control. As a result, this was time-consuming and difficult for business analysts to inherit and adapt as needed.\u00a0\n\nA standardized, six-step process\u00a0\nWith a standardized approach, there is one agreed-upon method for all projects, and one way to do common client requests. This is easier to use, more consistent, and saves time. It also makes it easier for analysts to work independently and adapt when needed. In the workflow, there are only a few nodes that an analyst needs to interact with to complete their client requests. It keeps the full patient cohort aligned across projects with minimal quality control needed.\u00a0\nConnect to the database where patient-level data is located.\u00a0Pull out column names to create filter options in the menu.\u00a0Introduce control files using linked components.Combine control files and columns from the original data table to create an interactive menu. Here, the user picks the options they need, the SQL code is automatically built out based on the combination the analyst has selected, and any business rules are built as variables for potential further analysis in the database query.\u00a0Variables from step four are implemented into the query, and the analyst can choose to aggregate the data however they need.\u00a0Data is read out and ready for the project team and client.\u00a0Results: Better data, better testing, better treatmentThis project has shown that it\u2019s possible to label healthcare data with many different variables including disease, disease stage, tested Biomarkers, method, and results. Labeled patient data and a standardized process ensures all analysts are working from the same base. Anyone working with the data has the same starting point with same patient cohort and methods. This means data can be analyzed and aggregated at a high level quickly and efficiently. In depth analysis can be performed more easily (when needed) as the patient cohort is readily available. This ultimately leads to better data, better testing, and better treatment.\u00a0\nWhy KNIME?\n\u201cSince starting at\u00a0Diaceutics, KNIME has been an integral part of my everyday work\u201d\u00a0- Isabel Stacey, Senior Data Analyst, Diaceutics.\u00a0\nKNIME\u00a0workflows are easy to build and allow a straightforward way to standardize business processes. All nodes and sections can be annotated, which not only provides a self-documenting workflow, but enables a new user to understand what is happening at each stage. One of the biggest benefits of KNIME is the linked\u00a0component\u00a0functionality. This ensures that changes can be made to the master workflow, and all versions of that downloaded workflow will get a notification warning the user that a change has been made. This also enables\u00a0version control of the workflows, by using snapshots on the\u00a0KNIME Server.\u00a0\nFrom a business perspective, this solution has highlighted how easy it is to scale with standardized workflows, without the risk of having different analysts interpreting different results out of the workflows. With the evolution of the data lake, KNIME, and ETL processes, project throughput has increased significantly \u2013 specifically moving from an Excel-based approach to this more standardized, streamlined approach.\u00a0\n"
    },
    {
        "id": 28,
        "url": "https://www.knime.com/success-story/how-diaceutics-automated-and-streamlined-data-labeling-increase-speed-insights",
        "title": "How Diaceutics automated and streamlined data labeling to increase speed to insights | KNIME",
        "company": "Diaceutics",
        "content": "Streamlining data analysis and enabling domain expert input\nWith so much data available and so many details within the data itself, an approach was needed to streamline the analysis of this data and empower analysts to add their medical knowledge to further enrich it.\nBy using a\u00a0data analytics platform\u00a0like KNIME and taking advantage of all the tools, it's possible to cleanse and label the data and then it in a standard workflow for project-specific analysts to easily use. This saves them time and improves project quality.\nThis specific example shows how Diaceutics implemented logic as well as business rules to label data, and how a\u00a0standard workflow for project use was\u00a0created.\u00a0\nLabeling of clinical data allows data-driven insights\u00a0\nThere are several reasons why patient data needs to be labeled. Primarily, healthcare data is transactional. In this raw form, it offers little insight, from which no data-driven insight can be made. Labeling data appropriately allows insights to be uncovered and provides a cleaner and easier dataset to work with. It also allows the creation of groupings and filters. Not only for project-specific internal analysts to work with, but also for the Diaeceutics DXRX platform, which clients can directly interact with themselves. The data that needs to be labeled varies and includes time point, disease, disease stage, patient history, biomarker tested, and test method.\nFor some of these, the task is to standardize or group the existing data. For example, with time points it\u2019s possible to group a specific data field by year, quarter, and/or month using a simple SQL statement. However, many parts of the data require a new label, created using a combination of logic and business rules - for example, disease stage.\nIn terms of labeling the data, straight-forward data can be hard-coded in SQL. However, for most data, control files and flexible SQL coding is used. In KNIME, linked components are used, which are files that contain all the logic for diseases: stage, biomarkers, methodologies, and business rules. A Build SQL Component builds out the SQL for all combinations\u00a0specified in the control files, or the options that are chosen by the business analyst or on DXRX.\nA project\u00a0typically consists of taking patient-level data and analyzing and aggregating it as appropriate for the client. The initial process involved approaching these on a project-by-project basis. With this method, workflows very quickly got complex and difficult to quality control. As a result, this was time-consuming and difficult for business analysts to inherit and adapt as needed.\u00a0\n\nA standardized, six-step process\u00a0\nWith a standardized approach, there is one agreed-upon method for all projects, and one way to do common client requests. This is easier to use, more consistent, and saves time. It also makes it easier for analysts to work independently and adapt when needed. In the workflow, there are only a few nodes that an analyst needs to interact with to complete their client requests. It keeps the full patient cohort aligned across projects with minimal quality control needed.\u00a0\nConnect to the database where patient-level data is located.\u00a0Pull out column names to create filter options in the menu.\u00a0Introduce control files using linked components.Combine control files and columns from the original data table to create an interactive menu. Here, the user picks the options they need, the SQL code is automatically built out based on the combination the analyst has selected, and any business rules are built as variables for potential further analysis in the database query.\u00a0Variables from step four are implemented into the query, and the analyst can choose to aggregate the data however they need.\u00a0Data is read out and ready for the project team and client.\u00a0Results: Better data, better testing, better treatmentThis project has shown that it\u2019s possible to label healthcare data with many different variables including disease, disease stage, tested Biomarkers, method, and results. Labeled patient data and a standardized process ensures all analysts are working from the same base. Anyone working with the data has the same starting point with same patient cohort and methods. This means data can be analyzed and aggregated at a high level quickly and efficiently. In depth analysis can be performed more easily (when needed) as the patient cohort is readily available. This ultimately leads to better data, better testing, and better treatment.\u00a0\nWhy KNIME?\n\u201cSince starting at\u00a0Diaceutics, KNIME has been an integral part of my everyday work\u201d\u00a0- Isabel Stacey, Senior Data Analyst, Diaceutics.\u00a0\nKNIME\u00a0workflows are easy to build and allow a straightforward way to standardize business processes. All nodes and sections can be annotated, which not only provides a self-documenting workflow, but enables a new user to understand what is happening at each stage. One of the biggest benefits of KNIME is the linked\u00a0component\u00a0functionality. This ensures that changes can be made to the master workflow, and all versions of that downloaded workflow will get a notification warning the user that a change has been made. This also enables\u00a0version control of the workflows, by using snapshots on the\u00a0KNIME Server.\u00a0\nFrom a business perspective, this solution has highlighted how easy it is to scale with standardized workflows, without the risk of having different analysts interpreting different results out of the workflows. With the evolution of the data lake, KNIME, and ETL processes, project throughput has increased significantly \u2013 specifically moving from an Excel-based approach to this more standardized, streamlined approach.\u00a0\n"
    },
    {
        "id": 29,
        "url": "https://www.knime.com/success-story/how-epam-built-recommendation-engine-brand-portfolio-decision-making",
        "title": "How EPAM built a recommendation engine for brand portfolio decision making | KNIME",
        "company": "EPAM",
        "content": "The challenge: optimize merchandise levels\nA trial and error approach to optimizing merchandise levels for a single retail store is inefficient and ineffective. To reduce the amount of unsold stock, Merchandising Managers and Brand Portfolio Managers must have a good idea of what will sell and what will not. Capitalizing on regional data to predict brands or products with higher sales potential puts any retail company on a better path.\nThe solution: Recommendation engine and Guided Analytics\nA data science team uses\u00a0data analytics platform, KNIME, to create a solution\u00a0similar to collaborative filtering. It makes automatic predictions of individual customer interests by collecting preferences from many customers. The workflow starts with ETL and other data preparation steps, before creating a recommendation engine, and lastly through to determining interaction points for the Analytical Application. The workflow is then deployed on\u00a0KNIME Server\u00a0as a\u00a0Guided Analytics\u00a0Application.\n\u21d2\u00a0Download Workflow from KNIME Hub\nWhy KNIME?\nTasks like ETL and data prep require a certain degree of technical knowledge - as does creating a recommendation engine. In this case, data scientists can focus on creating and deploying an Analytical Application from which Merchandising and Brand Portfolio Managers can draw insights and make decisions.\n\u21d2\u00a0Download this Innovation Note\n"
    },
    {
        "id": 30,
        "url": "https://www.knime.com/success-story/how-epam-built-recommendation-engine-brand-portfolio-decision-making",
        "title": "How EPAM built a recommendation engine for brand portfolio decision making | KNIME",
        "company": "EPAM",
        "content": "The challenge: optimize merchandise levels\nA trial and error approach to optimizing merchandise levels for a single retail store is inefficient and ineffective. To reduce the amount of unsold stock, Merchandising Managers and Brand Portfolio Managers must have a good idea of what will sell and what will not. Capitalizing on regional data to predict brands or products with higher sales potential puts any retail company on a better path.\nThe solution: Recommendation engine and Guided Analytics\nA data science team uses\u00a0data analytics platform, KNIME, to create a solution\u00a0similar to collaborative filtering. It makes automatic predictions of individual customer interests by collecting preferences from many customers. The workflow starts with ETL and other data preparation steps, before creating a recommendation engine, and lastly through to determining interaction points for the Analytical Application. The workflow is then deployed on\u00a0KNIME Server\u00a0as a\u00a0Guided Analytics\u00a0Application.\n\u21d2\u00a0Download Workflow from KNIME Hub\nWhy KNIME?\nTasks like ETL and data prep require a certain degree of technical knowledge - as does creating a recommendation engine. In this case, data scientists can focus on creating and deploying an Analytical Application from which Merchandising and Brand Portfolio Managers can draw insights and make decisions.\n\u21d2\u00a0Download this Innovation Note\n"
    },
    {
        "id": 31,
        "url": "https://www.knime.com/success-story/how-fda-improved-speed-accuracy-staff-forecasting-knime",
        "title": "How the FDA improved speed & accuracy of staff forecasting with KNIME | KNIME",
        "company": "FDA",
        "content": "The meticulous process of getting the perfect fully loaded cost model\nAs a part of its regulatory functions, the FDA collects over $3 billion a year in program administration fees from over 20,000 industry partners that produce regulated products, such as drugs and medical devices, and from some other entities, such as certain accreditation and certification bodies across the world to fund its operational needs. These fees are called \u201cuser fees.\u201d However, managing this process - with intense checks and balances in place - was a complex and time-consuming task that was handled manually using Excel spreadsheets, complex macros, and significant federal resources.\nOne of the key pain points for the FDA, within the various processes related to collecting user fees, was the Fully Loaded Cost (FLC) model. The FLC model is a valuable asset for the FDA when it comes to determining user fees. It helps them calculate how much funding they need to hire full-time staff to support the FDA\u2019s critical mission. This data is also used for capacity planning and to evaluate the cost of business cases internally. The FDA keeps the model up-to-date by using the latest financial data from their system, so they can set the right fees and keep operations running smoothly while bringing in the necessary revenue to cover operational expenditures.\u00a0But the process of manually updating the model using Excel was time-consuming and error-prone, especially with data coming in from various disparate sources - the obligation data from the financial system, reports from the budget planning system, and the consumer price index data from the US Department of Labor. The FDA\u2019s user fee team spent countless hours in Excel performing manual calculations, custom coding macros, and collaborating over multiple spreadsheets to develop the FLC model. The process often took as long as 3 months per year and spanned millions of rows to arrive at the exact obligation that the FDA incurred in that year to keep the American public health protected.\nThe error-prone nature of the Excel-based model and the lack of version control resulted in significant re-work for the user fee team. The ability to drill down into the millions of rows and process large datasets was limited, making it difficult to hone in on the exact cost center they were trying to project in the industry negotiations. The Excel-based macros generated FLC projections manually each time. All executive dashboards were created by the team from scratch and lacked any interactive data visualization. Additionally, they were unable to broaden their analyses with the systemic use of business data from other applications.\nFrom 3 months to one click: The power of automating with KNIME\nThe Deputy Director of the Office of Financial Management (OFM) at the FDA was looking for a way to upgrade and modernize these processes to make them more efficient. After evaluating multiple options, the FDA decided to use KNIME, a powerful data analytics platform that is well-suited to automating complex business processes.\nWith help from KNIME\u2019s partner Equinoxys, the FDA put KNIME in action to import data from various formats and data sources and perform current as well as historical data analysis with minimal manual intervention. KNIME's workflows allowed the FDA to automate the FLC model, and its ability to integrate with other applications like the FDA's Oracle-based budget applications made it a seamless fit for the FDA's environment.\nUsing KNIME, the automation of the FLC model was completed with only 3 dedicated resources within 3 months. Coincidentally, the same amount of time was spent each year in the past to accomplish this major lift. Since then, the FDA has saved hundreds of labor hours with the ability to generate cost models in minutes. By automating data processing with KNIME workflows, they are able to eliminate errors and drill-down on large datasets effortlessly regardless of the size.\u00a0Additionally, KNIME's advanced analytical dashboards provide FDA executives with the ability to make fact-based decisions using what-if analyses and scorecards with financial metrics. These analyses are further bolstered by near real-time business data from other OFM applications.\nAs a result of implementing KNIME, the FDA has been able to improve the speed and accuracy of staff forecasting and reduce the time and resources required to manage the user fee process. This has allowed the FDA to focus more on their core mission of protecting public health and enhancing regulatory processes.\nHaving witnessed the substantial benefits of automating the FLC model with KNIME, the FDA is now working on implementing a one-stop-shop to handle all user fee business processes in an automated manner using KNIME such as the five-year financial plan, annual financial reports, federal notices, business case management, undelivered orders monitoring, and much more.\n"
    },
    {
        "id": 32,
        "url": "https://www.knime.com/success-story/how-fda-improved-speed-accuracy-staff-forecasting-knime",
        "title": "How the FDA improved speed & accuracy of staff forecasting with KNIME | KNIME",
        "company": "FDA",
        "content": "The meticulous process of getting the perfect fully loaded cost model\nAs a part of its regulatory functions, the FDA collects over $3 billion a year in program administration fees from over 20,000 industry partners that produce regulated products, such as drugs and medical devices, and from some other entities, such as certain accreditation and certification bodies across the world to fund its operational needs. These fees are called \u201cuser fees.\u201d However, managing this process - with intense checks and balances in place - was a complex and time-consuming task that was handled manually using Excel spreadsheets, complex macros, and significant federal resources.\nOne of the key pain points for the FDA, within the various processes related to collecting user fees, was the Fully Loaded Cost (FLC) model. The FLC model is a valuable asset for the FDA when it comes to determining user fees. It helps them calculate how much funding they need to hire full-time staff to support the FDA\u2019s critical mission. This data is also used for capacity planning and to evaluate the cost of business cases internally. The FDA keeps the model up-to-date by using the latest financial data from their system, so they can set the right fees and keep operations running smoothly while bringing in the necessary revenue to cover operational expenditures.\u00a0But the process of manually updating the model using Excel was time-consuming and error-prone, especially with data coming in from various disparate sources - the obligation data from the financial system, reports from the budget planning system, and the consumer price index data from the US Department of Labor. The FDA\u2019s user fee team spent countless hours in Excel performing manual calculations, custom coding macros, and collaborating over multiple spreadsheets to develop the FLC model. The process often took as long as 3 months per year and spanned millions of rows to arrive at the exact obligation that the FDA incurred in that year to keep the American public health protected.\nThe error-prone nature of the Excel-based model and the lack of version control resulted in significant re-work for the user fee team. The ability to drill down into the millions of rows and process large datasets was limited, making it difficult to hone in on the exact cost center they were trying to project in the industry negotiations. The Excel-based macros generated FLC projections manually each time. All executive dashboards were created by the team from scratch and lacked any interactive data visualization. Additionally, they were unable to broaden their analyses with the systemic use of business data from other applications.\nFrom 3 months to one click: The power of automating with KNIME\nThe Deputy Director of the Office of Financial Management (OFM) at the FDA was looking for a way to upgrade and modernize these processes to make them more efficient. After evaluating multiple options, the FDA decided to use KNIME, a powerful data analytics platform that is well-suited to automating complex business processes.\nWith help from KNIME\u2019s partner Equinoxys, the FDA put KNIME in action to import data from various formats and data sources and perform current as well as historical data analysis with minimal manual intervention. KNIME's workflows allowed the FDA to automate the FLC model, and its ability to integrate with other applications like the FDA's Oracle-based budget applications made it a seamless fit for the FDA's environment.\nUsing KNIME, the automation of the FLC model was completed with only 3 dedicated resources within 3 months. Coincidentally, the same amount of time was spent each year in the past to accomplish this major lift. Since then, the FDA has saved hundreds of labor hours with the ability to generate cost models in minutes. By automating data processing with KNIME workflows, they are able to eliminate errors and drill-down on large datasets effortlessly regardless of the size.\u00a0Additionally, KNIME's advanced analytical dashboards provide FDA executives with the ability to make fact-based decisions using what-if analyses and scorecards with financial metrics. These analyses are further bolstered by near real-time business data from other OFM applications.\nAs a result of implementing KNIME, the FDA has been able to improve the speed and accuracy of staff forecasting and reduce the time and resources required to manage the user fee process. This has allowed the FDA to focus more on their core mission of protecting public health and enhancing regulatory processes.\nHaving witnessed the substantial benefits of automating the FLC model with KNIME, the FDA is now working on implementing a one-stop-shop to handle all user fee business processes in an automated manner using KNIME such as the five-year financial plan, annual financial reports, federal notices, business case management, undelivered orders monitoring, and much more.\n"
    },
    {
        "id": 33,
        "url": "https://www.knime.com/success-story/how-forest-grove-built-end-end-automated-inventory-management-solution",
        "title": "How Forest Grove built an end-to-end, automated inventory management solution | KNIME",
        "company": "Forest Grove",
        "content": "The challenge: replace an inefficient, cumbersome, manual two-week process\nAs Australia\u2019s largest motor dealership group, Automotive Holdings Group (AHG) owns over 180 car and truck franchises and offers a wide range of services including \ufb01nance, after care, insurance and equipment. The National Trucks Division regularly undertakes analysis to identify opportunities to transfer spare parts between dealerships in order to minimize the write-downs that occur when parts age without being utilized. Historically the analysis was completed on a monthly basis via an inefficient, cumbersome and manual two-week process that originated with CSV extracts. A scalable, flexible, transparent, and easy-to-update solution was needed, which also signi\ufb01cantly accelerated the time to insight.\nThe solution: An end-to-end, automated data science solution\nForest Grove used KNIME to develop an end-to-end automated data wrangling and reporting engine to expand AHG\u2019s capabilities in deep analysis through the process of data extraction, transformation, reporting and visualization. The solution is scalable, flexible and transparent, and can be easily updated to suit changes to the data environment and user requirements. It comprises \ufb01ve components:\nAutomation of data aggregation from the dealer management systemCleansing, blending, and transforming raw dataStorage of raw data in a cloud data warehouseCreation and update of warehouse tables for reportsPresentation of data in interactive and scheduled dashboards\nResults\nThis solution has vastly expanded AHG\u2019s deep analysis capabilties and represents a major milestone in its journey from descriptive to predictive analytics. By automating the data interpretation process with KNIME, managers are now able to identify potential transfer opportunities in minutes rather than weeks through:\nSingle data repository for all reportsScheduled data extraction and aggregation process with detailed governance (validation) checksSigni\ufb01cantly accelerated time to insight with an infrastructure solution that supports deeper analytical dives\n\nWhy KNIME?\nThe open nature of\u00a0KNIME Analytics Platform\u00a0made the development, access, and management of the solution much easier as integrations with other technologies were made possible. Specifically\u00a0Snowflake\u00a0for cloud-based data storage and\u00a0PowerBI\u00a0for the dashboards.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 34,
        "url": "https://www.knime.com/success-story/how-forest-grove-built-end-end-automated-inventory-management-solution",
        "title": "How Forest Grove built an end-to-end, automated inventory management solution | KNIME",
        "company": "Forest Grove",
        "content": "The challenge: replace an inefficient, cumbersome, manual two-week process\nAs Australia\u2019s largest motor dealership group, Automotive Holdings Group (AHG) owns over 180 car and truck franchises and offers a wide range of services including \ufb01nance, after care, insurance and equipment. The National Trucks Division regularly undertakes analysis to identify opportunities to transfer spare parts between dealerships in order to minimize the write-downs that occur when parts age without being utilized. Historically the analysis was completed on a monthly basis via an inefficient, cumbersome and manual two-week process that originated with CSV extracts. A scalable, flexible, transparent, and easy-to-update solution was needed, which also signi\ufb01cantly accelerated the time to insight.\nThe solution: An end-to-end, automated data science solution\nForest Grove used KNIME to develop an end-to-end automated data wrangling and reporting engine to expand AHG\u2019s capabilities in deep analysis through the process of data extraction, transformation, reporting and visualization. The solution is scalable, flexible and transparent, and can be easily updated to suit changes to the data environment and user requirements. It comprises \ufb01ve components:\nAutomation of data aggregation from the dealer management systemCleansing, blending, and transforming raw dataStorage of raw data in a cloud data warehouseCreation and update of warehouse tables for reportsPresentation of data in interactive and scheduled dashboards\nResults\nThis solution has vastly expanded AHG\u2019s deep analysis capabilties and represents a major milestone in its journey from descriptive to predictive analytics. By automating the data interpretation process with KNIME, managers are now able to identify potential transfer opportunities in minutes rather than weeks through:\nSingle data repository for all reportsScheduled data extraction and aggregation process with detailed governance (validation) checksSigni\ufb01cantly accelerated time to insight with an infrastructure solution that supports deeper analytical dives\n\nWhy KNIME?\nThe open nature of\u00a0KNIME Analytics Platform\u00a0made the development, access, and management of the solution much easier as integrations with other technologies were made possible. Specifically\u00a0Snowflake\u00a0for cloud-based data storage and\u00a0PowerBI\u00a0for the dashboards.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 35,
        "url": "https://www.knime.com/success-story/how-global-cyber-security-team-saves-hundreds-hours-month-knime",
        "title": "How a global cyber security team saves hundreds of hours per month with KNIME | KNIME",
        "company": "Gemmacon",
        "content": "An Excel alternative saving 150+ hours per month\nPreviously, one employee was responsible for this report, which included collecting and integrating the data, computing all KPIs, and visualizing and distributing the results. This was done entirely in Excel and required one month to be completed. The biggest pain points were the inability for Excel to handle the large amounts of data (often crashing mid computation) as well as the manual process, which had a precise order of steps that needed to be taken and was extremely error-prone.\nFor one year, the team tried to automate the processing and delivery of the KPIs in a dashboard using IBM Congos. However, this was resource-intensive and didn\u2019t meet the desired speed of development, nor the desired level of interactivity within the visualizations. The current solution, built by\u00a0KNIME Partner Gemmacon, uses\u00a0KNIME Analytics Platform,\u00a0KNIME Server, and PowerBI (via the\u00a0native KNIME PowerBI integration).\nAs the first step, the current, rather simple data transformation processes, which are currently done using Excel, are reproduced as KNIME workflows. Each of the six teams has its own KPIs and gets their own dashboard. Each workflow combines two to four data sources such as data extracts from proprietary software and they use data transformation and integration nodes such as the\u00a0Rule Engine node\u00a0for rule-based classification and group assignment of data rows such as security incidents, assignment of target values, and deletion/censoring of sensitive data. The\u00a0Date&Time nodes\u00a0are used to enable time filtering and the\u00a0Math Formula node\u00a0for calculating KPIs.\nCombining KNIME and PowerBI for efficient KPI dashboards\nOne advantage of performing the data transformation in KNIME instead of directly in PowerBI, is that transformation in KNIME is more straight forward and comprehensible than with DAX Formulas in PowerBI. Whenever a new calculation is needed, it\u2019s implemented in the KNIME workflow and the data source is automatically updated in PowerBI. The workflow results are collected in a database, meaning the dashboard views present both the current month\u2019s data as well as historical data. Dashboards in PowerBI are designed and queried using the data out of the database. In a second step, a KNIME Server Small, installed on an internal IT-managed server, hosts all the workflows where they are also executed automatically.\nResults\nThe dashboard replaces a report in Excel/Power Point and is\u00a0updated daily instead of monthly\u00a0\u2013 making the current status available to any user on demand.The effort needed for updating the KPIs was\u00a0reduced from 160 hours per month to ten hours per month.Users have access to more information than previously because they have the option to drill down into the visualization if and when needed.Due to the fixed process for how data is handled in a KNIME workflow and what it must look like, neighboring processes are now also standardized. This means colleagues can no longer handle the data in their own way, which has increased reliability and improved processes.More time is now available to implement additional KPIs because the processing limit is no longer measured by the monthly working hours of one person, and most dictionaries and basic data structures are already available.\nWhy KNIME?\nFrom a software and technology perspective, the biggest advantage of KNIME is the visual workflow builder and self-documenting workflows in\u00a0KNIME Analytics Platform. This not only documents important knowledge, but also makes any KNIME workflow comprehensible - and usable - to a new user. The gentle learning curve enables even those with minimal data science experience to understand what\u2019s happening with the data at any point in the workflow. KNIME workflows don\u2019t require external development, which removes the need for specialized software setups and expert knowledge, and enables the team to independently make any changes they want/need. KNIME offers extensions and integrations with many other open source and commercial tools. This enables data scientists who are building or adjusting the workflow to continue working with the tools that they know and like \u2013 in this case exporting visualizations to\u00a0PowerBI.\u00a0KNIME Server\u00a0is a valuable addition because it supports the automation of the entire process, thereby freeing up a team member who no longer has to manually execute the workflow and send the results to the team.\nGemmacon, a KNIME Partner, was brought on board after delays and difficulties with the previous solution, and because a solution was needed to communicate to all users and stakeholders at an internal, global event. The project deliverables, which consisted of six KNIME workflows and PowerBI dashboards, were completed in one month. More are to be developed by the cybersecurity department, who has been enabled and empowered to do this independently. Next steps in the project include removing the need for human effort at any step of the process by, for example, automatically extracting the data from the proprietary software as well as emails and text files. This will enable those team members to focus on more value-adding tasks and projects.\nThis Success Story is available for download here as a PDF.\n"
    },
    {
        "id": 36,
        "url": "https://www.knime.com/success-story/how-global-cyber-security-team-saves-hundreds-hours-month-knime",
        "title": "How a global cyber security team saves hundreds of hours per month with KNIME | KNIME",
        "company": "Gemmacon",
        "content": "An Excel alternative saving 150+ hours per month\nPreviously, one employee was responsible for this report, which included collecting and integrating the data, computing all KPIs, and visualizing and distributing the results. This was done entirely in Excel and required one month to be completed. The biggest pain points were the inability for Excel to handle the large amounts of data (often crashing mid computation) as well as the manual process, which had a precise order of steps that needed to be taken and was extremely error-prone.\nFor one year, the team tried to automate the processing and delivery of the KPIs in a dashboard using IBM Congos. However, this was resource-intensive and didn\u2019t meet the desired speed of development, nor the desired level of interactivity within the visualizations. The current solution, built by\u00a0KNIME Partner Gemmacon, uses\u00a0KNIME Analytics Platform,\u00a0KNIME Server, and PowerBI (via the\u00a0native KNIME PowerBI integration).\nAs the first step, the current, rather simple data transformation processes, which are currently done using Excel, are reproduced as KNIME workflows. Each of the six teams has its own KPIs and gets their own dashboard. Each workflow combines two to four data sources such as data extracts from proprietary software and they use data transformation and integration nodes such as the\u00a0Rule Engine node\u00a0for rule-based classification and group assignment of data rows such as security incidents, assignment of target values, and deletion/censoring of sensitive data. The\u00a0Date&Time nodes\u00a0are used to enable time filtering and the\u00a0Math Formula node\u00a0for calculating KPIs.\nCombining KNIME and PowerBI for efficient KPI dashboards\nOne advantage of performing the data transformation in KNIME instead of directly in PowerBI, is that transformation in KNIME is more straight forward and comprehensible than with DAX Formulas in PowerBI. Whenever a new calculation is needed, it\u2019s implemented in the KNIME workflow and the data source is automatically updated in PowerBI. The workflow results are collected in a database, meaning the dashboard views present both the current month\u2019s data as well as historical data. Dashboards in PowerBI are designed and queried using the data out of the database. In a second step, a KNIME Server Small, installed on an internal IT-managed server, hosts all the workflows where they are also executed automatically.\nResults\nThe dashboard replaces a report in Excel/Power Point and is\u00a0updated daily instead of monthly\u00a0\u2013 making the current status available to any user on demand.The effort needed for updating the KPIs was\u00a0reduced from 160 hours per month to ten hours per month.Users have access to more information than previously because they have the option to drill down into the visualization if and when needed.Due to the fixed process for how data is handled in a KNIME workflow and what it must look like, neighboring processes are now also standardized. This means colleagues can no longer handle the data in their own way, which has increased reliability and improved processes.More time is now available to implement additional KPIs because the processing limit is no longer measured by the monthly working hours of one person, and most dictionaries and basic data structures are already available.\nWhy KNIME?\nFrom a software and technology perspective, the biggest advantage of KNIME is the visual workflow builder and self-documenting workflows in\u00a0KNIME Analytics Platform. This not only documents important knowledge, but also makes any KNIME workflow comprehensible - and usable - to a new user. The gentle learning curve enables even those with minimal data science experience to understand what\u2019s happening with the data at any point in the workflow. KNIME workflows don\u2019t require external development, which removes the need for specialized software setups and expert knowledge, and enables the team to independently make any changes they want/need. KNIME offers extensions and integrations with many other open source and commercial tools. This enables data scientists who are building or adjusting the workflow to continue working with the tools that they know and like \u2013 in this case exporting visualizations to\u00a0PowerBI.\u00a0KNIME Server\u00a0is a valuable addition because it supports the automation of the entire process, thereby freeing up a team member who no longer has to manually execute the workflow and send the results to the team.\nGemmacon, a KNIME Partner, was brought on board after delays and difficulties with the previous solution, and because a solution was needed to communicate to all users and stakeholders at an internal, global event. The project deliverables, which consisted of six KNIME workflows and PowerBI dashboards, were completed in one month. More are to be developed by the cybersecurity department, who has been enabled and empowered to do this independently. Next steps in the project include removing the need for human effort at any step of the process by, for example, automatically extracting the data from the proprietary software as well as emails and text files. This will enable those team members to focus on more value-adding tasks and projects.\nThis Success Story is available for download here as a PDF.\n"
    },
    {
        "id": 37,
        "url": "https://www.knime.com/success-story/why-hirerights-hr-team-uses-knime-increase-accuracy-background-checks",
        "title": "Why HireRight's HR team uses KNIME to increase accuracy on background checks | KNIME",
        "company": "HireRight",
        "content": "Rapidly develop and refine workflows\nThe features and capabilities of KNIME, in comparison to other vendors, better meet the needs of HireRight. For example, the ability to rapidly develop and refine a workflow based on stakeholder feedback is essential for getting functionality into production on a complex platform. The ETL capabilities of KNIME make it easy to interact with databases across multiple technology stacks, and the machine learning capabilities empower the data science team to implement robust processes that would otherwise require significant development resources.\nComplex models made manageable\nKNIME makes it possible for complex models to be created and managed by a single data scientist. It makes large projects like this feasible and manageable with a very small team \u2013 what would typically be viewed as a complicated project can now be completed with only a handful of nodes in\u00a0KNIME Analytics Platform.\nNo upfront limitations\nThe free, open source\u00a0KNIME Analytics Platform\u00a0allows users across the organization to access the platform without going through a procurement process. On top of this, there are no limits the number of data rows being used in any version, meaning no up-front limitations. This enabled HireRight to build tangible and valuable solutions and helped justify the purchase of\u00a0KNIME Server, which is an excellent solution for scheduling workflows on a regular basis and at a reasonable\u00a0cost.\nAppreciated by all stakeholders\nFrom a business perspective, all stakeholders appreciate and value the features that make KNIME stand out from other vendors \u2013 particularly the ability to alter the structure of a model quickly and edit workflows in real-time. HireRight also developed connections between KNIME Server and its data centers across the world, making it possible to access transactional and warehoused data regardless of the technology stack.\n"
    },
    {
        "id": 38,
        "url": "https://www.knime.com/success-story/why-hirerights-hr-team-uses-knime-increase-accuracy-background-checks",
        "title": "Why HireRight's HR team uses KNIME to increase accuracy on background checks | KNIME",
        "company": "HireRight",
        "content": "Rapidly develop and refine workflows\nThe features and capabilities of KNIME, in comparison to other vendors, better meet the needs of HireRight. For example, the ability to rapidly develop and refine a workflow based on stakeholder feedback is essential for getting functionality into production on a complex platform. The ETL capabilities of KNIME make it easy to interact with databases across multiple technology stacks, and the machine learning capabilities empower the data science team to implement robust processes that would otherwise require significant development resources.\nComplex models made manageable\nKNIME makes it possible for complex models to be created and managed by a single data scientist. It makes large projects like this feasible and manageable with a very small team \u2013 what would typically be viewed as a complicated project can now be completed with only a handful of nodes in\u00a0KNIME Analytics Platform.\nNo upfront limitations\nThe free, open source\u00a0KNIME Analytics Platform\u00a0allows users across the organization to access the platform without going through a procurement process. On top of this, there are no limits the number of data rows being used in any version, meaning no up-front limitations. This enabled HireRight to build tangible and valuable solutions and helped justify the purchase of\u00a0KNIME Server, which is an excellent solution for scheduling workflows on a regular basis and at a reasonable\u00a0cost.\nAppreciated by all stakeholders\nFrom a business perspective, all stakeholders appreciate and value the features that make KNIME stand out from other vendors \u2013 particularly the ability to alter the structure of a model quickly and edit workflows in real-time. HireRight also developed connections between KNIME Server and its data centers across the world, making it possible to access transactional and warehoused data regardless of the technology stack.\n"
    },
    {
        "id": 39,
        "url": "https://www.knime.com/success-story/why-ing-upskilled-auditors-use-knime-data-analytics-process-mining",
        "title": "Why ING upskilled auditors to use KNIME for data analytics & process mining | KNIME",
        "company": "ING",
        "content": "Taking Extra Effort Out of Internal Auditing\u00a0\nInternal audit covers many different topics, and often the same topic is addressed in different audits, for instance in different countries. From a data analytics perspective, this means that different audit tests for different topics may be required. It can also mean that the same audit tests may be reused for similar topics but using different data sources. The complexity of analytics for internal audit lies therefore not in the tooling or use of advanced models, but rather in the wide range of topics with multiple and varying data sources. The biggest challenge is finding a way to handle different data types and sources while creating efficiencies and ensuring consistency across all applications.\nING/Internal Audit\u2019s goal is to make data analytics an indispensable part of every audit. The approach is for all Internal Audit staff to become proficient at applying analytics to their audits. An expert team performs advanced analytics and provides support and training. The approach, both from an audit and a staff perspective, requires a data analytics tool that is user-friendly and intuitive, has low (or no) code, supports reusability and sharing, and is flexible and scalable. Because KNIME ticks these boxes, it became the tool of choice for Internal Audit at ING.\nKNIME was first made available in 2017. Since 2020, it has been used not just for its additional computational power and storage, but also for building data apps, which has made working with data and doing analytics even easier.\nAutomating Review Work\nStopping terrorist financing, fighting economic crime, and complying with sanctions are major concerns for the banking industry. It\u2019s important to know your customer and properly screen transactions so you can comply. One of ING\u2019s daily tasks is to review payment transactions for short-term financing, which comes with a lot of trade finance documentation. Documents can be sent in multiple formats, such as invoices, contracts, loan documents, etc. The task is to review the entities on these documents (which could be human names or port/ship names) and cross-reference them with a sanctions list.\u00a0\nPart of the procedures when performing an audit on this topic is re-performing the screening process. Prior to discovering KNIME, the auditors\u00a0 needed to manually read each document, highlight all names or entities and compare the outcome with the lists prepared by the ING analysts.\u00a0\nUsing KNIME Analytics Platform, ING/Internal Audit built workflows to develop a tool that could automatically scan, read, and cross-reference documentation. This ensured completeness of optical character recognition (OCR) for each image. As a company, they must thoroughly and accurately scan 100% of documents going through review. If, for instance, only 90% have been scanned properly, they must identify which 10% were not and which 90% were. And this is where their KNIME-built workflow comes into play.\u00a0\nThis intuitive tool automatically reads a PDF and produces a color-coded result, indicating any words that were missed during scanning and any information/data flagged at a low confidence level of being scanned properly (based on OCR). Then, the tool scans all names and words on the document and compares them to the sanctions list. Therefore, instead of manually scanning over many pages by hand, the audit team only has to focus on the highlighted words indicated by the tool.\nCreating a Reusable Blueprint for Process Mining\u00a0\nProcess mining is used to analyze process flows based on event log data. In many current business processes, IT applications create log files, which register who is doing what and when for every transaction or trade. An event log consists of three important elements: a case ID, the activity itself, and the timestamp. It\u2019s usually enriched with additional data, such as user IDs and departments. It then visualizes the process while it\u2019s taking place \u2014 also known as the \u201cas-is\u201d process. In an audit, the main use case is process discovery. The \u201cas-is\u201d process is benchmarked against the \u201cshould-be\u201d process, which enables auditors to identify non-compliance quickly.\nThe solution at ING/Internal Audit consists of a workflow, which is built and deployed with KNIME as a branded web-based data app. The user opens the data app in their browser, selects the process mining solution, and follows the predetermined steps. These steps include uploading event log data, assigning columns, and conducting the analysis. The purpose-built but easy-to-follow interface offers the right amount of complexity for auditors to conduct their own data analysis anywhere, anytime. The interaction points are all pre-determined by the KNIME workflow, which is running in the background. The workflow consists of many KNIME components, which bundles the workflow functionality into logical, reusable steps.\nUsing KNIME, the team can process insights based on actual data, enable conformance checking, identify bottlenecks, and overall, optimize and simplify process mining efforts.\nING\u2019s Internal Audit Efficiency Gains\u00a0\nING/Internal Audit has managed to integrate data analytics into the everyday work of their auditors, without a lot of heavy lifting, which has gone a long way in helping achieve their vision of becoming data driven. Increasingly, users who didn\u2019t dive into the data before can now generate insights self-sufficiently and faster than before. Because KNIME workflows are reusable and shareable, it\u2019s helped to standardize processes and generate efficiencies at ING. While these efficiencies do come after investing significant time in creating reusable solutions, it\u2019s already paying off. In some cases, it\u2019s reduced the work of individual auditors from three days to 15 minutes.\nTo hear Tjasse Biewenga, head of data analytics for internal audit at ING, talk about these efficiency gains and leveraging KNIME for data analytics, watch his full presentation at the 2022 KNIME Fall Summit.\u00a0To hear more success stories like ING\u2019s and learn how other businesses and data experts across multiple industries are leveraging their data with KNIME, register for our upcoming Spring Summit in Berlin, April 17-19. Early bird tickets are still available.\n\u21d2\u00a0Download this Success Story as a PDF here.\n"
    },
    {
        "id": 40,
        "url": "https://www.knime.com/success-story/why-ing-upskilled-auditors-use-knime-data-analytics-process-mining",
        "title": "Why ING upskilled auditors to use KNIME for data analytics & process mining | KNIME",
        "company": "ING",
        "content": "Taking Extra Effort Out of Internal Auditing\u00a0\nInternal audit covers many different topics, and often the same topic is addressed in different audits, for instance in different countries. From a data analytics perspective, this means that different audit tests for different topics may be required. It can also mean that the same audit tests may be reused for similar topics but using different data sources. The complexity of analytics for internal audit lies therefore not in the tooling or use of advanced models, but rather in the wide range of topics with multiple and varying data sources. The biggest challenge is finding a way to handle different data types and sources while creating efficiencies and ensuring consistency across all applications.\nING/Internal Audit\u2019s goal is to make data analytics an indispensable part of every audit. The approach is for all Internal Audit staff to become proficient at applying analytics to their audits. An expert team performs advanced analytics and provides support and training. The approach, both from an audit and a staff perspective, requires a data analytics tool that is user-friendly and intuitive, has low (or no) code, supports reusability and sharing, and is flexible and scalable. Because KNIME ticks these boxes, it became the tool of choice for Internal Audit at ING.\nKNIME was first made available in 2017. Since 2020, it has been used not just for its additional computational power and storage, but also for building data apps, which has made working with data and doing analytics even easier.\nAutomating Review Work\nStopping terrorist financing, fighting economic crime, and complying with sanctions are major concerns for the banking industry. It\u2019s important to know your customer and properly screen transactions so you can comply. One of ING\u2019s daily tasks is to review payment transactions for short-term financing, which comes with a lot of trade finance documentation. Documents can be sent in multiple formats, such as invoices, contracts, loan documents, etc. The task is to review the entities on these documents (which could be human names or port/ship names) and cross-reference them with a sanctions list.\u00a0\nPart of the procedures when performing an audit on this topic is re-performing the screening process. Prior to discovering KNIME, the auditors\u00a0 needed to manually read each document, highlight all names or entities and compare the outcome with the lists prepared by the ING analysts.\u00a0\nUsing KNIME Analytics Platform, ING/Internal Audit built workflows to develop a tool that could automatically scan, read, and cross-reference documentation. This ensured completeness of optical character recognition (OCR) for each image. As a company, they must thoroughly and accurately scan 100% of documents going through review. If, for instance, only 90% have been scanned properly, they must identify which 10% were not and which 90% were. And this is where their KNIME-built workflow comes into play.\u00a0\nThis intuitive tool automatically reads a PDF and produces a color-coded result, indicating any words that were missed during scanning and any information/data flagged at a low confidence level of being scanned properly (based on OCR). Then, the tool scans all names and words on the document and compares them to the sanctions list. Therefore, instead of manually scanning over many pages by hand, the audit team only has to focus on the highlighted words indicated by the tool.\nCreating a Reusable Blueprint for Process Mining\u00a0\nProcess mining is used to analyze process flows based on event log data. In many current business processes, IT applications create log files, which register who is doing what and when for every transaction or trade. An event log consists of three important elements: a case ID, the activity itself, and the timestamp. It\u2019s usually enriched with additional data, such as user IDs and departments. It then visualizes the process while it\u2019s taking place \u2014 also known as the \u201cas-is\u201d process. In an audit, the main use case is process discovery. The \u201cas-is\u201d process is benchmarked against the \u201cshould-be\u201d process, which enables auditors to identify non-compliance quickly.\nThe solution at ING/Internal Audit consists of a workflow, which is built and deployed with KNIME as a branded web-based data app. The user opens the data app in their browser, selects the process mining solution, and follows the predetermined steps. These steps include uploading event log data, assigning columns, and conducting the analysis. The purpose-built but easy-to-follow interface offers the right amount of complexity for auditors to conduct their own data analysis anywhere, anytime. The interaction points are all pre-determined by the KNIME workflow, which is running in the background. The workflow consists of many KNIME components, which bundles the workflow functionality into logical, reusable steps.\nUsing KNIME, the team can process insights based on actual data, enable conformance checking, identify bottlenecks, and overall, optimize and simplify process mining efforts.\nING\u2019s Internal Audit Efficiency Gains\u00a0\nING/Internal Audit has managed to integrate data analytics into the everyday work of their auditors, without a lot of heavy lifting, which has gone a long way in helping achieve their vision of becoming data driven. Increasingly, users who didn\u2019t dive into the data before can now generate insights self-sufficiently and faster than before. Because KNIME workflows are reusable and shareable, it\u2019s helped to standardize processes and generate efficiencies at ING. While these efficiencies do come after investing significant time in creating reusable solutions, it\u2019s already paying off. In some cases, it\u2019s reduced the work of individual auditors from three days to 15 minutes.\nTo hear Tjasse Biewenga, head of data analytics for internal audit at ING, talk about these efficiency gains and leveraging KNIME for data analytics, watch his full presentation at the 2022 KNIME Fall Summit.\u00a0To hear more success stories like ING\u2019s and learn how other businesses and data experts across multiple industries are leveraging their data with KNIME, register for our upcoming Spring Summit in Berlin, April 17-19. Early bird tickets are still available.\n\u21d2\u00a0Download this Success Story as a PDF here.\n"
    },
    {
        "id": 41,
        "url": "https://www.knime.com/success-story/how-jysk-optimized-product-sales-knime-predictive-analytics",
        "title": "How JYSK optimized product sales with KNIME for predictive analytics | KNIME",
        "company": "JYSK",
        "content": "The challenge of becoming data-driven\nJYSK is an international home furnishing retailer with Scandinavian roots that makes it easy to furnish every room in any home and garden.\nIt is a part of the family-owned Lars Larsen Group (total turnover of DKK 39.1 billion in financial year 2020/21). JYSK\u2019s turnover was DKK 32.6 billion in the financial year 2020/21. Today, JYSK employs around 28,500 people.\nWith more than 3,100 stores and webshops spread out across different geographies, it was important for the company to be able to make data-driven decisions. However, after conducting a company-wide audit with an external consultancy, JYSK discovered it was not nearly as data-driven as it aimed to be. Not only did that mean operational decisions lacked data inputs, it also meant that time and manpower were being allocated to tasks that could be automated.\nOne of these tasks was to ensure JYSK would be able to meet demand in each of its different geographies. As the company makes a significant portion of its sales through promotions, it required useful data to predict which products would sell better in which geography (due to variations in style and trends across geographies), for a locally-optimized product mix.\nRecognizing the need for data-driven decisions\nA few years ago, JYSK adopted a new top-level strategy to make operations more data-driven. It was decided that the company would use predictive and prescriptive analytics throughout its operations, and not just rely on transactional reporting. To achieve this, the company decided not only to establish a dedicated department for data science and analytics, but also to collect pain points from across the company for which analytics could provide breakthroughs.\nReorganizing the company & leveraging KNIME for advanced analytics\nBy creating a dedicated Advanced Analytics department, JYSK addressed most of its analytics problems in one fell swoop. The department was developed to create value using analytics throughout the company, and began to collect pain points and identify use cases around the company where analytics could be leveraged. KNIME was chosen to be a core analytics solution in the newly-formed department. Working alongside other tools and languages (SAP HANA, R, Python, QGIS), KNIME became the key tool for developing and deploying operational and customer-facing analytics.\nTackling the problem of making data-driven decisions\nThe first of JYSK\u2019s challenges: How to make data available to the entire organization for data- driven decision-making? Using KNIME, JYSK built a browser-based app which was made available to the wider organization, allowing business experts to access data relevant to them. By creating an Advanced Analytics department and making data available throughout the organization, JYSK solved the initial challenge of making it possible for the company to use data science and advanced analytics. Furthermore, by making data accessible to all users across the organization, JYSK tackled operational challenges of using data to make more informed decisions.\nAdvanced analytics challenges\nJYSK manages its geographies directly from its head office in Brabrand, Denmark. The 28 markets in which JYSK operates are of different maturity levels: In some countries, such as Denmark, there is tremendous market penetration. In other countries, such as Ireland, there is less market share. Whenever JYSK enters a new country, there is a new, different set of challenges, including \u2013 but not limited to \u2013 identifying product preferences, buying habits and market trends. In addition, JYSK\u2019s business is primarily driven by promotions, and most customers buy products benefitting from these offers. As different promotions and products are successful in different geographies, JYSK must be prepared to have sufficient stock of relevant products. Without a predictive model to forecast how much of a given item would sell in a given country, significant manpower was allocated to figure this out manually, via Excel. The company needed the ability to accurately predict how much an item will sell in a given country, so it could run appropriate promotions, maintain sufficient inventory in the relevant country without the risk of stock-outs, and to do so without spending 100s of hours of employees\u2019 time. This issue, alongside other pain-points would be overcome with KNIME.\nUsing KNIME for predictive analytics, and other use cases\nKNIME was used in a variety of ways to overcome challenges:\nFor the aforementioned forecasting issues, a predictive model was built in KNIME to establish which products would sell best in which geographies, so that the right allocations of products could be applied, and the company could save man-hours compared to identifying those trends manually.\nWhen dealing with the specifics of different geographies, JYSK used KNIME and QGIS to help find the optimal locations for new stores. By measuring the distance between existing locations and JYSK warehouses, as well as customer density, JYSK was able to predict the best location for new stores easily.\nKNIME was also used to see discount-share by country, so the company could understand how much promotions affected margins.\nJYSK not only developed these analytic workflows, but also used KNIME for scheduling these workflows.\nWhile not currently not in deployment, the company also used KNIME combined with Python to build a recommendation engine to find complementary products based on color in a web portal app.\nSaving manpower hours and creating a data-driven culture\nBesides decision optimization, the major benefit of using KNIME was in time-saved. Across the organization, 100s of hours were saved per month, with hours of work reduced to seconds. What\u2019s more, the management could use insights from KNIME to make smarter decisions about store geographies and product allocation. With flexible tooling and seamless fit across a broad range of environments, Business Intelligence tools, and scripting languages such as Python and R, as well as the ability to consume data from SAP HANA, then enable modeling in the same environment, KNIME provided JYSK the critical capabilities it needed. Finally, as it is low-to-no code, KNIME could be widely used across the organization easily. Not only did KNIME save JYSK employees time and enabled them to make smarter decisions, its uptake was encouraged around the company allowing departments to use analytics independently.\n\u21d2\u00a0Download this Success Story here as a PDF\n"
    },
    {
        "id": 42,
        "url": "https://www.knime.com/success-story/how-jysk-optimized-product-sales-knime-predictive-analytics",
        "title": "How JYSK optimized product sales with KNIME for predictive analytics | KNIME",
        "company": "JYSK",
        "content": "The challenge of becoming data-driven\nJYSK is an international home furnishing retailer with Scandinavian roots that makes it easy to furnish every room in any home and garden.\nIt is a part of the family-owned Lars Larsen Group (total turnover of DKK 39.1 billion in financial year 2020/21). JYSK\u2019s turnover was DKK 32.6 billion in the financial year 2020/21. Today, JYSK employs around 28,500 people.\nWith more than 3,100 stores and webshops spread out across different geographies, it was important for the company to be able to make data-driven decisions. However, after conducting a company-wide audit with an external consultancy, JYSK discovered it was not nearly as data-driven as it aimed to be. Not only did that mean operational decisions lacked data inputs, it also meant that time and manpower were being allocated to tasks that could be automated.\nOne of these tasks was to ensure JYSK would be able to meet demand in each of its different geographies. As the company makes a significant portion of its sales through promotions, it required useful data to predict which products would sell better in which geography (due to variations in style and trends across geographies), for a locally-optimized product mix.\nRecognizing the need for data-driven decisions\nA few years ago, JYSK adopted a new top-level strategy to make operations more data-driven. It was decided that the company would use predictive and prescriptive analytics throughout its operations, and not just rely on transactional reporting. To achieve this, the company decided not only to establish a dedicated department for data science and analytics, but also to collect pain points from across the company for which analytics could provide breakthroughs.\nReorganizing the company & leveraging KNIME for advanced analytics\nBy creating a dedicated Advanced Analytics department, JYSK addressed most of its analytics problems in one fell swoop. The department was developed to create value using analytics throughout the company, and began to collect pain points and identify use cases around the company where analytics could be leveraged. KNIME was chosen to be a core analytics solution in the newly-formed department. Working alongside other tools and languages (SAP HANA, R, Python, QGIS), KNIME became the key tool for developing and deploying operational and customer-facing analytics.\nTackling the problem of making data-driven decisions\nThe first of JYSK\u2019s challenges: How to make data available to the entire organization for data- driven decision-making? Using KNIME, JYSK built a browser-based app which was made available to the wider organization, allowing business experts to access data relevant to them. By creating an Advanced Analytics department and making data available throughout the organization, JYSK solved the initial challenge of making it possible for the company to use data science and advanced analytics. Furthermore, by making data accessible to all users across the organization, JYSK tackled operational challenges of using data to make more informed decisions.\nAdvanced analytics challenges\nJYSK manages its geographies directly from its head office in Brabrand, Denmark. The 28 markets in which JYSK operates are of different maturity levels: In some countries, such as Denmark, there is tremendous market penetration. In other countries, such as Ireland, there is less market share. Whenever JYSK enters a new country, there is a new, different set of challenges, including \u2013 but not limited to \u2013 identifying product preferences, buying habits and market trends. In addition, JYSK\u2019s business is primarily driven by promotions, and most customers buy products benefitting from these offers. As different promotions and products are successful in different geographies, JYSK must be prepared to have sufficient stock of relevant products. Without a predictive model to forecast how much of a given item would sell in a given country, significant manpower was allocated to figure this out manually, via Excel. The company needed the ability to accurately predict how much an item will sell in a given country, so it could run appropriate promotions, maintain sufficient inventory in the relevant country without the risk of stock-outs, and to do so without spending 100s of hours of employees\u2019 time. This issue, alongside other pain-points would be overcome with KNIME.\nUsing KNIME for predictive analytics, and other use cases\nKNIME was used in a variety of ways to overcome challenges:\nFor the aforementioned forecasting issues, a predictive model was built in KNIME to establish which products would sell best in which geographies, so that the right allocations of products could be applied, and the company could save man-hours compared to identifying those trends manually.\nWhen dealing with the specifics of different geographies, JYSK used KNIME and QGIS to help find the optimal locations for new stores. By measuring the distance between existing locations and JYSK warehouses, as well as customer density, JYSK was able to predict the best location for new stores easily.\nKNIME was also used to see discount-share by country, so the company could understand how much promotions affected margins.\nJYSK not only developed these analytic workflows, but also used KNIME for scheduling these workflows.\nWhile not currently not in deployment, the company also used KNIME combined with Python to build a recommendation engine to find complementary products based on color in a web portal app.\nSaving manpower hours and creating a data-driven culture\nBesides decision optimization, the major benefit of using KNIME was in time-saved. Across the organization, 100s of hours were saved per month, with hours of work reduced to seconds. What\u2019s more, the management could use insights from KNIME to make smarter decisions about store geographies and product allocation. With flexible tooling and seamless fit across a broad range of environments, Business Intelligence tools, and scripting languages such as Python and R, as well as the ability to consume data from SAP HANA, then enable modeling in the same environment, KNIME provided JYSK the critical capabilities it needed. Finally, as it is low-to-no code, KNIME could be widely used across the organization easily. Not only did KNIME save JYSK employees time and enabled them to make smarter decisions, its uptake was encouraged around the company allowing departments to use analytics independently.\n\u21d2\u00a0Download this Success Story here as a PDF\n"
    },
    {
        "id": 43,
        "url": "https://www.knime.com/success-story/how-karcher-reduced-inventory-15-knime-while-enhancing-customer-service",
        "title": "How K\u00e4rcher reduced inventory by 15% with KNIME while enhancing customer service | KNIME",
        "company": "K\u00e4rcher",
        "content": "The goal: reduce overstock sustainably\nOverstock occurs when a part or product is kept in storage in quantities much larger than is needed. K\u00e4rcher measures the average time goods are in the warehouse until used or sold in \u201cstock days\u201d and KPIs are set to avoid excess inventories.\nEach K\u00e4rcher location worldwide was responsible for monitoring their own overstock, each with their own approach to meet the KPI targets. At over 80 locations, that\u2019s over 80 different approaches. Some monitored every product manually, some analyzed in Excel, some focused on low stocks of fast-moving products, while others simply stopped ordering and restarted only after meeting their targets.\nDifferent approaches meant higher risk of stock outages for fast-moving products, leaving the company unable to meet customer expectations (while carrying stock overages, and thereby higher inventory costs for slow-moving \u201cB\u201d and \u201cC\u201d articles). Each subsidiary was spending too much time on their own analysis, and coming to the Analytics and Performance Management team with questions.\nTo achieve their KPI target, teams were treating the symptoms rather than finding the cause for the overstock. They needed to dig deeper to reveal information that KPIs don\u2019t cover to find the root cause for overstocks.\nK\u00e4rcher needed a proactive and sustainable solution to reduce overstock and maintain an efficient and responsive supply chain.\nFrom lagging indicators to leading indicators\nAntonina Polkovnikova\u2019s team began by centralizing how overstock was measured. They developed a solution in KNIME to gather and analyze data from all K\u00e4rcher subsidiaries and forecast stock issues across the entire K\u00e4rcher organization.\nAs the team built the new KNIME solution, they integrated expertise from logistics experts at K\u00e4rcher, which helped them identify and merge considerably more data to gain the full picture of how stocks are handled.\n\u201cWe now make extremely precise stock-level recommendations by computing huge quantities of data \u2013 from current stocks, to open orders, to delivered orders, to planned promotions, to master data and more. Our computed solution is able to take much more data into consideration than is humanly possible and produce extremely accurate forecasts,\u201d said Antonina.\nFully operative analytics for all levels\nThe solution presents the data in a dashboard, which is updated once a fortnight. It focuses on the top ten materials for each location, representing approx. 60% of overstock, while still providing the full picture on the other 40%.\nInventory value reduced by 15% at continued high service level\n \u201cWith the new solution, we\u2019ve reduced inventory value by 15% while maintaining our customer service level,\u201d says Antonina Polknikova. And with stock information now centralized, K\u00e4rcher can set internal benchmarks for the most efficient and responsive supply chain. \n Why KNIME?\nA reusable and sustainable solution\nThe person with the best skills to optimize supply chain performance is the supply chain specialist. But these people aren\u2019t usually programmers; neither do they want to become one.\nIn KNIME\u2019s no-code/low-code environment the team can build and implement complex approaches themselves as well as share and reuse workflows. For example, the task of merging data of different types from multiple in-house systems and databases for their solution was challenging, but they only had to do it once.\nFast onboarding of new specialists\nCollaboration is also easier. If a team member is out of office and a subsidiary calls needing an update or a change in the logic, other colleagues can jump in and adjust and run the workflow for the subsidiary because of the immediate clarity of the low-code platform. And when a new specialist joins the team, they only need a couple of weeks to be onboarded in KNIME.\nPerformance specialists focus on more value-adding activities\nWith the single overstock solution covering K\u00e4rcher\u2019s entire supply chain, the supply chain performance specialists no longer have to perform individual analysis for each subsidiary. Instead they are freed up to work on further value-adding activities.\nThey are now also using KNIME to analyze ordering behavior and reconcile demand and financial forecasts. When subsidiaries have a high financial forecast but a low demand forecast, they can reduce procurement to achieve the financial and sales targets. Vice versa, they might run into a risk of ordering too much, thus causing overstock.\nUsing KNIME they have found a way to reconcile these forecasts, ensuring the supply will be replenished just in time to meet customer demand.\n\u201cWith the new data-driven solutions we have set a precedent. We\u2019re now able to improve efficiency and customer service in parallel,\u201d said Polkovnikova.\nThis Success Story is available here as a PDF.\n"
    },
    {
        "id": 44,
        "url": "https://www.knime.com/success-story/how-karcher-reduced-inventory-15-knime-while-enhancing-customer-service",
        "title": "How K\u00e4rcher reduced inventory by 15% with KNIME while enhancing customer service | KNIME",
        "company": "K\u00e4rcher",
        "content": "The goal: reduce overstock sustainably\nOverstock occurs when a part or product is kept in storage in quantities much larger than is needed. K\u00e4rcher measures the average time goods are in the warehouse until used or sold in \u201cstock days\u201d and KPIs are set to avoid excess inventories.\nEach K\u00e4rcher location worldwide was responsible for monitoring their own overstock, each with their own approach to meet the KPI targets. At over 80 locations, that\u2019s over 80 different approaches. Some monitored every product manually, some analyzed in Excel, some focused on low stocks of fast-moving products, while others simply stopped ordering and restarted only after meeting their targets.\nDifferent approaches meant higher risk of stock outages for fast-moving products, leaving the company unable to meet customer expectations (while carrying stock overages, and thereby higher inventory costs for slow-moving \u201cB\u201d and \u201cC\u201d articles). Each subsidiary was spending too much time on their own analysis, and coming to the Analytics and Performance Management team with questions.\nTo achieve their KPI target, teams were treating the symptoms rather than finding the cause for the overstock. They needed to dig deeper to reveal information that KPIs don\u2019t cover to find the root cause for overstocks.\nK\u00e4rcher needed a proactive and sustainable solution to reduce overstock and maintain an efficient and responsive supply chain.\nFrom lagging indicators to leading indicators\nAntonina Polkovnikova\u2019s team began by centralizing how overstock was measured. They developed a solution in KNIME to gather and analyze data from all K\u00e4rcher subsidiaries and forecast stock issues across the entire K\u00e4rcher organization.\nAs the team built the new KNIME solution, they integrated expertise from logistics experts at K\u00e4rcher, which helped them identify and merge considerably more data to gain the full picture of how stocks are handled.\n\u201cWe now make extremely precise stock-level recommendations by computing huge quantities of data \u2013 from current stocks, to open orders, to delivered orders, to planned promotions, to master data and more. Our computed solution is able to take much more data into consideration than is humanly possible and produce extremely accurate forecasts,\u201d said Antonina.\nFully operative analytics for all levels\nThe solution presents the data in a dashboard, which is updated once a fortnight. It focuses on the top ten materials for each location, representing approx. 60% of overstock, while still providing the full picture on the other 40%.\nInventory value reduced by 15% at continued high service level\n \u201cWith the new solution, we\u2019ve reduced inventory value by 15% while maintaining our customer service level,\u201d says Antonina Polknikova. And with stock information now centralized, K\u00e4rcher can set internal benchmarks for the most efficient and responsive supply chain. \n Why KNIME?\nA reusable and sustainable solution\nThe person with the best skills to optimize supply chain performance is the supply chain specialist. But these people aren\u2019t usually programmers; neither do they want to become one.\nIn KNIME\u2019s no-code/low-code environment the team can build and implement complex approaches themselves as well as share and reuse workflows. For example, the task of merging data of different types from multiple in-house systems and databases for their solution was challenging, but they only had to do it once.\nFast onboarding of new specialists\nCollaboration is also easier. If a team member is out of office and a subsidiary calls needing an update or a change in the logic, other colleagues can jump in and adjust and run the workflow for the subsidiary because of the immediate clarity of the low-code platform. And when a new specialist joins the team, they only need a couple of weeks to be onboarded in KNIME.\nPerformance specialists focus on more value-adding activities\nWith the single overstock solution covering K\u00e4rcher\u2019s entire supply chain, the supply chain performance specialists no longer have to perform individual analysis for each subsidiary. Instead they are freed up to work on further value-adding activities.\nThey are now also using KNIME to analyze ordering behavior and reconcile demand and financial forecasts. When subsidiaries have a high financial forecast but a low demand forecast, they can reduce procurement to achieve the financial and sales targets. Vice versa, they might run into a risk of ordering too much, thus causing overstock.\nUsing KNIME they have found a way to reconcile these forecasts, ensuring the supply will be replenished just in time to meet customer demand.\n\u201cWith the new data-driven solutions we have set a precedent. We\u2019re now able to improve efficiency and customer service in parallel,\u201d said Polkovnikova.\nThis Success Story is available here as a PDF.\n"
    },
    {
        "id": 45,
        "url": "https://www.knime.com/success-story/how-knoldus-enables-efficient-inventory-forecasting-knime",
        "title": "How Knoldus enables efficient inventory forecasting with KNIME | KNIME",
        "company": "Knoldus",
        "content": "The challenge: maintaining inventory and efficient consumption of stock\nThese are key decisions that many companies - particularly those in retail - have to make. An excess or shortage has a major effect on profitability and can cost retailers worldwide up to $1.1 trillion annually. Overstocking can lead to decisions like marking down the item\u2019s price, which increases sales turnover. Having limited stock results in lost sales and dissatisfied customers who then purchase from the competition.\nInventory forecasting is a basic procedure for any business, particularly those in Consumer Packaged Goods (CPG). Stock, production, storage, delivery, and showcase \u2013 are all influenced by accurate inventory forecasting. However, an accurate forecasting model may not be everything that an organization wants. It may want to involve different stakeholders in the workflow. This is where\u00a0KNIME partner Knoldus\u00a0and their Knoldus Forecasting Platform (KFP), which is built using KNIME, comes into play.\nWith the KFP, data scientists create a model to forecast sales and tune it for accuracy. Decision makers then set parameters for the forecast based on their needs. The KFP is deployed on KNIME Server as a web application via the KNIME WebPortal. This makes it an easy to use tool for users who aren\u2019t data science experts. Even without the technical details of how the forecasting works, they can customize the input and model parameters and visualize the result of each manipulation. Using inventory forecasting solutions to predict sales or stock consumption is not new. However, most organizations experience the following challenges:\u00a0\nHistory is not enough to predict the future. Most forecasting systems are built on the assumption that historical data is enough to predict the future. However, with the increasing complexity of supply chains, extraneous data makes a big impact on the future. Enterprises have rigid forecasting processes, which make it impossible to implement changes and build new models. This is more acute if the forecasting is done using packaged applications because the integration of external data is complex and time-consuming.No two products are alike.\u00a0Enterprises rely on demand planners who use ERP systems extensively. They know that no two regions or two products are the same. Yet the models built in the ERP are rigid and use the same model across all product attributes. This may simplify the forecasting process, but the forecast is of no use. As a result, stakeholders who have low confidence make manual changes to reflect their impressions. A better approach is to have different models for different products or other classifications. However, current forecasting data flows are complex and too intertwined to accomplish this.(Un)-availability of accurate transactional data.\u00a0Transactional data is continuous, dynamic, and constantly changing. Forecasting systems are usually either embedded into large ERP applications or run on specialized statistical platforms. The difficulty of converting real-time transactional data into these systems is long and complex. Furthermore, these pipelines are developed on other software packages or custom scripts, making it extremely difficult to change or improve.Solution: inventory forecasting using KNIMETo aid in solving these problems,\u00a0KNIME Partner Knoldus\u00a0built the\u00a0Knoldus Forecasting Platform (KFP), a web application built using KNIME that allows decision makers and stakeholders to be as equally involved as data engineers and data scientists in creating a pipeline.\u00a0\nThe KFP provides several advantages over historical forecasting solutions:\u00a0\nConfigurable, dynamic platform.\u00a0Allows the underlying forecasting process to be customized by changing the parameters, datasets, or models, which can be done within a few hours or minutes to provide a timely forecast.Faster, flexible processing with big data.\u00a0End to end pipelines can be run, in most cases, multiple times a day and are only limited by the computational spending users are prepared to incur. Companies can choose to generate forecasts on any cadence that they want or need.\u00a0Rich set of prediction models.\u00a0Machine learning allows for a quick change in models that fit what companies are trying to forecast. The biggest strength of KNIME and, as a result, the KFP, is the ability to plug in advanced models such as neural networks and random forest algorithms with no code (but coding is possible when needed), making the forecast sophisticated and accurate without increasing the complexity.Accuracy measurement.\u00a0Enables measuring the accuracy of the forecast following the principles of machine learning systems. Machine learning algorithms inherently come with accuracy measurements, versions of data-like training, test and production datasets, and give valuable feedback early on.Ability to react to black swan events.\u00a0Reduces the risk of missing out on key global events by allowing for quick changes. Many companies miss out on critical events due to the lack of an easy way to integrate external events.\u00a0Discipline due to implemented forecasting process.\u00a0Forecasting processes are generally very well established and too rigid to change. For a well-tuned supply chain, flexibility is needed to incorporate stakeholder feedback, configure different forecasting parameters, and integrate it into a legacy system. The KFP can be managed independently and integrated into already-existed business processes.Building the analysis with KNIMEWith this Guided Analytics application, companies can create data visualization dashboards and inventory forecasting models as well as generate forecasting for their business intelligently and collaboratively by:\nIngesting data from different data filesConfiguring parameters for the forecasting processCreating data visualization dashboards in an easy and guided wayUsing already available statistical, machine learning, and AI-based algorithmsUsing an in-built email service for collaborating on resultsOnce the reports and visualizations are generated, data scientists, business users, and domain experts can collaborate on the final results.\nINSERT IMAGE KNIME WORKFLOW FOR INVENTORY FORECASTING\nDownload workflow from KNIME Hub.\nWhy KNIME?\nThe free and open source\u00a0KNIME Analytics Platform\u00a0made the development, access, and management of this solution very easy due to the seamless integration with other technologies. For example the\u00a0JavaScript Extension\u00a0and\u00a0Python Integration.\nThe range of customizable KNIME core nodes for data transformation helped in making tedious pre-processing and data cleaning tasks much simpler without needing to manipulate code. On top of the core transformation nodes, KNIME also provides nodes specific to\u00a0time series data. The\u00a0machine learning nodes\u00a0provided assistance in training different models to compare their accuracy for best performance.\nOnce the solution was ready, it was very simple to deploy to KNIME WebPortal via\u00a0KNIME Server\u00a0to create a powerful web application. This enabled domain experts and decision makers to become part of the process.\nThis Innovation Note is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 46,
        "url": "https://www.knime.com/success-story/how-knoldus-enables-efficient-inventory-forecasting-knime",
        "title": "How Knoldus enables efficient inventory forecasting with KNIME | KNIME",
        "company": "Knoldus",
        "content": "The challenge: maintaining inventory and efficient consumption of stock\nThese are key decisions that many companies - particularly those in retail - have to make. An excess or shortage has a major effect on profitability and can cost retailers worldwide up to $1.1 trillion annually. Overstocking can lead to decisions like marking down the item\u2019s price, which increases sales turnover. Having limited stock results in lost sales and dissatisfied customers who then purchase from the competition.\nInventory forecasting is a basic procedure for any business, particularly those in Consumer Packaged Goods (CPG). Stock, production, storage, delivery, and showcase \u2013 are all influenced by accurate inventory forecasting. However, an accurate forecasting model may not be everything that an organization wants. It may want to involve different stakeholders in the workflow. This is where\u00a0KNIME partner Knoldus\u00a0and their Knoldus Forecasting Platform (KFP), which is built using KNIME, comes into play.\nWith the KFP, data scientists create a model to forecast sales and tune it for accuracy. Decision makers then set parameters for the forecast based on their needs. The KFP is deployed on KNIME Server as a web application via the KNIME WebPortal. This makes it an easy to use tool for users who aren\u2019t data science experts. Even without the technical details of how the forecasting works, they can customize the input and model parameters and visualize the result of each manipulation. Using inventory forecasting solutions to predict sales or stock consumption is not new. However, most organizations experience the following challenges:\u00a0\nHistory is not enough to predict the future. Most forecasting systems are built on the assumption that historical data is enough to predict the future. However, with the increasing complexity of supply chains, extraneous data makes a big impact on the future. Enterprises have rigid forecasting processes, which make it impossible to implement changes and build new models. This is more acute if the forecasting is done using packaged applications because the integration of external data is complex and time-consuming.No two products are alike.\u00a0Enterprises rely on demand planners who use ERP systems extensively. They know that no two regions or two products are the same. Yet the models built in the ERP are rigid and use the same model across all product attributes. This may simplify the forecasting process, but the forecast is of no use. As a result, stakeholders who have low confidence make manual changes to reflect their impressions. A better approach is to have different models for different products or other classifications. However, current forecasting data flows are complex and too intertwined to accomplish this.(Un)-availability of accurate transactional data.\u00a0Transactional data is continuous, dynamic, and constantly changing. Forecasting systems are usually either embedded into large ERP applications or run on specialized statistical platforms. The difficulty of converting real-time transactional data into these systems is long and complex. Furthermore, these pipelines are developed on other software packages or custom scripts, making it extremely difficult to change or improve.Solution: inventory forecasting using KNIMETo aid in solving these problems,\u00a0KNIME Partner Knoldus\u00a0built the\u00a0Knoldus Forecasting Platform (KFP), a web application built using KNIME that allows decision makers and stakeholders to be as equally involved as data engineers and data scientists in creating a pipeline.\u00a0\nThe KFP provides several advantages over historical forecasting solutions:\u00a0\nConfigurable, dynamic platform.\u00a0Allows the underlying forecasting process to be customized by changing the parameters, datasets, or models, which can be done within a few hours or minutes to provide a timely forecast.Faster, flexible processing with big data.\u00a0End to end pipelines can be run, in most cases, multiple times a day and are only limited by the computational spending users are prepared to incur. Companies can choose to generate forecasts on any cadence that they want or need.\u00a0Rich set of prediction models.\u00a0Machine learning allows for a quick change in models that fit what companies are trying to forecast. The biggest strength of KNIME and, as a result, the KFP, is the ability to plug in advanced models such as neural networks and random forest algorithms with no code (but coding is possible when needed), making the forecast sophisticated and accurate without increasing the complexity.Accuracy measurement.\u00a0Enables measuring the accuracy of the forecast following the principles of machine learning systems. Machine learning algorithms inherently come with accuracy measurements, versions of data-like training, test and production datasets, and give valuable feedback early on.Ability to react to black swan events.\u00a0Reduces the risk of missing out on key global events by allowing for quick changes. Many companies miss out on critical events due to the lack of an easy way to integrate external events.\u00a0Discipline due to implemented forecasting process.\u00a0Forecasting processes are generally very well established and too rigid to change. For a well-tuned supply chain, flexibility is needed to incorporate stakeholder feedback, configure different forecasting parameters, and integrate it into a legacy system. The KFP can be managed independently and integrated into already-existed business processes.Building the analysis with KNIMEWith this Guided Analytics application, companies can create data visualization dashboards and inventory forecasting models as well as generate forecasting for their business intelligently and collaboratively by:\nIngesting data from different data filesConfiguring parameters for the forecasting processCreating data visualization dashboards in an easy and guided wayUsing already available statistical, machine learning, and AI-based algorithmsUsing an in-built email service for collaborating on resultsOnce the reports and visualizations are generated, data scientists, business users, and domain experts can collaborate on the final results.\nINSERT IMAGE KNIME WORKFLOW FOR INVENTORY FORECASTING\nDownload workflow from KNIME Hub.\nWhy KNIME?\nThe free and open source\u00a0KNIME Analytics Platform\u00a0made the development, access, and management of this solution very easy due to the seamless integration with other technologies. For example the\u00a0JavaScript Extension\u00a0and\u00a0Python Integration.\nThe range of customizable KNIME core nodes for data transformation helped in making tedious pre-processing and data cleaning tasks much simpler without needing to manipulate code. On top of the core transformation nodes, KNIME also provides nodes specific to\u00a0time series data. The\u00a0machine learning nodes\u00a0provided assistance in training different models to compare their accuracy for best performance.\nOnce the solution was ready, it was very simple to deploy to KNIME WebPortal via\u00a0KNIME Server\u00a0to create a powerful web application. This enabled domain experts and decision makers to become part of the process.\nThis Innovation Note is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 47,
        "url": "https://www.knime.com/success-story/how-netzeffekt-provides-bespoke-project-delivery-customers",
        "title": "How netzeffekt provides bespoke project delivery to customers | KNIME",
        "company": "Netzeffekt",
        "content": "Self-service analytics independent of IT\nThe KNIME solution enables the Digital Analytics & BI team to harmonize and enrich data. The team can now build flexible, multiple-channel datasets independently of IT efforts. As a low-code tool, KNIME was easy for the BI team to learn. Building workflows by drag and drop is intuitive and the self-documenting nature of each workflow facilitates easy sharing of work. Additionally, all standardized, repeatable operations are bundled into so-called components to plug and play.\nUniform harmonization of data\nOne application fetches the data automatically at 2 AM, combines the cross-channel sources, harmonizing, and enriching the data. When the BI team comes to work, the data is waiting for their analysis. KNIME has merged the data, taking care for example, that all rows and column headings are uniform. Instead of the team having to calculate the metrics from \u201cOrganic\u201d in one channel and \u201corganic\u201d from another, KNIME ensures all metrics related to \u201cOrganic/organic\u201d are combined in a single, consistently named location, providing the team with reliable, accurate data.\nBespoke project delivery\nThe new solution gives the team more time to invest on their analysis. The team can now manage the data and define their dashboards entirely according to customer wishes, free from the predefined settings of their BI tools. The dashboards are updated automatically with actual data and their customers can view it live. Any required changes to the underlying data can be implemented by the BI team themselves immediately.\n\u201cThe ability to automate the complex process of harmonizing data from different channels enables us to deliver refined campaigns to our customers fast,\u201d\u00a0confirmed Messmer.\nGreater trust in the data\nThe new solution means less miscommunication between requirements and actual implementation. With the ability to access and understand the underlying dataset, the BI team automatically spends less time worrying about the integrity of the data and can focus on gaining more insight from it. With this sophisticated data-driven insight, the team can adjust and improve campaign performance, e.g. shifting budget allocations to higher-performing channels or specific ads.\nWhy KNIME\nUsability and price were the two convincing factors to opt for KNIME. netzeffekt wanted to enable their business analysts the flexibility to work with the BI tools of their choice and not need to purchase additional licenses for this or that tool. Universal connectivity through free access to 300+ data sources ensures their solution remains flexible in the future. KNIME provides integrations with all the relevant tools and environments.\n\u201cThe open source solution allowed us to explore the tool without making an upfront investment. After achieving initial positive results, we made the decision to invest in KNIME Server to handle automation and job scheduling,\u201d\u00a0explains Tanja Messmer, Director Digital Analytics & BI, netzeffekt.\n\u21d2\u00a0Download this Success Story here as a PDF\n"
    },
    {
        "id": 48,
        "url": "https://www.knime.com/success-story/how-netzeffekt-provides-bespoke-project-delivery-customers",
        "title": "How netzeffekt provides bespoke project delivery to customers | KNIME",
        "company": "Netzeffekt",
        "content": "Self-service analytics independent of IT\nThe KNIME solution enables the Digital Analytics & BI team to harmonize and enrich data. The team can now build flexible, multiple-channel datasets independently of IT efforts. As a low-code tool, KNIME was easy for the BI team to learn. Building workflows by drag and drop is intuitive and the self-documenting nature of each workflow facilitates easy sharing of work. Additionally, all standardized, repeatable operations are bundled into so-called components to plug and play.\nUniform harmonization of data\nOne application fetches the data automatically at 2 AM, combines the cross-channel sources, harmonizing, and enriching the data. When the BI team comes to work, the data is waiting for their analysis. KNIME has merged the data, taking care for example, that all rows and column headings are uniform. Instead of the team having to calculate the metrics from \u201cOrganic\u201d in one channel and \u201corganic\u201d from another, KNIME ensures all metrics related to \u201cOrganic/organic\u201d are combined in a single, consistently named location, providing the team with reliable, accurate data.\nBespoke project delivery\nThe new solution gives the team more time to invest on their analysis. The team can now manage the data and define their dashboards entirely according to customer wishes, free from the predefined settings of their BI tools. The dashboards are updated automatically with actual data and their customers can view it live. Any required changes to the underlying data can be implemented by the BI team themselves immediately.\n\u201cThe ability to automate the complex process of harmonizing data from different channels enables us to deliver refined campaigns to our customers fast,\u201d\u00a0confirmed Messmer.\nGreater trust in the data\nThe new solution means less miscommunication between requirements and actual implementation. With the ability to access and understand the underlying dataset, the BI team automatically spends less time worrying about the integrity of the data and can focus on gaining more insight from it. With this sophisticated data-driven insight, the team can adjust and improve campaign performance, e.g. shifting budget allocations to higher-performing channels or specific ads.\nWhy KNIME\nUsability and price were the two convincing factors to opt for KNIME. netzeffekt wanted to enable their business analysts the flexibility to work with the BI tools of their choice and not need to purchase additional licenses for this or that tool. Universal connectivity through free access to 300+ data sources ensures their solution remains flexible in the future. KNIME provides integrations with all the relevant tools and environments.\n\u201cThe open source solution allowed us to explore the tool without making an upfront investment. After achieving initial positive results, we made the decision to invest in KNIME Server to handle automation and job scheduling,\u201d\u00a0explains Tanja Messmer, Director Digital Analytics & BI, netzeffekt.\n\u21d2\u00a0Download this Success Story here as a PDF\n"
    },
    {
        "id": 49,
        "url": "https://www.knime.com/success-story/how-pg-uses-real-time-data-supply-chain-resiliency",
        "title": "How P&G uses real-time data for supply chain resiliency | KNIME",
        "company": "Procter & Gamble",
        "content": "Meet supply chain disruption with fast data-driven insight\nReal-time data is essential for supply chain resiliency, per research by economic advisory firm, Oxford Economics, because it gives organizations \u201csuperior visibility up and down the supply chain\u201d. While 75% of leaders see the importance of increased visibility for more informed decisions, few actually have it.\nProcter & Gamble, the multinational consumer goods corporation, is actively working to make this a reality.\nWhen a hurricane eliminates supply from Florida or the Suez Canal blocks with 6 months of orders stuck on a boat, or a pandemic shuts down borders, the supply chain repercussions are huge.\nIdentifying exactly which products, supplier networks, and plant equipment this will affect is a tall order when you have one of the largest supply chains in the world. P&G\u2019s Personal Health Care sub-sector alone counts more than 5,000 unique products. Those 5,000 products are made up of over 22,000 individual components, with the supply and demand of each affecting the one that follows it.\nP&G was already using data from multiple systems to answer these supply questions, but integrating the data was often time-consuming, manual, and required up to 10 different types of expertise. So when the pandemic upended retail, the need for real-time, data-driven insight became even more acute. \nA major step towards superior visibility through real-time forecasting\nBrennon Sessions, Sr. Scientist in P&G\u2019s R&D group, and a chemist by trade, is one of the functional experts at P&G who has been identifying which data reporting practices can be improved by automation to improve speed, accuracy, and practice. In a collaboration with analytics provider, phData, they developed a solution that gathers and merges all the relevant technical and supply data to tell a single, holistic story \u2013 getting closer to superior visibility up and down the supply chain.\nData activation and automation is the focus of Sessions\u2019 team. They faced the challenge of integrating complex data from multiple sources: from manufacturing, to lab information, to supply chain, marketing, and quality assurance.\nFrom individual pieces of information to a single, holistic data story\nSupply chain data is one of the largest sources with records easily in the 10s of millions. Every lot of material and batch of a product has associated quality testing records, with at least 10 quality tests performed on each batch.\n\u201cIt\u2019s common for final dataset sizes to exceed a million rows after appropriate filtering & cleaning; a single Excel sheet maxes out at 1,048,576 rows,\u201d says Sessions.\nPreviously data collection was manual. It had to be performed by people with expertise for that data type. The manufacturing-, lab-information-, supply chain-, marketing-, quality-assurance-data experts spent hundreds of hours gathering and integrating data. In daily meetings, the integrity of the data was checked and involved going through multiple, differing Excel files, by hand.\nAutomatic data integration provides immediate insight\nCombining the technical expertise in KNIME from phData with subject matter expertise from Brennon\u2019s team, P&G has upskilled to the next level. They developed a KNIME solution to fully automate data collection to produce a single, accessible pool of information.\nManagers, scientists, and researchers are now enabled to explore and gain insight from this accessible pool. What took hours now only takes minutes and they have answers to supply and demand questions instantaneously.\nInstead of people spending valuable time every day collecting data they can put their expertise to best use, exploring the data for additional insight.\nThe KNIME solution automatically combines multiple data sources together to assess the potential impacts of unexpected changes to supply & demand forecasts, and provide guidance on options that could help mitigate risks.\nLive data delivers precise supply and demand forecasting\nTo provide precise real-time supply and demand forecasting, the KNIME solution brings together:\nBill of materials data to identify the 22,000 components making up each of the 5000 productsSupply chain data on all the permitted vendors for each of the 22,000 components, plus the involved manufacturing plants, warehouses, and distribution centerCurrent inventory data to understand the potential risks to supply and demand changesUsing all this data the solution is able to produce a real-time summary of expected demand together with a real-time supplier projection of the quantities and timings for component deliveries.\nWith the new solution, leadership has a more efficient view of the data and can make more specific decisions. Daily, regional decision meetings have now been replaced with one global decision meeting.\nUser-friendly dashboards enable multi-level views of live data\nThe dataset generated by the KNIME workflows were used to produce a summary specification view, designed to provide live data to as many different people as possible in a user-friendly interface.\u00a0\nP&G leveraged data and analytics services provided by phData, a KNIME partner, to create the initial iterations of the dashboard. With this foundation, P&G\u2019s technical experts can now modify and expand without having to become dashboard experts themselves.\nDifferent views exist for different levels in the business, with the global summary providing insight for leaders and close-up views allowing more hands-on colleagues to dig deeper to look up all the parts associated with specific products. Previously, digging through all the systems to find all the product parts could take hours. It now only takes seconds.\nBenefits at a glance: before and after KNIME\nFrom manual data collection to automated data integrationFrom 10+data navigation experts to 0 peopleFrom 2+ hours/day to get answers to questions to instantaneous insightFrom daily regional review meetings to a single global decision meetingOrganization upskilled to the next levelP&G researchers and scientists had already started to use KNIME as a way to layer data science over their work. In fact, KNIME is used throughout P&G in multiple sectors ranging across both technical and commercial functions, including marketing, finance, sales and manufacturing operations. The intuitiveness of the low-code environment makes it easier for \u201ccitizen developers\u201d with little or no programming experience to get started and learn quickly.\n\u201cKNIME gives us access to advanced data science techniques and we can still focus on our work. There are now around 8,000 KNIME users across P&G globally,\u201d said Brennon Sessions.\nWith so many employees using KNIME it\u2019s a great opportunity to share knowledge, collaborate, and learn from each other.\n"
    },
    {
        "id": 50,
        "url": "https://www.knime.com/success-story/how-pg-uses-real-time-data-supply-chain-resiliency",
        "title": "How P&G uses real-time data for supply chain resiliency | KNIME",
        "company": "Procter & Gamble",
        "content": "Meet supply chain disruption with fast data-driven insight\nReal-time data is essential for supply chain resiliency, per research by economic advisory firm, Oxford Economics, because it gives organizations \u201csuperior visibility up and down the supply chain\u201d. While 75% of leaders see the importance of increased visibility for more informed decisions, few actually have it.\nProcter & Gamble, the multinational consumer goods corporation, is actively working to make this a reality.\nWhen a hurricane eliminates supply from Florida or the Suez Canal blocks with 6 months of orders stuck on a boat, or a pandemic shuts down borders, the supply chain repercussions are huge.\nIdentifying exactly which products, supplier networks, and plant equipment this will affect is a tall order when you have one of the largest supply chains in the world. P&G\u2019s Personal Health Care sub-sector alone counts more than 5,000 unique products. Those 5,000 products are made up of over 22,000 individual components, with the supply and demand of each affecting the one that follows it.\nP&G was already using data from multiple systems to answer these supply questions, but integrating the data was often time-consuming, manual, and required up to 10 different types of expertise. So when the pandemic upended retail, the need for real-time, data-driven insight became even more acute. \nA major step towards superior visibility through real-time forecasting\nBrennon Sessions, Sr. Scientist in P&G\u2019s R&D group, and a chemist by trade, is one of the functional experts at P&G who has been identifying which data reporting practices can be improved by automation to improve speed, accuracy, and practice. In a collaboration with analytics provider, phData, they developed a solution that gathers and merges all the relevant technical and supply data to tell a single, holistic story \u2013 getting closer to superior visibility up and down the supply chain.\nData activation and automation is the focus of Sessions\u2019 team. They faced the challenge of integrating complex data from multiple sources: from manufacturing, to lab information, to supply chain, marketing, and quality assurance.\nFrom individual pieces of information to a single, holistic data story\nSupply chain data is one of the largest sources with records easily in the 10s of millions. Every lot of material and batch of a product has associated quality testing records, with at least 10 quality tests performed on each batch.\n\u201cIt\u2019s common for final dataset sizes to exceed a million rows after appropriate filtering & cleaning; a single Excel sheet maxes out at 1,048,576 rows,\u201d says Sessions.\nPreviously data collection was manual. It had to be performed by people with expertise for that data type. The manufacturing-, lab-information-, supply chain-, marketing-, quality-assurance-data experts spent hundreds of hours gathering and integrating data. In daily meetings, the integrity of the data was checked and involved going through multiple, differing Excel files, by hand.\nAutomatic data integration provides immediate insight\nCombining the technical expertise in KNIME from phData with subject matter expertise from Brennon\u2019s team, P&G has upskilled to the next level. They developed a KNIME solution to fully automate data collection to produce a single, accessible pool of information.\nManagers, scientists, and researchers are now enabled to explore and gain insight from this accessible pool. What took hours now only takes minutes and they have answers to supply and demand questions instantaneously.\nInstead of people spending valuable time every day collecting data they can put their expertise to best use, exploring the data for additional insight.\nThe KNIME solution automatically combines multiple data sources together to assess the potential impacts of unexpected changes to supply & demand forecasts, and provide guidance on options that could help mitigate risks.\nLive data delivers precise supply and demand forecasting\nTo provide precise real-time supply and demand forecasting, the KNIME solution brings together:\nBill of materials data to identify the 22,000 components making up each of the 5000 productsSupply chain data on all the permitted vendors for each of the 22,000 components, plus the involved manufacturing plants, warehouses, and distribution centerCurrent inventory data to understand the potential risks to supply and demand changesUsing all this data the solution is able to produce a real-time summary of expected demand together with a real-time supplier projection of the quantities and timings for component deliveries.\nWith the new solution, leadership has a more efficient view of the data and can make more specific decisions. Daily, regional decision meetings have now been replaced with one global decision meeting.\nUser-friendly dashboards enable multi-level views of live data\nThe dataset generated by the KNIME workflows were used to produce a summary specification view, designed to provide live data to as many different people as possible in a user-friendly interface.\u00a0\nP&G leveraged data and analytics services provided by phData, a KNIME partner, to create the initial iterations of the dashboard. With this foundation, P&G\u2019s technical experts can now modify and expand without having to become dashboard experts themselves.\nDifferent views exist for different levels in the business, with the global summary providing insight for leaders and close-up views allowing more hands-on colleagues to dig deeper to look up all the parts associated with specific products. Previously, digging through all the systems to find all the product parts could take hours. It now only takes seconds.\nBenefits at a glance: before and after KNIME\nFrom manual data collection to automated data integrationFrom 10+data navigation experts to 0 peopleFrom 2+ hours/day to get answers to questions to instantaneous insightFrom daily regional review meetings to a single global decision meetingOrganization upskilled to the next levelP&G researchers and scientists had already started to use KNIME as a way to layer data science over their work. In fact, KNIME is used throughout P&G in multiple sectors ranging across both technical and commercial functions, including marketing, finance, sales and manufacturing operations. The intuitiveness of the low-code environment makes it easier for \u201ccitizen developers\u201d with little or no programming experience to get started and learn quickly.\n\u201cKNIME gives us access to advanced data science techniques and we can still focus on our work. There are now around 8,000 KNIME users across P&G globally,\u201d said Brennon Sessions.\nWith so many employees using KNIME it\u2019s a great opportunity to share knowledge, collaborate, and learn from each other.\n"
    },
    {
        "id": 51,
        "url": "https://www.knime.com/success-story/how-pure-romance-redesigned-their-data-architecture-saving-1000s-hours-month",
        "title": "How Pure Romance redesigned their data architecture, saving 1000s of hours per month",
        "company": "Pure Romance",
        "content": "\nBuilt correctly, a company\u2019s data architecture is invisible. The right systems can easily collect and exchange error-free data, and different end-user groups can effortlessly access what they need. However, as Pure Romance found, when the data architecture is optimized for short-term reporting needs and not to follow best practices, this can not only create frustration, but also cause misalignment with medium-to-long-term operational goals, affecting the bottom line.\nPure Romance\u2019s business and application logic evolved gradually, and was geared toward financial reporting. Although this helped day to day, it was unsustainable in the long term, as this financial reporting logic was built into the database. The data team became a bottleneck; not only were they relied upon to prepare reports for use by external consultants (with the financial logic extracted from the data), but they were also prevented from performing advanced analytics tasks and providing their real value to the company.\nAs such, the database was especially restrictive when it came to changing the company\u2019s strategic and operational goals. When the financial analysts wanted to make swift changes to their reports (for example, to include discounts in sales reporting), the data team was again tasked with wrestling with the existing logic in the database.\nAiming to align the company\u2019s data architecture with its day-to-day operations, Pure Romance set about correcting several years\u2019 worth of false starts and finding the right data science platform. The company wanted to remove business and application logic from the database, so that internal financial analysts could work independently of the data team, external consultants could produce their own reports, and the data team could be freed to work on more complex tasks.\nAfter a competitive head-to-head proof of concept, KNIME was determined to be the right choice.\n\nThe models of the past can be a drag on future business\nBefore it had a dedicated financial analytics department, Pure Romance built financial reporting right into its database. Since the data model which evolved during this time was primarily geared toward financial reporting, the ETL was heavily propagated with business logic stored on multiple layers. This dramatically slowed the entire process, and locked the data team into supporting exceptionally complex data sources that mixed dimensions, facts, and grain.\nEvery time internal financial analysts or external consultants wanted to create individualized reports, or whenever data needed to be pulled for advanced analytics (or even checking inventory levels), the company was bogged down by its database.\nThe company was using SQL Server Reporting Services for commercial reporting and hard-coded, canned reports which weren\u2019t editable on the consultant front end, with few reports targeting the business and executive layers. When the company hired a Database Administrator (now Director), this one-person data team had to create and design reports using Tableau, with consistent layouts and dates on the consultant front end.\nThis was a sub-optimal use of resources (since data experts can solve far more complex and urgent problems), and also restricted the company\u2019s overall reporting flexibility, given that business intelligence analysts could create their own reports if given the tools to do so.\nIt was soon evident that the company\u2019s OLAP cube (the structure optimized for quick data analysis) was also inadequately organized. It was predicated upon certain errors in the data not only being expected, but required. When they tested the system by inputting error-free and corrected data, it caused the cube processing to crash.\n\n\nA first attempted solution: Tableau\nThe company purchased two Tableau servers to let both internal (financial analysts) and external (consultants) workers carry out their own reporting. Although Tableau enabled easy reporting with distributed access for different teams, new issues arose. For example, many of the datasets had to be published to both servers, and only SQL queries would enable Tableau\u2019s scheduler to access that data. So whether it was internally or externally accessed, both Tableau servers were being hit, resulting in an almost constant churn of data, limiting the window of opportunity to load new data from source systems.\nFurthermore, in spite of these changes, the ETL still contained too much business logic, meaning the tables and views created for Tableau were deep and fat. So when data was accessed, either internally or externally, not only was the warehouse accessed twice, but it was done so using complex business and application logic embedded in the ETL \u2014 a circuitous and inefficient process. This process is also unconventional, as typically ETL is done before data enters the warehouse.\nThis prevented the data team from working on advanced analytics tasks, and these problems directly impacted the company\u2019s business intelligence structures, and ultimately its bottom line.\nPure Romance is a direct sales company. As Rachel Ambler, Director of Database and Business Intelligence Systems, puts it, they deal with consultants who range from \u201cpart-time ladies looking to make some pocket money to \u2018rock star\u2019 team leaders leading million-dollar teams.\u201d Any database problems have direct impacts across every business function and operation, with unreliable customer data affecting revenue. For example, poor data can prevent additional sales opportunities, offer incorrect personalized recommendations, and impact whether a product\u2019s availability is properly displayed on a sales portal.\nNevertheless, reengineering the warehouse was not a possibility, as it would take up too much redevelopment time and capacity. So the company began looking for a new tool.\nFinding the right tool for the right job: A head-to-head proof of concept\nPure Romance conducted a proof of concept, comparing KNIME to another data science platform to see which would best solve its problems.Flexibility and extensibility\nInitially the company needed a flexible and extensible tool that could work with Tableau. Later they needed one that could work with both Tableau and PowerBI, both of which its data team now uses. KNIME\u2019s flexibility and extensibility was preferred over the competitor; it could only work with Tableau and PowerBI, and could easily integrate with other solutions at no extra cost, while integrations could be costly for the competitor.\nWhat\u2019s more, KNIME\u2019s reusable components and Python scripting features enabled Pure Romance to remove the need to publish twice on Tableau. The fact that KNIME could run on Mac, Windows, and Linux was also advantageous for a company like Pure Romance, whose staff uses a mix of technologies.A single source of truth\nKNIME was selected over the competitor, as it enabled financial analysts to take the business and application logic from the ETL using their own workflows. They could access the data and reports they needed without making the data team into a bottleneck.\nWhen the company also started using PowerBI, they found that it functions best with Kimballized data for performance, rather than deep and fat datasets. This forced the company to switch from its previous model to a Kimball-data one, a bottom-up approach in which data marts are formed based on the business requirements, as opposed to establishing a data warehouse and trying to make it fit the business needs.\nBy implementing PowerBI, the data department became responsible for the application logic but not the business logic. In terms of ETL, the department only became responsible for a minimal amount of effort in cleaning up the application layer. The ETL was slowly transforming into more of an EL(T), leaving transformation to the financial analysts.\nIn this new setup, KNIME is used to impose rules on the business logic of the Kimballized data, which is what domain experts now do. They can simply extract the business logic from the database using a workflow in KNIME, and then access the data they need, while the data team is free to focus on other tasks.\nThe data models are now better able to accurately represent the source systems data, and thus get Pure Romance ever closer to a single source of truth, and still allow the analysts to add business logic on top of these models to ensure the resulting data matches the needs of the business.Support for all data sources and types\nHaving two servers meant Pure Romance needed both to have the same data sources, which meant both were required to run some very heavy scheduled extracts every hour of every day. However, they were restricted by Tableau\u2019s scheduler.\nAs KNIME could work with many data types and sources and enabled smart scheduling, the comparison in the POC was found to be \u201ca slam dunk for KNIME,\u201d as it allowed the data team to create a custom component in Python to publish to both Tableau servers simultaneously.Total cost of ownership\nKNIME offered zero startup cost, with KNIME Analytics Platform being open source, so the team could test and play with the tool before scaling to the on-premises version KNIME Server (now KNIME Business Hub).\nThe commercial offering provided much functionality out of the box, rather than charging extra for APIs, as the competitor tool did. Its ability to integrate with other tools and systems without additional charge was also a bonus for Pure Romance.Partners, not a customer\nPure Romance found they preferred KNIME over the competitor because \u201cKNIME was concerned with creating a partnership, and not just selling a product,\u201d as Ambler put it. She continued: \u201cEveryone we worked with at KNIME, from the business to the technical teams, was extremely helpful, and went above and beyond in supporting our needs. The competitor, on the other hand, was more concerned with making a sale. Plus, I must say, KNIME offers a Smorgasbord of data manipulation goodness in its components and tools.\u201d\nOutcome\nBy using KNIME and PowerBI with a new data warehouse, Pure Romance has been able to cut down end-to-end load time of the data from three hours to about one and a half \u2013 and are still looking at further time savings as they continue to utilize more and more aspects of KNIME\u2019s functionality. Over the course of the year, this time accumulates to many hours saved in data processing.\nFurthermore, the data team is slowly being removed as the bottleneck: they are free to work on more advanced sourcing and cleanup tasks, while domain experts can access reports free of business logic without relying on the data team.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 52,
        "url": "https://www.knime.com/success-story/how-pure-romance-redesigned-their-data-architecture-saving-1000s-hours-month",
        "title": "How Pure Romance redesigned their data architecture, saving 1000s of hours per month",
        "company": "Pure Romance",
        "content": "\nBuilt correctly, a company\u2019s data architecture is invisible. The right systems can easily collect and exchange error-free data, and different end-user groups can effortlessly access what they need. However, as Pure Romance found, when the data architecture is optimized for short-term reporting needs and not to follow best practices, this can not only create frustration, but also cause misalignment with medium-to-long-term operational goals, affecting the bottom line.\nPure Romance\u2019s business and application logic evolved gradually, and was geared toward financial reporting. Although this helped day to day, it was unsustainable in the long term, as this financial reporting logic was built into the database. The data team became a bottleneck; not only were they relied upon to prepare reports for use by external consultants (with the financial logic extracted from the data), but they were also prevented from performing advanced analytics tasks and providing their real value to the company.\nAs such, the database was especially restrictive when it came to changing the company\u2019s strategic and operational goals. When the financial analysts wanted to make swift changes to their reports (for example, to include discounts in sales reporting), the data team was again tasked with wrestling with the existing logic in the database.\nAiming to align the company\u2019s data architecture with its day-to-day operations, Pure Romance set about correcting several years\u2019 worth of false starts and finding the right data science platform. The company wanted to remove business and application logic from the database, so that internal financial analysts could work independently of the data team, external consultants could produce their own reports, and the data team could be freed to work on more complex tasks.\nAfter a competitive head-to-head proof of concept, KNIME was determined to be the right choice.\n\nThe models of the past can be a drag on future business\nBefore it had a dedicated financial analytics department, Pure Romance built financial reporting right into its database. Since the data model which evolved during this time was primarily geared toward financial reporting, the ETL was heavily propagated with business logic stored on multiple layers. This dramatically slowed the entire process, and locked the data team into supporting exceptionally complex data sources that mixed dimensions, facts, and grain.\nEvery time internal financial analysts or external consultants wanted to create individualized reports, or whenever data needed to be pulled for advanced analytics (or even checking inventory levels), the company was bogged down by its database.\nThe company was using SQL Server Reporting Services for commercial reporting and hard-coded, canned reports which weren\u2019t editable on the consultant front end, with few reports targeting the business and executive layers. When the company hired a Database Administrator (now Director), this one-person data team had to create and design reports using Tableau, with consistent layouts and dates on the consultant front end.\nThis was a sub-optimal use of resources (since data experts can solve far more complex and urgent problems), and also restricted the company\u2019s overall reporting flexibility, given that business intelligence analysts could create their own reports if given the tools to do so.\nIt was soon evident that the company\u2019s OLAP cube (the structure optimized for quick data analysis) was also inadequately organized. It was predicated upon certain errors in the data not only being expected, but required. When they tested the system by inputting error-free and corrected data, it caused the cube processing to crash.\n\n\nA first attempted solution: Tableau\nThe company purchased two Tableau servers to let both internal (financial analysts) and external (consultants) workers carry out their own reporting. Although Tableau enabled easy reporting with distributed access for different teams, new issues arose. For example, many of the datasets had to be published to both servers, and only SQL queries would enable Tableau\u2019s scheduler to access that data. So whether it was internally or externally accessed, both Tableau servers were being hit, resulting in an almost constant churn of data, limiting the window of opportunity to load new data from source systems.\nFurthermore, in spite of these changes, the ETL still contained too much business logic, meaning the tables and views created for Tableau were deep and fat. So when data was accessed, either internally or externally, not only was the warehouse accessed twice, but it was done so using complex business and application logic embedded in the ETL \u2014 a circuitous and inefficient process. This process is also unconventional, as typically ETL is done before data enters the warehouse.\nThis prevented the data team from working on advanced analytics tasks, and these problems directly impacted the company\u2019s business intelligence structures, and ultimately its bottom line.\nPure Romance is a direct sales company. As Rachel Ambler, Director of Database and Business Intelligence Systems, puts it, they deal with consultants who range from \u201cpart-time ladies looking to make some pocket money to \u2018rock star\u2019 team leaders leading million-dollar teams.\u201d Any database problems have direct impacts across every business function and operation, with unreliable customer data affecting revenue. For example, poor data can prevent additional sales opportunities, offer incorrect personalized recommendations, and impact whether a product\u2019s availability is properly displayed on a sales portal.\nNevertheless, reengineering the warehouse was not a possibility, as it would take up too much redevelopment time and capacity. So the company began looking for a new tool.\nFinding the right tool for the right job: A head-to-head proof of concept\nPure Romance conducted a proof of concept, comparing KNIME to another data science platform to see which would best solve its problems.Flexibility and extensibility\nInitially the company needed a flexible and extensible tool that could work with Tableau. Later they needed one that could work with both Tableau and PowerBI, both of which its data team now uses. KNIME\u2019s flexibility and extensibility was preferred over the competitor; it could only work with Tableau and PowerBI, and could easily integrate with other solutions at no extra cost, while integrations could be costly for the competitor.\nWhat\u2019s more, KNIME\u2019s reusable components and Python scripting features enabled Pure Romance to remove the need to publish twice on Tableau. The fact that KNIME could run on Mac, Windows, and Linux was also advantageous for a company like Pure Romance, whose staff uses a mix of technologies.A single source of truth\nKNIME was selected over the competitor, as it enabled financial analysts to take the business and application logic from the ETL using their own workflows. They could access the data and reports they needed without making the data team into a bottleneck.\nWhen the company also started using PowerBI, they found that it functions best with Kimballized data for performance, rather than deep and fat datasets. This forced the company to switch from its previous model to a Kimball-data one, a bottom-up approach in which data marts are formed based on the business requirements, as opposed to establishing a data warehouse and trying to make it fit the business needs.\nBy implementing PowerBI, the data department became responsible for the application logic but not the business logic. In terms of ETL, the department only became responsible for a minimal amount of effort in cleaning up the application layer. The ETL was slowly transforming into more of an EL(T), leaving transformation to the financial analysts.\nIn this new setup, KNIME is used to impose rules on the business logic of the Kimballized data, which is what domain experts now do. They can simply extract the business logic from the database using a workflow in KNIME, and then access the data they need, while the data team is free to focus on other tasks.\nThe data models are now better able to accurately represent the source systems data, and thus get Pure Romance ever closer to a single source of truth, and still allow the analysts to add business logic on top of these models to ensure the resulting data matches the needs of the business.Support for all data sources and types\nHaving two servers meant Pure Romance needed both to have the same data sources, which meant both were required to run some very heavy scheduled extracts every hour of every day. However, they were restricted by Tableau\u2019s scheduler.\nAs KNIME could work with many data types and sources and enabled smart scheduling, the comparison in the POC was found to be \u201ca slam dunk for KNIME,\u201d as it allowed the data team to create a custom component in Python to publish to both Tableau servers simultaneously.Total cost of ownership\nKNIME offered zero startup cost, with KNIME Analytics Platform being open source, so the team could test and play with the tool before scaling to the on-premises version KNIME Server (now KNIME Business Hub).\nThe commercial offering provided much functionality out of the box, rather than charging extra for APIs, as the competitor tool did. Its ability to integrate with other tools and systems without additional charge was also a bonus for Pure Romance.Partners, not a customer\nPure Romance found they preferred KNIME over the competitor because \u201cKNIME was concerned with creating a partnership, and not just selling a product,\u201d as Ambler put it. She continued: \u201cEveryone we worked with at KNIME, from the business to the technical teams, was extremely helpful, and went above and beyond in supporting our needs. The competitor, on the other hand, was more concerned with making a sale. Plus, I must say, KNIME offers a Smorgasbord of data manipulation goodness in its components and tools.\u201d\nOutcome\nBy using KNIME and PowerBI with a new data warehouse, Pure Romance has been able to cut down end-to-end load time of the data from three hours to about one and a half \u2013 and are still looking at further time savings as they continue to utilize more and more aspects of KNIME\u2019s functionality. Over the course of the year, this time accumulates to many hours saved in data processing.\nFurthermore, the data team is slowly being removed as the bottleneck: they are free to work on more advanced sourcing and cleanup tasks, while domain experts can access reports free of business logic without relying on the data team.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 53,
        "url": "https://www.knime.com/success-story/how-qfc-regulatory-authority-empowered-bank-supervisors-advanced-analytical-models",
        "title": "How QFC Regulatory Authority empowered bank supervisors with advanced analytical model",
        "company": "Qatar Financial Center Regulatory Authority",
        "content": "\nIn banking supervision the emphasis has moved from scrutiny of financial statements at a point-in-time to being more analytical, e.g. observing differences over time, individual firms, and variables. But, complicating the analysis, the firms can submit data at different frequencies and their financial years do not always align with the calendar year.\u00a0KNIME Analytics Platform\u00a0is used to get the data on the same basis \u2013 for example to interpolate quarterly submissions into monthly, to calculate CYTD out of FYTD, to create rolling 12-month variables, and more. The KNIME workflow is deployed via\u00a0KNIME Server,\u00a0which pushes the prepared data to Tableau Server (using the native\u00a0Tableau Integration), where it is underpinning standardized dashboards and enables supervisors to perform flexible ad-hoc analysis.\n\n"
    },
    {
        "id": 54,
        "url": "https://www.knime.com/success-story/how-qfc-regulatory-authority-empowered-bank-supervisors-advanced-analytical-models",
        "title": "How QFC Regulatory Authority empowered bank supervisors with advanced analytical model",
        "company": "Qatar Financial Center Regulatory Authority",
        "content": "\nIn banking supervision the emphasis has moved from scrutiny of financial statements at a point-in-time to being more analytical, e.g. observing differences over time, individual firms, and variables. But, complicating the analysis, the firms can submit data at different frequencies and their financial years do not always align with the calendar year.\u00a0KNIME Analytics Platform\u00a0is used to get the data on the same basis \u2013 for example to interpolate quarterly submissions into monthly, to calculate CYTD out of FYTD, to create rolling 12-month variables, and more. The KNIME workflow is deployed via\u00a0KNIME Server,\u00a0which pushes the prepared data to Tableau Server (using the native\u00a0Tableau Integration), where it is underpinning standardized dashboards and enables supervisors to perform flexible ad-hoc analysis.\n\n"
    },
    {
        "id": 55,
        "url": "https://www.knime.com/success-story/how-rabobank-got-300-auditors-run-anomaly-detection-independently",
        "title": "How Rabobank got 300 auditors to run anomaly detection independently",
        "company": "Rabobank",
        "content": "\nMoney laundering makes large amounts of money that have been obtained through illegal activity appear as if they have come from a legitimate source, and avoiding this is a top priority for auditing teams. The key element in detecting money laundering is understanding known vs unknown patterns.\nSenior auditors at Rabobank are first interviewed to understand what identifies the behavior of people with wrong intentions. Those hypotheses are then translated into a set of defined business rules. KNIME helps significantly by defining these rules and applying them to the entire portfolio. This is one of the biggest wins as it increases quality assurance. This is because instead of a sample of twenty, it\u2019s possible to test, approve, and show across an entire population to uncover known patterns.\nHowever, in money laundering, unknown patterns are more interesting. The chances of missing something are also much higher because individuals with bad intentions quickly learn how to avoid patterns that are already known. Anomaly detection, cluster analysis, and text mining are a few ways to find unknown patterns. With machine learning, it\u2019s possible to learn what the currently unknown patterns are.\n\nA machine learning approach made for 300 non-tech auditors\nThis machine learning approach was applied using\u00a0KNIME\u00a0and needed to be made available to all 300 auditors - most of whom were non-tech. A KNIME workflow was developed, which is reusable and easy to handle for those who have even just a small amount of KNIME knowledge. The basic steps include accessing and inspecting the data, conducting anomaly detection, creating a visualization, and exporting and storing the output. The\u00a0Data Explorer\u00a0node enables users to first inspect the data and better understand it, which, in the case of anomaly detection, is essential. Under the hood of the basic KNIME workflow, which is largely made up of\u00a0KNIME components, are many more KNIME nodes. This approach is valuable because it hides the data science complexity, but still makes it possible to adapt, reuse, and share the workflow as needed.\n\nHow Rabobank is building a data-driven culture in audit\nRabobank management recognizes that the digital world is changing rapidly and is becoming more and more data driven. This is having a profound impact on the auditing profession, auditing services, and on the auditors themselves. Three data-driven targets were therefore set, and the message from management was clear: just get started!\nBecome more efficient with data analyticsApply more data analytics in the audits that are being doneHave datasets readily available for deep dives, follow-ups, and Q&AAbout 90% of what Rabobank does is basic analytics, which is already creating a significant impact with, for example, descriptive analytics, dashboarding, data quality profiling and reconciliation, benchmarking, as well as translating knowledge of senior auditors into business rules for testing. The remaining 10% is more advanced. For example, trend analysis, process mining, anomaly detection, cluster analysis, and text analysis.\nIt took approximately one year to understand that it\u2019s not just about hiring a data scientist to do data analytics. The Audit team therefore uses a target operating model to embed their own way of working with data. Rabobank adopted an innovation cycle, and in two days all 300 auditors were trained to become data literate and to start using KNIME themselves. Whilst it was recognized that they might not be the ones ultimately doing the analysis, emphasis was placed on the importance of being able to see and understand the potential in the data and how it can be used. Through this, the culture has become more and more data driven. The team\u2019s unique technical architecture includes KNIME as one of the key tools for data analysis.\nResults\nThe approach at Rabobank has been to start small and scale fast. To test the value of data in audit the first year consisted of seven use cases. In the second year this increased to 13, in the third year 35, and in 2020 there were 57 auditing use cases taking advantage of data analytics - which was only possible via empowering the auditors to do their own analyses.\n\u21d2\u00a0Download this Success Story here as a PDF\n"
    },
    {
        "id": 56,
        "url": "https://www.knime.com/success-story/how-rabobank-got-300-auditors-run-anomaly-detection-independently",
        "title": "How Rabobank got 300 auditors to run anomaly detection independently",
        "company": "Rabobank",
        "content": "\nMoney laundering makes large amounts of money that have been obtained through illegal activity appear as if they have come from a legitimate source, and avoiding this is a top priority for auditing teams. The key element in detecting money laundering is understanding known vs unknown patterns.\nSenior auditors at Rabobank are first interviewed to understand what identifies the behavior of people with wrong intentions. Those hypotheses are then translated into a set of defined business rules. KNIME helps significantly by defining these rules and applying them to the entire portfolio. This is one of the biggest wins as it increases quality assurance. This is because instead of a sample of twenty, it\u2019s possible to test, approve, and show across an entire population to uncover known patterns.\nHowever, in money laundering, unknown patterns are more interesting. The chances of missing something are also much higher because individuals with bad intentions quickly learn how to avoid patterns that are already known. Anomaly detection, cluster analysis, and text mining are a few ways to find unknown patterns. With machine learning, it\u2019s possible to learn what the currently unknown patterns are.\n\nA machine learning approach made for 300 non-tech auditors\nThis machine learning approach was applied using\u00a0KNIME\u00a0and needed to be made available to all 300 auditors - most of whom were non-tech. A KNIME workflow was developed, which is reusable and easy to handle for those who have even just a small amount of KNIME knowledge. The basic steps include accessing and inspecting the data, conducting anomaly detection, creating a visualization, and exporting and storing the output. The\u00a0Data Explorer\u00a0node enables users to first inspect the data and better understand it, which, in the case of anomaly detection, is essential. Under the hood of the basic KNIME workflow, which is largely made up of\u00a0KNIME components, are many more KNIME nodes. This approach is valuable because it hides the data science complexity, but still makes it possible to adapt, reuse, and share the workflow as needed.\n\nHow Rabobank is building a data-driven culture in audit\nRabobank management recognizes that the digital world is changing rapidly and is becoming more and more data driven. This is having a profound impact on the auditing profession, auditing services, and on the auditors themselves. Three data-driven targets were therefore set, and the message from management was clear: just get started!\nBecome more efficient with data analyticsApply more data analytics in the audits that are being doneHave datasets readily available for deep dives, follow-ups, and Q&AAbout 90% of what Rabobank does is basic analytics, which is already creating a significant impact with, for example, descriptive analytics, dashboarding, data quality profiling and reconciliation, benchmarking, as well as translating knowledge of senior auditors into business rules for testing. The remaining 10% is more advanced. For example, trend analysis, process mining, anomaly detection, cluster analysis, and text analysis.\nIt took approximately one year to understand that it\u2019s not just about hiring a data scientist to do data analytics. The Audit team therefore uses a target operating model to embed their own way of working with data. Rabobank adopted an innovation cycle, and in two days all 300 auditors were trained to become data literate and to start using KNIME themselves. Whilst it was recognized that they might not be the ones ultimately doing the analysis, emphasis was placed on the importance of being able to see and understand the potential in the data and how it can be used. Through this, the culture has become more and more data driven. The team\u2019s unique technical architecture includes KNIME as one of the key tools for data analysis.\nResults\nThe approach at Rabobank has been to start small and scale fast. To test the value of data in audit the first year consisted of seven use cases. In the second year this increased to 13, in the third year 35, and in 2020 there were 57 auditing use cases taking advantage of data analytics - which was only possible via empowering the auditors to do their own analyses.\n\u21d2\u00a0Download this Success Story here as a PDF\n"
    },
    {
        "id": 57,
        "url": "https://www.knime.com/success-story/how-seagate-benefited-upskilling-100s-data-citizens-knime",
        "title": "How Seagate benefited from upskilling 100s of data citizens with KNIME | KNIME",
        "company": "Seagate",
        "content": "From descriptive to predictive analytics with KNIME\nSeagate has recognized that the digital transformation is something that cannot be ignored. Every day at one of Seagate\u2019s many global sites, people are interacting with or requiring results out of data. Moving up the analytics maturity curve - from descriptive to predictive analytics - was an important strategic objective. To achieve this, a tool that could easily be rolled out across the entire organization, was simple for others to learn, and could slot in with existing tools and infrastructure such as Excel, JMP, Minitab, Tableau, Matlab, Python, and R was needed.\nKNIME was selected as the tool of choice for Seagate\u2019s data needs. The company implemented a tailored and unique Citizen Data Scientist (CDS) training program to train employees in using not only KNIME Analytics Platform, but also other complimentary data science tools. The program consisted of a mix of online and onsite trainings as well as workshops to teach users and advocate for the CDS program. Some of those attendees are now trainers and advocates themselves.\nTo help excite and encourage Seagate employees, the main messages shared at these trainings and workshops were the importance of 1) moving with the digital transformation to stay relevant in the industry 2) upskilling employees to become citizen data scientists and to enable them to get the most out of their data, independently.\nDemos created in 3 days\nWith new products or product ideas, there is always an issue with the data. Some ideas are researched for many years, with different reasons for delay. For a long time, the US R&D engineers had a long wait time to get the data they needed. KNIME Analytics Platform was used to demo the idea of pulling the data from Asia with a multi-threading scheme. It took approximately two weeks to fine tune everything, but this action is what really started to highlight the benefit of the CDS program and, more importantly, the value of using KNIME Analytics Platform. What people had been struggling with for almost a year, had taken just a few days to demo a solution. The magic behind it is that KNIME allows for multi-thread parallel queries by simply dragging and dropping nodes that would otherwise need sophisticated coding skills.\nIncreased productivity\nIn another case, KNIME is enabling the Research and Product Development team to be more creative and productive. The team\u2019s goal is to provide creative product solutions. There\u2019s no such thing as routine because there\u2019s always the need to change and adapt to the way data is analyzed. Furthermore, there\u2019s always a new way of doing things, new data to read, new structures, new formats, and more.\nKNIME Analytics Platform is the perfect solution for prototyping new ideas and presenting these ideas and results quickly. \"I would describe prototyping in KNIME as similar to building with Lego,\u201d says Debin Wang, Staff Engineer. \u201cThere are many different building blocks to choose from.\u201d KNIME, just like Lego, encourages individuals to be creative and imaginative when building something. And the best part: when R&D teams have a new requirement or change in the data structure, the KNIME workflow can quickly and easily adapt to it. In R&D, not everything works out \u2013 especially not the first time. KNIME helps to significantly shorten the exploratory cycle. What previously took an engineer weeks in line-by-line coding, can now be done in a few days. This is because KNIME, compared to other data science tools, is more flexible and intuitive.\n$1 M enabled in savings\nKNIME is used by the Recording Head Engineering Group for the dynamic modeling of downstream metrics wafers to sliders. Recording head manufacturing is very complex. The sequential layering process includes more than 1,500 steps, which form patterns of electrical conductors and magnetic material on a ceramic disk (wafer). A single wafer takes more than four months to complete. Even then, it can still require subsequent processing downstream in the supply chain before testing takes place, which could highlight processing issues. One of these 1,500 steps previously had an average duration of six months between processing and testing. That meant six months of potentially at-risk material before a fault was detected. Using KNIME, an advanced modeling workflow was created to accurately predict expected results. The user-friendly nature of KNIME also enabled Seagate to integrate the model into the existing wafer fab control system. This reduced the feedback loop from six months to four weeks, and enabled savings of over $1,000,000 for this single business area.\nIn another example, a similar methodology was used. Here the team created a KNIME a workflow, which resulted in saving $300,000 worth of scrap materials. This workflow was able to predict the materials required for future processes in a different process area, which would have otherwise been scrapped under the existing univariate process control (SPC) system.\nIn a third example, the team reduced the time spent in ensuring that the two wafer fabrication facilities (Minnesota and Northern Ireland), were completely in sync. Existing systems required a significant number of monitoring hours per week. A KNIME workflow was built to cater for the high number of false positives that the team was dealing with \u2013 eliminating them from the review process and saving valuable time each week.\n150+ users work independently with data\nThe Citizen Data Scientist program has been extremely effective. Since launching in 2017, there are over 100 general users (learners, practitioners, and analysts) and almost 50 power users (KNIME/Seagate evangelists). Employees are working more independently with their data and getting better insights, faster. They are also able to generate significant business savings \u2013 in terms of both time and money \u2013 by developing workflows and solutions to overcome business challenges or pain points. Moreover, the feeling of empowerment has been a significant motivator for Seagate employees.\u00a0\u201cKNIME has empowered people who previously may have not considered the discovery and application of machine learning techniques to dip their toes into the world of data science\u201d\u00a0says Brendan Doherty, Staff Data Scientist, Seagate Technology.\nIn 2019, Seagate purchased a KNIME Server and it\u2019s predicted that the number of both general and power users will continue to increase. Looking forward, the CDS program will likely involve AWS so that learners have access to KNIME Server applications on AWS in order to do sandbox or development work. Seagate is already getting into the next level of maturity with predictive analytics and is starting to see tremendous business impact.\nWhy KNIME?\nWith the objective of wanting to move up the analytics maturity curve, it was important to find a platform that was both strong in Guided Analytics and machine learning. KNIME was chosen not only because it checked these boxes, but because of its openness and simplicity in getting started. Attending one of the KNIME summits in Austin, Texas, where many other practitioners shared their KNIME stories, also made it clear that there was a strong community and passionate team behind the software.\nIt was very easy to get people excited about KNIME because it was (and continues to be) easy for them to learn the tool and see significant results quickly. Employees who have completed the CDS program are now some of the strongest advocates and evangelists \u2013 spreading the word and getting others on board.\nOther reasons why KNIME has become so popular at Seagate include the ability to seamlessly integrate with other tools and technologies such as\u00a0Python,\u00a0R,\u00a0Excel,\u00a0Tableau, and more. Non-coders are able to use it independently, but those who do want to code, can still do so. And lastly, it\u2019s more than just data analytics \u2013 it\u2019s data engineering, ETL, and more.\n"
    },
    {
        "id": 58,
        "url": "https://www.knime.com/success-story/how-seagate-benefited-upskilling-100s-data-citizens-knime",
        "title": "How Seagate benefited from upskilling 100s of data citizens with KNIME | KNIME",
        "company": "Seagate",
        "content": "From descriptive to predictive analytics with KNIME\nSeagate has recognized that the digital transformation is something that cannot be ignored. Every day at one of Seagate\u2019s many global sites, people are interacting with or requiring results out of data. Moving up the analytics maturity curve - from descriptive to predictive analytics - was an important strategic objective. To achieve this, a tool that could easily be rolled out across the entire organization, was simple for others to learn, and could slot in with existing tools and infrastructure such as Excel, JMP, Minitab, Tableau, Matlab, Python, and R was needed.\nKNIME was selected as the tool of choice for Seagate\u2019s data needs. The company implemented a tailored and unique Citizen Data Scientist (CDS) training program to train employees in using not only KNIME Analytics Platform, but also other complimentary data science tools. The program consisted of a mix of online and onsite trainings as well as workshops to teach users and advocate for the CDS program. Some of those attendees are now trainers and advocates themselves.\nTo help excite and encourage Seagate employees, the main messages shared at these trainings and workshops were the importance of 1) moving with the digital transformation to stay relevant in the industry 2) upskilling employees to become citizen data scientists and to enable them to get the most out of their data, independently.\nDemos created in 3 days\nWith new products or product ideas, there is always an issue with the data. Some ideas are researched for many years, with different reasons for delay. For a long time, the US R&D engineers had a long wait time to get the data they needed. KNIME Analytics Platform was used to demo the idea of pulling the data from Asia with a multi-threading scheme. It took approximately two weeks to fine tune everything, but this action is what really started to highlight the benefit of the CDS program and, more importantly, the value of using KNIME Analytics Platform. What people had been struggling with for almost a year, had taken just a few days to demo a solution. The magic behind it is that KNIME allows for multi-thread parallel queries by simply dragging and dropping nodes that would otherwise need sophisticated coding skills.\nIncreased productivity\nIn another case, KNIME is enabling the Research and Product Development team to be more creative and productive. The team\u2019s goal is to provide creative product solutions. There\u2019s no such thing as routine because there\u2019s always the need to change and adapt to the way data is analyzed. Furthermore, there\u2019s always a new way of doing things, new data to read, new structures, new formats, and more.\nKNIME Analytics Platform is the perfect solution for prototyping new ideas and presenting these ideas and results quickly. \"I would describe prototyping in KNIME as similar to building with Lego,\u201d says Debin Wang, Staff Engineer. \u201cThere are many different building blocks to choose from.\u201d KNIME, just like Lego, encourages individuals to be creative and imaginative when building something. And the best part: when R&D teams have a new requirement or change in the data structure, the KNIME workflow can quickly and easily adapt to it. In R&D, not everything works out \u2013 especially not the first time. KNIME helps to significantly shorten the exploratory cycle. What previously took an engineer weeks in line-by-line coding, can now be done in a few days. This is because KNIME, compared to other data science tools, is more flexible and intuitive.\n$1 M enabled in savings\nKNIME is used by the Recording Head Engineering Group for the dynamic modeling of downstream metrics wafers to sliders. Recording head manufacturing is very complex. The sequential layering process includes more than 1,500 steps, which form patterns of electrical conductors and magnetic material on a ceramic disk (wafer). A single wafer takes more than four months to complete. Even then, it can still require subsequent processing downstream in the supply chain before testing takes place, which could highlight processing issues. One of these 1,500 steps previously had an average duration of six months between processing and testing. That meant six months of potentially at-risk material before a fault was detected. Using KNIME, an advanced modeling workflow was created to accurately predict expected results. The user-friendly nature of KNIME also enabled Seagate to integrate the model into the existing wafer fab control system. This reduced the feedback loop from six months to four weeks, and enabled savings of over $1,000,000 for this single business area.\nIn another example, a similar methodology was used. Here the team created a KNIME a workflow, which resulted in saving $300,000 worth of scrap materials. This workflow was able to predict the materials required for future processes in a different process area, which would have otherwise been scrapped under the existing univariate process control (SPC) system.\nIn a third example, the team reduced the time spent in ensuring that the two wafer fabrication facilities (Minnesota and Northern Ireland), were completely in sync. Existing systems required a significant number of monitoring hours per week. A KNIME workflow was built to cater for the high number of false positives that the team was dealing with \u2013 eliminating them from the review process and saving valuable time each week.\n150+ users work independently with data\nThe Citizen Data Scientist program has been extremely effective. Since launching in 2017, there are over 100 general users (learners, practitioners, and analysts) and almost 50 power users (KNIME/Seagate evangelists). Employees are working more independently with their data and getting better insights, faster. They are also able to generate significant business savings \u2013 in terms of both time and money \u2013 by developing workflows and solutions to overcome business challenges or pain points. Moreover, the feeling of empowerment has been a significant motivator for Seagate employees.\u00a0\u201cKNIME has empowered people who previously may have not considered the discovery and application of machine learning techniques to dip their toes into the world of data science\u201d\u00a0says Brendan Doherty, Staff Data Scientist, Seagate Technology.\nIn 2019, Seagate purchased a KNIME Server and it\u2019s predicted that the number of both general and power users will continue to increase. Looking forward, the CDS program will likely involve AWS so that learners have access to KNIME Server applications on AWS in order to do sandbox or development work. Seagate is already getting into the next level of maturity with predictive analytics and is starting to see tremendous business impact.\nWhy KNIME?\nWith the objective of wanting to move up the analytics maturity curve, it was important to find a platform that was both strong in Guided Analytics and machine learning. KNIME was chosen not only because it checked these boxes, but because of its openness and simplicity in getting started. Attending one of the KNIME summits in Austin, Texas, where many other practitioners shared their KNIME stories, also made it clear that there was a strong community and passionate team behind the software.\nIt was very easy to get people excited about KNIME because it was (and continues to be) easy for them to learn the tool and see significant results quickly. Employees who have completed the CDS program are now some of the strongest advocates and evangelists \u2013 spreading the word and getting others on board.\nOther reasons why KNIME has become so popular at Seagate include the ability to seamlessly integrate with other tools and technologies such as\u00a0Python,\u00a0R,\u00a0Excel,\u00a0Tableau, and more. Non-coders are able to use it independently, but those who do want to code, can still do so. And lastly, it\u2019s more than just data analytics \u2013 it\u2019s data engineering, ETL, and more.\n"
    },
    {
        "id": 59,
        "url": "https://www.knime.com/success-story/how-siemens-got-thousands-business-users-working-data-knime",
        "title": "How Siemens got thousands of business users working with data in KNIME",
        "company": "Siemens",
        "content": "Getting 3,500 + data citizens working better and independently\nThe demand for automating day-to-day procedures is growing daily. On top of that, billions of bytes of data, multiple data sources, and hours of manual work put into sorting it all out, make these procedures hugely complex and time consuming.\nSince January 2018, the Data Visions Team at Siemens has been developing analytical \u2018products\u2019 to support the Digital Industries (DI) strategy and drive a data citizen approach for future collaboration - preparing for the time when data science will eventually become a commodity. The team has made it possible for not only data scientists to work with data, but also data citizens \u2013 those charged with pulling insights out of data \u2013 to make decisions and drive change.\nToday at Siemens over 3,500 data citizens globally are working better and more independently with data using KNIME Analytics Platform and other integrated software. KNIME has also become an invaluable tool for Robotic Process Automation (RPA), by completely automating many mundane, manual tasks. This has freed up lots of time to work on other areas of the business.\n\nExample: automating competitive analyses with text mining and web crawling\nOne of the more recent projects that the Data Visions team got involved in, was automating competitive analyses using financial statements from the internet. The projects\u2019 objectives were to search for financial reports on company websites, extract relevant financial statements out of the PDF reports, and transform the statements into a structured format and prepare an internal report. From a business perspective, the requirements were to:\nAutomate the extraction of financial reports from the web using a crawlerIdentify key KPIs using template-based text analysisIntegrate the data flow into an existing report workflow\nSaving 30 hours per quarter\nPreviously, the user did a manual website check of the competition with (often) incomplete information. The repetitive task of analyzing competitors was repeated every quarter in a manual and time-consuming process: search competitor website for PDF, extract relevant information, prepare finance report, send to management. The Siemens Data Visions team built a process using\u00a0KNIME Analytics Platform\u00a0to automate this entire process. A KNIME workflow, built by a team of data scientists, crawls the competitor\u2019s website and downloads financial reports as PDFs. This step is repeated automatically across different websites. The same KNIME workflow then applies data mining, classifies document types, and extracts values out of the PDF. Then using the\u00a0KNIME Integration with Tableau, a report is created and published in an interactive dashboard. Here, the end user can simply open the dashboard and view the results. KNIME has saved this person approximately 30 hours every quarter. That time is now spent on valuable tasks such as analyzing and interpreting the aggregated statements.\n"
    },
    {
        "id": 60,
        "url": "https://www.knime.com/success-story/how-siemens-got-thousands-business-users-working-data-knime",
        "title": "How Siemens got thousands of business users working with data in KNIME",
        "company": "Siemens",
        "content": "Getting 3,500 + data citizens working better and independently\nThe demand for automating day-to-day procedures is growing daily. On top of that, billions of bytes of data, multiple data sources, and hours of manual work put into sorting it all out, make these procedures hugely complex and time consuming.\nSince January 2018, the Data Visions Team at Siemens has been developing analytical \u2018products\u2019 to support the Digital Industries (DI) strategy and drive a data citizen approach for future collaboration - preparing for the time when data science will eventually become a commodity. The team has made it possible for not only data scientists to work with data, but also data citizens \u2013 those charged with pulling insights out of data \u2013 to make decisions and drive change.\nToday at Siemens over 3,500 data citizens globally are working better and more independently with data using KNIME Analytics Platform and other integrated software. KNIME has also become an invaluable tool for Robotic Process Automation (RPA), by completely automating many mundane, manual tasks. This has freed up lots of time to work on other areas of the business.\n\nExample: automating competitive analyses with text mining and web crawling\nOne of the more recent projects that the Data Visions team got involved in, was automating competitive analyses using financial statements from the internet. The projects\u2019 objectives were to search for financial reports on company websites, extract relevant financial statements out of the PDF reports, and transform the statements into a structured format and prepare an internal report. From a business perspective, the requirements were to:\nAutomate the extraction of financial reports from the web using a crawlerIdentify key KPIs using template-based text analysisIntegrate the data flow into an existing report workflow\nSaving 30 hours per quarter\nPreviously, the user did a manual website check of the competition with (often) incomplete information. The repetitive task of analyzing competitors was repeated every quarter in a manual and time-consuming process: search competitor website for PDF, extract relevant information, prepare finance report, send to management. The Siemens Data Visions team built a process using\u00a0KNIME Analytics Platform\u00a0to automate this entire process. A KNIME workflow, built by a team of data scientists, crawls the competitor\u2019s website and downloads financial reports as PDFs. This step is repeated automatically across different websites. The same KNIME workflow then applies data mining, classifies document types, and extracts values out of the PDF. Then using the\u00a0KNIME Integration with Tableau, a report is created and published in an interactive dashboard. Here, the end user can simply open the dashboard and view the results. KNIME has saved this person approximately 30 hours every quarter. That time is now spent on valuable tasks such as analyzing and interpreting the aggregated statements.\n"
    },
    {
        "id": 61,
        "url": "https://www.knime.com/success-story/automating-parsing-1000s-safety-data-sheets-mitigate-risk",
        "title": "Automating the parsing of 1000s of Safety Data Sheets to mitigate risk | KNIME",
        "company": "Soluzioni Informatiche",
        "content": "Automating text mining with KNIME\nSDS from different sources, customers, and providers are gathered. The user uploads either a single PDF, a library of PDFs, or a PDF-containing folder, as well as an Excel file with the list of all the requested phrases to be updated, to a KNIME workflow - which can be deployed on\u00a0KNIME Server\u00a0if more computational power is needed.\u00a0Text mining\u00a0nodes are applied to the result of the\u00a0Tika Parser\u00a0to extract all sentences composing each file. Every sentence, using string or regex manipulation, is analyzed by searching the Chemical Abstracts Service (CAS) number, product name, and all risk phrases. A try and catch construct helps with large variations in the input files. The results report the file name, product name, all the CAS numbers retrieved in each document, and all the retrieved phrases, which are matched with the defined user list.\n\u21d2\u00a0Download Workflow from KNIME Hub\n\nWhy KNIME\nThe open source\u00a0KNIME Analytics Platform\u00a0makes this task not only faster, but also reduces the risk of human error. The\u00a0Tika Parser node\u00a0enables the retrieval of meta information from each file, the try/catch errors construct effectively avoids workflow errors, and regex code in a java snippet isolates CAS numbers from PDFs.\n\u21d2\u00a0Download this Innovation Note as a PDF\n\n"
    },
    {
        "id": 62,
        "url": "https://www.knime.com/success-story/automating-parsing-1000s-safety-data-sheets-mitigate-risk",
        "title": "Automating the parsing of 1000s of Safety Data Sheets to mitigate risk | KNIME",
        "company": "Soluzioni Informatiche",
        "content": "Automating text mining with KNIME\nSDS from different sources, customers, and providers are gathered. The user uploads either a single PDF, a library of PDFs, or a PDF-containing folder, as well as an Excel file with the list of all the requested phrases to be updated, to a KNIME workflow - which can be deployed on\u00a0KNIME Server\u00a0if more computational power is needed.\u00a0Text mining\u00a0nodes are applied to the result of the\u00a0Tika Parser\u00a0to extract all sentences composing each file. Every sentence, using string or regex manipulation, is analyzed by searching the Chemical Abstracts Service (CAS) number, product name, and all risk phrases. A try and catch construct helps with large variations in the input files. The results report the file name, product name, all the CAS numbers retrieved in each document, and all the retrieved phrases, which are matched with the defined user list.\n\u21d2\u00a0Download Workflow from KNIME Hub\n\nWhy KNIME\nThe open source\u00a0KNIME Analytics Platform\u00a0makes this task not only faster, but also reduces the risk of human error. The\u00a0Tika Parser node\u00a0enables the retrieval of meta information from each file, the try/catch errors construct effectively avoids workflow errors, and regex code in a java snippet isolates CAS numbers from PDFs.\n\u21d2\u00a0Download this Innovation Note as a PDF\n\n"
    },
    {
        "id": 63,
        "url": "https://www.knime.com/success-story/how-automate-employee-expense-reimbursements-knime",
        "title": "How to automate employee expense reimbursements with KNIME | KNIME",
        "company": "Soluzioni Informatiche",
        "content": "The challenge\nEach month, travel and event expenses are collected, summarized, and analyzed in order to match costs with successful sales. Previously, the accounting manager gathered Excel files from every employee and copied and pasted these in to a global Excel file. When the repository of these Excel files was moved to a shared folder (OneDrive), it was discovered that the data blending abilities of KNIME could be used to create the summary and, with the Table to PDF node, create a very simple expenses report.\nThe solution\nA previously defined Excel template simplifies accounting file filling by each employee. A team of data scientists creates a KNIME workflow that can be deployed as an analytical application in the KNIME WebPortal. The Accounting Manager selects the starting folder, which contains different sub-folders (one for each month) with the Excel files of each employee and the summary Excel files generated from the previously executed workflow. The most recent summary Excel file is identified and split from the other files. Individual Excel files are renamed to avoid future reprocessing. After concatenating all the individual accounting files, the workflow appends the resulting table to the split summary Excel file and writes a new summary Excel file with the process date in the name. Lastly, a simple PDF report is sent to the HR manager with a list of all the reimbursements.\n\u21d2\u00a0Download Workflow from KNIME Hub\n\nWhy KNIME\nKNIME Analytics Platform is used to collect and summarize employee expenses. Nodes like Excel Reader, Excel Writer, Table to PDF, and Copy/Move make the work of an accounting manager easier and less prone to errors. The workflow, deployed via KNIME Server, is available as an analytical application in the KNIME WebPortal. It provides a simple interface for users to enter expenses and the executed workflow automatically generates the required reports.\n\u21d2\u00a0Download this Innovation Note as a PDF\n"
    },
    {
        "id": 64,
        "url": "https://www.knime.com/success-story/how-automate-employee-expense-reimbursements-knime",
        "title": "How to automate employee expense reimbursements with KNIME | KNIME",
        "company": "Soluzioni Informatiche",
        "content": "The challenge\nEach month, travel and event expenses are collected, summarized, and analyzed in order to match costs with successful sales. Previously, the accounting manager gathered Excel files from every employee and copied and pasted these in to a global Excel file. When the repository of these Excel files was moved to a shared folder (OneDrive), it was discovered that the data blending abilities of KNIME could be used to create the summary and, with the Table to PDF node, create a very simple expenses report.\nThe solution\nA previously defined Excel template simplifies accounting file filling by each employee. A team of data scientists creates a KNIME workflow that can be deployed as an analytical application in the KNIME WebPortal. The Accounting Manager selects the starting folder, which contains different sub-folders (one for each month) with the Excel files of each employee and the summary Excel files generated from the previously executed workflow. The most recent summary Excel file is identified and split from the other files. Individual Excel files are renamed to avoid future reprocessing. After concatenating all the individual accounting files, the workflow appends the resulting table to the split summary Excel file and writes a new summary Excel file with the process date in the name. Lastly, a simple PDF report is sent to the HR manager with a list of all the reimbursements.\n\u21d2\u00a0Download Workflow from KNIME Hub\n\nWhy KNIME\nKNIME Analytics Platform is used to collect and summarize employee expenses. Nodes like Excel Reader, Excel Writer, Table to PDF, and Copy/Move make the work of an accounting manager easier and less prone to errors. The workflow, deployed via KNIME Server, is available as an analytical application in the KNIME WebPortal. It provides a simple interface for users to enter expenses and the executed workflow automatically generates the required reports.\n\u21d2\u00a0Download this Innovation Note as a PDF\n"
    },
    {
        "id": 65,
        "url": "https://www.knime.com/success-story/how-sparkasse-improved-success-product-campaigns-200-knime",
        "title": "How Sparkasse Improved the Success of Product Campaigns up to 200% with KNIME | KNIME",
        "company": "Sparkasse K\u00f6lnBonn",
        "content": "An intuitive platform provided a lingua franca between discipline\nChris\u2019 team came from a variety of backgrounds \u2013 some from information science, but others came from market research and finance. Some had coding experience, while others specialized in Business Intelligence tools and Excel.\nAs a starting point, KNIME\u2019s low-code, no-code interface provided a common ground. The tool was sophisticated enough to allow for his data science experts to work with machine learning models, but intuitive enough for the rest of the team to use.\nThe team could now collaborate across disciplines. At the same time, more of his team could start working with advanced techniques. After a few weeks, anyone could use the same interface to build and train models with or without code. Those who were newer to data science were able to upskill from the pre-built workflows available on KNIME Hub and KNIME\u2019s data-scientist-led tutorials.\nOnce the whole team was working in the same environment, they were able to start collaborating more efficiently. Together, they started prototyping models for product-campaigns and comparing the results to those of their expert-based approach. The team was able to learn about and apply popular techniques like Random Forest and Gradient Boosted Trees for classification without writing a single line of Python. Once the models were built, they could automate their execution and compare results.\nWhen the experiment proved successful, the previous, experience-based recommendation paradigm was then replaced by a modern, algorithm-based approach.\n\nEnd-to-end coverage meant faster experiments\nThe first product campaign experiment, including building the model, only took a few months. Because KNIME offered a complete, end-to-end data science platform, the entire process \u2013 from data understanding and modeling to deployment \u2013 could be done in one environment. After testing and validating the success of the first experiment they were able to quickly move on to new, more strategic projects.\n\u201cWe put the model in a horse race with the past expert-based approach. In almost every experiment, the model outperformed the expert-based approach \u2013 and often gave us ideas for what else we could try,\u201d said Christian.\nOne such project was a sustainability campaign. In addition to global scoring models, provided by their parent organization \u2013 Savings Banks Financial Group \u2013 they could supplement with models that focused on their local, specific goals. As sustainability had been a major strategic initiative at Sparkasse K\u00f6lnBonn, Chris\u2019 team decided to complement the standard scores and test their own, customized models on \u201csustainability\u201d products.\nNow the team can spin up models for many customized product campaigns relatively quickly, rather than relying on global scoring. According to Chris, \u201cEach campaign performs from 50 to 200% better with the models we built, trained and deployed in KNIME.\u201d\n\nSelf-documenting meant easily explaining results to stakeholders\nThe corporate development department strives for leadership buy-in from the start, so the team made sure to communicate its initiatives early and often.\nThe team now offers a digital platform \u2013 called \u201cdigital driving license\u201d program \u2013 where anyone is invited to share knowledge on digital topics like e.g. crypto currencies throughout the organization. In this context, Chris\u2019 team uses KNIME\u2019s self-documenting interface to communicate and explain how these algorithm-based recommendations work and why they outperform the past, expert-based recommendation system.\n\nWhy KNIME?\nKNIME\u2019s no-code/low-code approach paired with the completeness of the platform resulted in fast iterations and easy communication both within, and outside the Advanced Analytics Department. However, it was KNIME\u2019s open-source approach that initially sparked the team\u2019s interest, since it removed any barrier to entry associated with cost.\nDownload this Success Story here as a PDF.\n"
    },
    {
        "id": 66,
        "url": "https://www.knime.com/success-story/how-sparkasse-improved-success-product-campaigns-200-knime",
        "title": "How Sparkasse Improved the Success of Product Campaigns up to 200% with KNIME | KNIME",
        "company": "Sparkasse K\u00f6lnBonn",
        "content": "An intuitive platform provided a lingua franca between discipline\nChris\u2019 team came from a variety of backgrounds \u2013 some from information science, but others came from market research and finance. Some had coding experience, while others specialized in Business Intelligence tools and Excel.\nAs a starting point, KNIME\u2019s low-code, no-code interface provided a common ground. The tool was sophisticated enough to allow for his data science experts to work with machine learning models, but intuitive enough for the rest of the team to use.\nThe team could now collaborate across disciplines. At the same time, more of his team could start working with advanced techniques. After a few weeks, anyone could use the same interface to build and train models with or without code. Those who were newer to data science were able to upskill from the pre-built workflows available on KNIME Hub and KNIME\u2019s data-scientist-led tutorials.\nOnce the whole team was working in the same environment, they were able to start collaborating more efficiently. Together, they started prototyping models for product-campaigns and comparing the results to those of their expert-based approach. The team was able to learn about and apply popular techniques like Random Forest and Gradient Boosted Trees for classification without writing a single line of Python. Once the models were built, they could automate their execution and compare results.\nWhen the experiment proved successful, the previous, experience-based recommendation paradigm was then replaced by a modern, algorithm-based approach.\n\nEnd-to-end coverage meant faster experiments\nThe first product campaign experiment, including building the model, only took a few months. Because KNIME offered a complete, end-to-end data science platform, the entire process \u2013 from data understanding and modeling to deployment \u2013 could be done in one environment. After testing and validating the success of the first experiment they were able to quickly move on to new, more strategic projects.\n\u201cWe put the model in a horse race with the past expert-based approach. In almost every experiment, the model outperformed the expert-based approach \u2013 and often gave us ideas for what else we could try,\u201d said Christian.\nOne such project was a sustainability campaign. In addition to global scoring models, provided by their parent organization \u2013 Savings Banks Financial Group \u2013 they could supplement with models that focused on their local, specific goals. As sustainability had been a major strategic initiative at Sparkasse K\u00f6lnBonn, Chris\u2019 team decided to complement the standard scores and test their own, customized models on \u201csustainability\u201d products.\nNow the team can spin up models for many customized product campaigns relatively quickly, rather than relying on global scoring. According to Chris, \u201cEach campaign performs from 50 to 200% better with the models we built, trained and deployed in KNIME.\u201d\n\nSelf-documenting meant easily explaining results to stakeholders\nThe corporate development department strives for leadership buy-in from the start, so the team made sure to communicate its initiatives early and often.\nThe team now offers a digital platform \u2013 called \u201cdigital driving license\u201d program \u2013 where anyone is invited to share knowledge on digital topics like e.g. crypto currencies throughout the organization. In this context, Chris\u2019 team uses KNIME\u2019s self-documenting interface to communicate and explain how these algorithm-based recommendations work and why they outperform the past, expert-based recommendation system.\n\nWhy KNIME?\nKNIME\u2019s no-code/low-code approach paired with the completeness of the platform resulted in fast iterations and easy communication both within, and outside the Advanced Analytics Department. However, it was KNIME\u2019s open-source approach that initially sparked the team\u2019s interest, since it removed any barrier to entry associated with cost.\nDownload this Success Story here as a PDF.\n"
    },
    {
        "id": 67,
        "url": "https://www.knime.com/success-story/how-sport-england-gained-efficiencies-automated-campaign-reporting",
        "title": "How Sport England gained efficiencies with automated campaign reporting | KNIME",
        "company": "Sport England",
        "content": "Increasing the efficiency of social media campaign reporting to get to insights quicker\nPreviously, the process required logging in to each of the social media platforms, collecting statistics from accounts, transforming the data so it was comparable, before visualizing it in Excel and producing a report in PowerPoint. This was a labor-intensive process, which could only be done by one person and created a significant lag in reporting. The delay in reporting meant that while a view of overall success was available to senior management, information was not available for operational decisions where a nimble response to events might be required. Off the shelf solutions were considered but could not provide the targeted approach or custom visuals required by Sport England for their This Girl Can campaign.\nBlending technologies for an automated, bespoke, and branded reporting solution\nAtos\u00a0(a Trusted KNIME Partner) created workflows in KNIME Analytics Platform to collect, transform, join, and write data from social media platforms to an SQL database on Azure, where it was visualized in Power BI. KNIME Server was used to schedule data collection and an analytical application was made available via the KNIME WebPortal to allow a supplementary data file to be uploaded in a user-friendly way by their admin team.\nAtos used functionality already built into KNIME to connect to\u00a0Twitter,\u00a0Python\u00a0nodes to connect to, navigate, and retrieve data from Facebook and Instagram graph databases, as well as web-scraping for data from Medium. Python nodes were also used to query Google and YouTube APIs. Data was transformed and joined in KNIME to provide a comparable snapshot of activity over the previous 24 hours when the collected data ranged from cumulative counts to six-week windows.\nThe workflows were scheduled to run shortly after midnight on KNIME Server, which was running in a virtual machine on Sport England\u2019s Azure platform. Data was saved to an SQL database where it was available to Power BI (using the\u00a0KNIME PowerBI Integration) for visualization.\nInitially Azure Cognitive Services were to perform sentiment analysis on tweets in Power BI, but this functionality was folded back into KNIME. Therefore, a tweet was only ever passed to Cognitive Services once, and the score was stored in the database for cost and efficiency savings.\nAtos created custom metrics for rating features such as a user\u2019s influence, which could be explained, understood, and adapted as necessary. These were then visualized to Sport England\u2019s specification in adherence with their internal style guides and branding to make it their tool. This highlighted that the data that was important to them and the success of their campaign.\nAutomatic data processing and increased reporting frequency\nThe frequency of reporting was increased from monthly to daily. Instead of relying on a single person to generate the report, which would take half a day, the latest data was collected and processed automatically and made available via a Power BI report at the beginning of the day. The Power BI report was more useful than the PowerPoint report because it allowed users to apply multiple filters to find the data they needed \u2013 instead of relying on the analyst to anticipate requirements and generate insight.\nWhy KNIME?\nKNIME Analytics Platform\u00a0makes it easy to seamlessly blend core functionality for data manipulation with highly complex and bespoke operations for data collection and cleaning using R and Python. The visual workflow builder enables workflow creators to explain the process to the media team and others outside data specialist roles who don\u2019t necessarily have the technical data science knowledge, drawing parallels to how data was treated in the manual process. This means the solution is less of a \u201cblack box\u201d, but rather something that those who use the data can have confidence in because they understand how the parts fit together.\nKNIME Server\u00a0makes scheduling data collection a simple task and provides feedback that the workflows are executed without errors. Additional benefits include the ability to work remotely in the cloud, and the ability for Sport England to easily manage the data governance to sandbox this project from their other data.\nThe licensing structure of KNIME Software provides unparalleled flexibility and inclusivity. The free and open source KNIME Analytics Platform can be installed on multiple computers and VMs for development. This allows all team members to participate and learn, not only those already sufficiently proficient to merit a paid-for seat. The commercial KNIME Server is used for the delivery of business value; workflow scheduling, as well as making workflows easily accessible for business users via the WebPortal. This means costs are only incurred once business benefits have been proven and are ready to be used by the client. KNIME Server functionality is great because it \u201cjust works\u201d. It is simple to upload workflows and set scheduling. It is also easy for administrators to set and manage permissions.\nThe KNIME team is responsive and friendly in helping. There is also an active community in the\u00a0KNIME Forum, which is monitored by KNIME, so users often receive assistance from the experts. Being a KNIME Partner, this gives Atos the confidence to innovate for clients, knowing they have willing support to complete a workflow or server integration if needed.\n"
    },
    {
        "id": 68,
        "url": "https://www.knime.com/success-story/how-sport-england-gained-efficiencies-automated-campaign-reporting",
        "title": "How Sport England gained efficiencies with automated campaign reporting | KNIME",
        "company": "Sport England",
        "content": "Increasing the efficiency of social media campaign reporting to get to insights quicker\nPreviously, the process required logging in to each of the social media platforms, collecting statistics from accounts, transforming the data so it was comparable, before visualizing it in Excel and producing a report in PowerPoint. This was a labor-intensive process, which could only be done by one person and created a significant lag in reporting. The delay in reporting meant that while a view of overall success was available to senior management, information was not available for operational decisions where a nimble response to events might be required. Off the shelf solutions were considered but could not provide the targeted approach or custom visuals required by Sport England for their This Girl Can campaign.\nBlending technologies for an automated, bespoke, and branded reporting solution\nAtos\u00a0(a Trusted KNIME Partner) created workflows in KNIME Analytics Platform to collect, transform, join, and write data from social media platforms to an SQL database on Azure, where it was visualized in Power BI. KNIME Server was used to schedule data collection and an analytical application was made available via the KNIME WebPortal to allow a supplementary data file to be uploaded in a user-friendly way by their admin team.\nAtos used functionality already built into KNIME to connect to\u00a0Twitter,\u00a0Python\u00a0nodes to connect to, navigate, and retrieve data from Facebook and Instagram graph databases, as well as web-scraping for data from Medium. Python nodes were also used to query Google and YouTube APIs. Data was transformed and joined in KNIME to provide a comparable snapshot of activity over the previous 24 hours when the collected data ranged from cumulative counts to six-week windows.\nThe workflows were scheduled to run shortly after midnight on KNIME Server, which was running in a virtual machine on Sport England\u2019s Azure platform. Data was saved to an SQL database where it was available to Power BI (using the\u00a0KNIME PowerBI Integration) for visualization.\nInitially Azure Cognitive Services were to perform sentiment analysis on tweets in Power BI, but this functionality was folded back into KNIME. Therefore, a tweet was only ever passed to Cognitive Services once, and the score was stored in the database for cost and efficiency savings.\nAtos created custom metrics for rating features such as a user\u2019s influence, which could be explained, understood, and adapted as necessary. These were then visualized to Sport England\u2019s specification in adherence with their internal style guides and branding to make it their tool. This highlighted that the data that was important to them and the success of their campaign.\nAutomatic data processing and increased reporting frequency\nThe frequency of reporting was increased from monthly to daily. Instead of relying on a single person to generate the report, which would take half a day, the latest data was collected and processed automatically and made available via a Power BI report at the beginning of the day. The Power BI report was more useful than the PowerPoint report because it allowed users to apply multiple filters to find the data they needed \u2013 instead of relying on the analyst to anticipate requirements and generate insight.\nWhy KNIME?\nKNIME Analytics Platform\u00a0makes it easy to seamlessly blend core functionality for data manipulation with highly complex and bespoke operations for data collection and cleaning using R and Python. The visual workflow builder enables workflow creators to explain the process to the media team and others outside data specialist roles who don\u2019t necessarily have the technical data science knowledge, drawing parallels to how data was treated in the manual process. This means the solution is less of a \u201cblack box\u201d, but rather something that those who use the data can have confidence in because they understand how the parts fit together.\nKNIME Server\u00a0makes scheduling data collection a simple task and provides feedback that the workflows are executed without errors. Additional benefits include the ability to work remotely in the cloud, and the ability for Sport England to easily manage the data governance to sandbox this project from their other data.\nThe licensing structure of KNIME Software provides unparalleled flexibility and inclusivity. The free and open source KNIME Analytics Platform can be installed on multiple computers and VMs for development. This allows all team members to participate and learn, not only those already sufficiently proficient to merit a paid-for seat. The commercial KNIME Server is used for the delivery of business value; workflow scheduling, as well as making workflows easily accessible for business users via the WebPortal. This means costs are only incurred once business benefits have been proven and are ready to be used by the client. KNIME Server functionality is great because it \u201cjust works\u201d. It is simple to upload workflows and set scheduling. It is also easy for administrators to set and manage permissions.\nThe KNIME team is responsive and friendly in helping. There is also an active community in the\u00a0KNIME Forum, which is monitored by KNIME, so users often receive assistance from the experts. Being a KNIME Partner, this gives Atos the confidence to innovate for clients, knowing they have willing support to complete a workflow or server integration if needed.\n"
    },
    {
        "id": 69,
        "url": "https://www.knime.com/success-story/how-star-cooperative-leverages-knime-optimize-pricing-strategy",
        "title": "How Star Cooperative leverages KNIME to optimize pricing strategy | KNIME",
        "company": "Star Cooperative",
        "content": "A 5-step process to test pricing\nFive steps using KNIME:\nAnalyze product, price, and sales data to understand which factors influence sales performance on a granular level. KNIME is used to merge and transform data from different data sources. Grouping and pivoting as well as data visualization nodes are used when a deep-dive into the data is needed.Discover the need for price adaption by analyzing and comparing the data. Pricing expertise as well as product, customer, and market knowledge are required to interpret the prepared data from Phase 1.Adjust prices systematically \u2013 for example with the value-based pricing approach or the price line optimization approach. Both take different value components of products into account and therefore promote data-driven pricing. KNIME is used to realize both approaches. For a value-based pricing approach, the KNIME rule engine helps apply a complex set of value driver rules. For a price line optimization approach, different statistical models for clustering and regression are applied and combined.Calculate and evaluate the effects of the price adjustment. KNIME is used to simulate effects, however pricing expertise is needed to check the plausibility of new prices and to weigh up different pricing measures.Approve the final prices and document these within the pricing system (completed by the pricing manager).\u21d2\u00a0Download Workflow from KNIME Hub\n"
    },
    {
        "id": 70,
        "url": "https://www.knime.com/success-story/how-star-cooperative-leverages-knime-optimize-pricing-strategy",
        "title": "How Star Cooperative leverages KNIME to optimize pricing strategy | KNIME",
        "company": "Star Cooperative",
        "content": "A 5-step process to test pricing\nFive steps using KNIME:\nAnalyze product, price, and sales data to understand which factors influence sales performance on a granular level. KNIME is used to merge and transform data from different data sources. Grouping and pivoting as well as data visualization nodes are used when a deep-dive into the data is needed.Discover the need for price adaption by analyzing and comparing the data. Pricing expertise as well as product, customer, and market knowledge are required to interpret the prepared data from Phase 1.Adjust prices systematically \u2013 for example with the value-based pricing approach or the price line optimization approach. Both take different value components of products into account and therefore promote data-driven pricing. KNIME is used to realize both approaches. For a value-based pricing approach, the KNIME rule engine helps apply a complex set of value driver rules. For a price line optimization approach, different statistical models for clustering and regression are applied and combined.Calculate and evaluate the effects of the price adjustment. KNIME is used to simulate effects, however pricing expertise is needed to check the plausibility of new prices and to weigh up different pricing measures.Approve the final prices and document these within the pricing system (completed by the pricing manager).\u21d2\u00a0Download Workflow from KNIME Hub\n"
    },
    {
        "id": 71,
        "url": "https://www.knime.com/success-story/how-tata-steel-gained-savings-switching-excel-knime",
        "title": "How Tata Steel gained savings switching from Excel to KNIME | KNIME",
        "company": "Tata Steel",
        "content": "Contract management solution prevents overcharging & incorrect tariffs\nChallenge: Employees of contractors could perform jobs at multiple locations in a single day, which made it difficult to see when an employee started and ended their working day.Solution: Timesheet data is imported and combined to identify employees with long working days \u2014 some with over 20 hours. Auditors can now identify other errors due to incorrect surcharges for overtime, weekend/night shift, and public holidays.Why KNIME: It's easy to show and explain which anomalies had been detected in timesheets. Since the total population was reviewed, the exact impact of these errors could be calculated. Data transformation, e.g., working with date and time fields, is much easier in KNIME than in Excel. \nImproved sales price analysis: invoice & order checks ensure correct pricing\nChallenge: Prices not always invoiced correctly, regular customer complaints, complex pricing structure, and various order fulfillment strategies. Furthermore, a legacy system made it difficult for sales staff to identify whether sales orders and prices were correctly recorded.Solution: a sales-price analysis was performed by taking extracts of all sales orders and invoices raised within a given period. Prices between previous periods were compared, as well as prices of similar products. Price corrections and provisional pricing were also considered. For the identified potential erroneous prices, the root causes were identified by the sales staff, and controls were enhanced.Why KNIME: Running the workflows takes only a few minutes in KNIME. The majority of the time is used to extract new data and check for any new inconsistencies. More time is saved by reusing the same analysis in other sales sectors. \nRemoval of customer order & sales order inconsistencies in product master data\nChallenge: The product master data consists of a large number of very different product specifications including dimensions, tolerances, and mechanical and chemical properties.Solution: A traditional audit was performed and a sample of twenty sales orders was manually tested, which uncovered several inconsistencies. This was extended to support the business by performing an extensive reconciliation of the product master data for all orders and not just for the selected orders.Why KNIME: KNIME was able to connect and extract data from existing and legacy systems using a single workflow. Furthermore, it was easy to train the manufacturing and sales staff to reperform these analyses themselves.Benefits: Time savings of up to 95%Immediately after getting started with KNIME, and whilst still being in the experimental phase with hobby projects, time savings of up to 95% could be achieved in some use cases. Furthermore, by comparing KNIME results with Excel reports, several data issues and calculation errors were identified in the Excel reports. This was an even stronger motivation to get started with KNIME.\nWhy KNIME\nKNIME\u2019s visual programming environment makes data analytics accessible to people who don\u2019t have coding or scripting backgrounds. In a data-driven culture, this is essential because non-experts can independently use the available data, enhance control procedures, and turn insights into business value. It does take time to learn KNIME and basic data analysis skills are required to build workflows.\nIn the case of Tata Steel Europe, it took approximately 10 hours to get familiar with the workbench and 100-200 hours to become confident in building workflows. However, the time investment has significant rewards and is worth it. Using KNIME saves time.\nWorkflows are reusable and shareable, which reduces the need to recreate them from scratch for every single project. Repeated steps such as mundane pre-processing tasks can be captured as a component and used in other workflows or by other colleagues. Configuration changes can also be programmed to update across all components if needed \u2013 guaranteeing consistency in business processes. All KNIME workflows are self-documenting, meaning knowledge is stored in the workflow itself and not in the mind of an employee. Not everyone has to become a KNIME specialist, but with basic knowledge, even non-technical auditors can contribute meaningfully to data analytics in audit.\n"
    },
    {
        "id": 72,
        "url": "https://www.knime.com/success-story/how-tata-steel-gained-savings-switching-excel-knime",
        "title": "How Tata Steel gained savings switching from Excel to KNIME | KNIME",
        "company": "Tata Steel",
        "content": "Contract management solution prevents overcharging & incorrect tariffs\nChallenge: Employees of contractors could perform jobs at multiple locations in a single day, which made it difficult to see when an employee started and ended their working day.Solution: Timesheet data is imported and combined to identify employees with long working days \u2014 some with over 20 hours. Auditors can now identify other errors due to incorrect surcharges for overtime, weekend/night shift, and public holidays.Why KNIME: It's easy to show and explain which anomalies had been detected in timesheets. Since the total population was reviewed, the exact impact of these errors could be calculated. Data transformation, e.g., working with date and time fields, is much easier in KNIME than in Excel. \nImproved sales price analysis: invoice & order checks ensure correct pricing\nChallenge: Prices not always invoiced correctly, regular customer complaints, complex pricing structure, and various order fulfillment strategies. Furthermore, a legacy system made it difficult for sales staff to identify whether sales orders and prices were correctly recorded.Solution: a sales-price analysis was performed by taking extracts of all sales orders and invoices raised within a given period. Prices between previous periods were compared, as well as prices of similar products. Price corrections and provisional pricing were also considered. For the identified potential erroneous prices, the root causes were identified by the sales staff, and controls were enhanced.Why KNIME: Running the workflows takes only a few minutes in KNIME. The majority of the time is used to extract new data and check for any new inconsistencies. More time is saved by reusing the same analysis in other sales sectors. \nRemoval of customer order & sales order inconsistencies in product master data\nChallenge: The product master data consists of a large number of very different product specifications including dimensions, tolerances, and mechanical and chemical properties.Solution: A traditional audit was performed and a sample of twenty sales orders was manually tested, which uncovered several inconsistencies. This was extended to support the business by performing an extensive reconciliation of the product master data for all orders and not just for the selected orders.Why KNIME: KNIME was able to connect and extract data from existing and legacy systems using a single workflow. Furthermore, it was easy to train the manufacturing and sales staff to reperform these analyses themselves.Benefits: Time savings of up to 95%Immediately after getting started with KNIME, and whilst still being in the experimental phase with hobby projects, time savings of up to 95% could be achieved in some use cases. Furthermore, by comparing KNIME results with Excel reports, several data issues and calculation errors were identified in the Excel reports. This was an even stronger motivation to get started with KNIME.\nWhy KNIME\nKNIME\u2019s visual programming environment makes data analytics accessible to people who don\u2019t have coding or scripting backgrounds. In a data-driven culture, this is essential because non-experts can independently use the available data, enhance control procedures, and turn insights into business value. It does take time to learn KNIME and basic data analysis skills are required to build workflows.\nIn the case of Tata Steel Europe, it took approximately 10 hours to get familiar with the workbench and 100-200 hours to become confident in building workflows. However, the time investment has significant rewards and is worth it. Using KNIME saves time.\nWorkflows are reusable and shareable, which reduces the need to recreate them from scratch for every single project. Repeated steps such as mundane pre-processing tasks can be captured as a component and used in other workflows or by other colleagues. Configuration changes can also be programmed to update across all components if needed \u2013 guaranteeing consistency in business processes. All KNIME workflows are self-documenting, meaning knowledge is stored in the workflow itself and not in the mind of an employee. Not everyone has to become a KNIME specialist, but with basic knowledge, even non-technical auditors can contribute meaningfully to data analytics in audit.\n"
    },
    {
        "id": 73,
        "url": "https://www.knime.com/success-story/how-travel-agency-improved-product-management-decisions-using-knime",
        "title": "How a travel agency improved product management decisions using KNIME | KNIME",
        "company": "thaltegos",
        "content": "Blending different data sources\nIn the increasingly competitive tourism market, companies offering hotel rooms, holiday packages, cruises or flights must pay close attention to their market share and customers. In specialized sub-branches of the industry, such as holiday cruises, the offer of a comprehensive and all-round attractive package to inspire travelers must be closely monitored and improved. To this end, data-driven dashboard solutions for customer-centric performance management help companies cope with the market requirements.\nIn order to get a comprehensive overview of a company\u2019s current offers on the market and focus on customer reviews, various data sources must be integrated and merged: business data, publicly available customer ratings, news data, journalist reports, and press releases. By crawling these data from a variety of different public sources using the\u00a0native KNIME Python integration\u00a0and enriching them with company and other relevant data, a comprehensive database is created, which can be fed into any dashboard-solution software.\nDeploying a Guided Analytics application\nA KNIME workflow is built, which crawls all these sources. After the data has been crawled and saved using a REST API interface, it is imported from the JSON files and converted into a KNIME table format using filtering, mapping, and data encoding. Then a set of pre-defined dictionaries is used to ensure that the customers\u2019 ratings are correctly mapped to the specific ocean carriers, the correct ships, even the cabin they stayed in. From these data, high-level data aggregations can be formed to create a compact set of rating dimensions. The workflow is then deployed in an\u00a0Azure cloud environment\u00a0as a Guided Analytics Application, making vast computational resources available to deploy in-depth descriptive analysis on data integrated from various resources, enabling alerts and notifications to company managers for improving/deteriorating products.\nResults\nWith this analytical application, companies can manage their product more in a more informed and intelligent way due to:\nComprehensive data integration generated by KNIMEEase-of-use production system, which is automated for continuous data integrationNotification and alerts for improving/deteriorating company productsWhy KNIME?KNIME provides the tools and resources to easily blend data from different sources in one visual workbench. The vast selection of nodes that are available, plus the procedures that are possible, make it easy to create solutions such as this. In this case,\u00a0JPython Function,\u00a0JSON to Table,\u00a0String Manipulation, and Chunk/List Loops nodes were heavily used. Furthermore, deploying the workflow in a\u00a0Microsoft Azure cloud environment, provides additional computational resources when needed.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 74,
        "url": "https://www.knime.com/success-story/how-travel-agency-improved-product-management-decisions-using-knime",
        "title": "How a travel agency improved product management decisions using KNIME | KNIME",
        "company": "thaltegos",
        "content": "Blending different data sources\nIn the increasingly competitive tourism market, companies offering hotel rooms, holiday packages, cruises or flights must pay close attention to their market share and customers. In specialized sub-branches of the industry, such as holiday cruises, the offer of a comprehensive and all-round attractive package to inspire travelers must be closely monitored and improved. To this end, data-driven dashboard solutions for customer-centric performance management help companies cope with the market requirements.\nIn order to get a comprehensive overview of a company\u2019s current offers on the market and focus on customer reviews, various data sources must be integrated and merged: business data, publicly available customer ratings, news data, journalist reports, and press releases. By crawling these data from a variety of different public sources using the\u00a0native KNIME Python integration\u00a0and enriching them with company and other relevant data, a comprehensive database is created, which can be fed into any dashboard-solution software.\nDeploying a Guided Analytics application\nA KNIME workflow is built, which crawls all these sources. After the data has been crawled and saved using a REST API interface, it is imported from the JSON files and converted into a KNIME table format using filtering, mapping, and data encoding. Then a set of pre-defined dictionaries is used to ensure that the customers\u2019 ratings are correctly mapped to the specific ocean carriers, the correct ships, even the cabin they stayed in. From these data, high-level data aggregations can be formed to create a compact set of rating dimensions. The workflow is then deployed in an\u00a0Azure cloud environment\u00a0as a Guided Analytics Application, making vast computational resources available to deploy in-depth descriptive analysis on data integrated from various resources, enabling alerts and notifications to company managers for improving/deteriorating products.\nResults\nWith this analytical application, companies can manage their product more in a more informed and intelligent way due to:\nComprehensive data integration generated by KNIMEEase-of-use production system, which is automated for continuous data integrationNotification and alerts for improving/deteriorating company productsWhy KNIME?KNIME provides the tools and resources to easily blend data from different sources in one visual workbench. The vast selection of nodes that are available, plus the procedures that are possible, make it easy to create solutions such as this. In this case,\u00a0JPython Function,\u00a0JSON to Table,\u00a0String Manipulation, and Chunk/List Loops nodes were heavily used. Furthermore, deploying the workflow in a\u00a0Microsoft Azure cloud environment, provides additional computational resources when needed.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 75,
        "url": "https://www.knime.com/success-story/how-hungarian-government-automated-reporting-processes-key-decision-makers",
        "title": "How the Hungarian government automated reporting processes for key decision makers | KNIME",
        "company": "The Hungarian Government",
        "content": "Developing an efficient and customer-friendly public administration\nOperating in 174 cities and in the 23 districts of the capital city, these offices have more than 1,000 competencies and responsibilities. One of them is to offer and promote a service-provider state to citizens and businesses. This initiative aims to develop an efficient and customer-friendly public administration, resulting in an improved quality of public services. Factors contributing to this include reorganizing central public administration, reforming district offices, modernizing and reducing bureaucracy, and creating so-called \u201cGovernment Windows.\u201d These windows provide information and services to citizens in more than 1,500 different types of administrative cases. To support the reform initiatives, the government mandated the development of the\u00a0Government Offices Integrated Management System, part of which is the\u00a0Management Information System.\nThe capital and county government offices, along with the district offices and Government Windows, form the largest organization in Hungary. Their diverse functions are supported by many enterprise-sized IT systems including operations management, human resources management, financial planning and accounting, customer service and customer calling, as well as document and records management. There are also many specialized administrative systems, for example social insurance.\nThere were a few shortcomings of the existing solution. There was no unified reporting dashboard that was accessible to all stakeholders. There was also no automated information flow, meaning reports took weeks to be created and delivered. Lastly, the data sources were not interconnected. The Prime Minister\u2019s office often requests information from administrative staff in the form of consolidated and aggregated reports. Previously, this was a manual, labor-intensive, and slow process.\n\nA no-code solution to produce automated, web-based reports\nA solution that could automate the collection, aggregation, and consolidation of the data from the separate systems, and produce both web-based and printed reports with current information on demand was needed. Furthermore, the government\u2019s own data analyst team needed to be able to develop new reports when needed, without coding.\nA simple, low-cost data warehouse was built on several PostgreSQL databases. Because KNIME workflows can handle\u00a0multiple databases\u00a0as one, this enables data from diverse sources to be segmented across the databases, resulting in easy scaling on inexpensive hardware.\u00a0KNIME Server\u00a0and a set of special KNIME workflows are used as an automated ETL tool for extracting the data from the different systems daily, and then transforming, aggregating, and storing them in the data warehouse. The SQL, SOAP, and\u00a0REST interfaces\u00a0of KNIME are used to connect to the various systems, while the scheduled execution is used for evenly spreading and timing the data transfers to off-peak hours.\nA\u00a0KNIME Server\u00a0installation consists of one KNIME Server and four distributed\u00a0KNIME Executors, which are connected through a RabbitMQ. The entire system is duplicated in a geo-redundant, high availability setup, and the deployment and configuration are automated with Ansible. This ensures that the development, test, and production systems are identical, and allows new environments to be installed within minutes if needed.\nThe\u00a0Management Information System\u00a0is integrated into an existing government Network Access Manager (NAM) system, providing OAuth authentication, a directory, an SSO system, and a reverse proxy, which protects all system resources behind it. It\u2019s also integrated into a central logging system, which collects all\u00a0KNIME Server, database, and OS system logs. A customized\u00a0KNIME WebPortal\u00a0meets government guidelines and is completely in Hungarian. A custom system-monitoring software checks the system health at the service level, which is where the\u00a0KNIME REST API\u00a0is extremely useful.\nThe bulk of the project was the development of 45 KNIME workflows, which produced 80 interactive\u00a0WebPortal\u00a0reports and their PDF versions using the BIRT integration. These reports are two versions of the same workflow, which was a requirement of the project so that different users could interact with the data. Recreating the reports in BIRT required a significant amount of work but improved the overall project deliverables and user satisfaction. The email sending capability of\u00a0KNIME Server\u00a0automatically distributes the PDF reports.\nAn interesting and unique feature of the\u00a0Management Information System\u00a0is the in-workflow authorization mechanism. This restricts the scope of the information available for the user during the workflow execution and user interaction, based on privileges stored in the NAM directory server, which is queried using\u00a0KNIME REST nodes.\nResults\nWith this solution, all data reporting processes have been converted from manual to automatic. Government executives can access reports on the web, in real-time, providing them with vital, timely information for better decision making. The interactive, web-based reports enable executives at various levels to drill down into the data as needed. For example, from the country level down to counties, cities, and lower levels of the organization. This was previously not possible without support from analysts and has therefore empowered executives to be more independent with the data.\nWhy KNIME?\nKNIME Software\u00a0was chosen because it offers out-of-the-box solutions to almost all requirements - and was easily extensible where it didn\u2019t. Specifically, KNIME:\nEnables non-coder data analysts to create new reports on demandIs an excellent and flexible report distribution platform that also offers user authentication and authorizationActs as an automated ETL toolEnables solutions to be scaled and reproduced when needed, and all steps are self-documentingIntegrates with a multitude of external tools, which is invaluable in a project where many different data sources must be connectedIs extensible and integrates seamlessly with other tools and technologies, which ensures that unforeseen problems can be easily resolvedBecause of the free, open source, and lightweight nature of\u00a0KNIME Analytics Platform, it was easy to evaluate and verify it against the project requirements. The software learning curve was low, and it was possible to develop a proof of concept quickly and efficiently. The KNIME team provided expert support with the few issues related to the new and more complex features such as OAuth authentication and SSO integration, which added another level of confidence to the final solution. The transparent\u00a0pricing of KNIME Server\u00a0is fair \u2013 particularly the unlimited number of consumers with access to KNIME WebPortal with Server Large. This was essential for giving executives access to the reports. Lastly, the partner program meant that\u00a0iCode, a KNIME Partner, could provide the required expertise and support due to their in-depth knowledge of data solutions in the government context.\n\u21d2\u00a0Download this Success Story as a PDF here.\n"
    },
    {
        "id": 76,
        "url": "https://www.knime.com/success-story/how-hungarian-government-automated-reporting-processes-key-decision-makers",
        "title": "How the Hungarian government automated reporting processes for key decision makers | KNIME",
        "company": "The Hungarian Government",
        "content": "Developing an efficient and customer-friendly public administration\nOperating in 174 cities and in the 23 districts of the capital city, these offices have more than 1,000 competencies and responsibilities. One of them is to offer and promote a service-provider state to citizens and businesses. This initiative aims to develop an efficient and customer-friendly public administration, resulting in an improved quality of public services. Factors contributing to this include reorganizing central public administration, reforming district offices, modernizing and reducing bureaucracy, and creating so-called \u201cGovernment Windows.\u201d These windows provide information and services to citizens in more than 1,500 different types of administrative cases. To support the reform initiatives, the government mandated the development of the\u00a0Government Offices Integrated Management System, part of which is the\u00a0Management Information System.\nThe capital and county government offices, along with the district offices and Government Windows, form the largest organization in Hungary. Their diverse functions are supported by many enterprise-sized IT systems including operations management, human resources management, financial planning and accounting, customer service and customer calling, as well as document and records management. There are also many specialized administrative systems, for example social insurance.\nThere were a few shortcomings of the existing solution. There was no unified reporting dashboard that was accessible to all stakeholders. There was also no automated information flow, meaning reports took weeks to be created and delivered. Lastly, the data sources were not interconnected. The Prime Minister\u2019s office often requests information from administrative staff in the form of consolidated and aggregated reports. Previously, this was a manual, labor-intensive, and slow process.\n\nA no-code solution to produce automated, web-based reports\nA solution that could automate the collection, aggregation, and consolidation of the data from the separate systems, and produce both web-based and printed reports with current information on demand was needed. Furthermore, the government\u2019s own data analyst team needed to be able to develop new reports when needed, without coding.\nA simple, low-cost data warehouse was built on several PostgreSQL databases. Because KNIME workflows can handle\u00a0multiple databases\u00a0as one, this enables data from diverse sources to be segmented across the databases, resulting in easy scaling on inexpensive hardware.\u00a0KNIME Server\u00a0and a set of special KNIME workflows are used as an automated ETL tool for extracting the data from the different systems daily, and then transforming, aggregating, and storing them in the data warehouse. The SQL, SOAP, and\u00a0REST interfaces\u00a0of KNIME are used to connect to the various systems, while the scheduled execution is used for evenly spreading and timing the data transfers to off-peak hours.\nA\u00a0KNIME Server\u00a0installation consists of one KNIME Server and four distributed\u00a0KNIME Executors, which are connected through a RabbitMQ. The entire system is duplicated in a geo-redundant, high availability setup, and the deployment and configuration are automated with Ansible. This ensures that the development, test, and production systems are identical, and allows new environments to be installed within minutes if needed.\nThe\u00a0Management Information System\u00a0is integrated into an existing government Network Access Manager (NAM) system, providing OAuth authentication, a directory, an SSO system, and a reverse proxy, which protects all system resources behind it. It\u2019s also integrated into a central logging system, which collects all\u00a0KNIME Server, database, and OS system logs. A customized\u00a0KNIME WebPortal\u00a0meets government guidelines and is completely in Hungarian. A custom system-monitoring software checks the system health at the service level, which is where the\u00a0KNIME REST API\u00a0is extremely useful.\nThe bulk of the project was the development of 45 KNIME workflows, which produced 80 interactive\u00a0WebPortal\u00a0reports and their PDF versions using the BIRT integration. These reports are two versions of the same workflow, which was a requirement of the project so that different users could interact with the data. Recreating the reports in BIRT required a significant amount of work but improved the overall project deliverables and user satisfaction. The email sending capability of\u00a0KNIME Server\u00a0automatically distributes the PDF reports.\nAn interesting and unique feature of the\u00a0Management Information System\u00a0is the in-workflow authorization mechanism. This restricts the scope of the information available for the user during the workflow execution and user interaction, based on privileges stored in the NAM directory server, which is queried using\u00a0KNIME REST nodes.\nResults\nWith this solution, all data reporting processes have been converted from manual to automatic. Government executives can access reports on the web, in real-time, providing them with vital, timely information for better decision making. The interactive, web-based reports enable executives at various levels to drill down into the data as needed. For example, from the country level down to counties, cities, and lower levels of the organization. This was previously not possible without support from analysts and has therefore empowered executives to be more independent with the data.\nWhy KNIME?\nKNIME Software\u00a0was chosen because it offers out-of-the-box solutions to almost all requirements - and was easily extensible where it didn\u2019t. Specifically, KNIME:\nEnables non-coder data analysts to create new reports on demandIs an excellent and flexible report distribution platform that also offers user authentication and authorizationActs as an automated ETL toolEnables solutions to be scaled and reproduced when needed, and all steps are self-documentingIntegrates with a multitude of external tools, which is invaluable in a project where many different data sources must be connectedIs extensible and integrates seamlessly with other tools and technologies, which ensures that unforeseen problems can be easily resolvedBecause of the free, open source, and lightweight nature of\u00a0KNIME Analytics Platform, it was easy to evaluate and verify it against the project requirements. The software learning curve was low, and it was possible to develop a proof of concept quickly and efficiently. The KNIME team provided expert support with the few issues related to the new and more complex features such as OAuth authentication and SSO integration, which added another level of confidence to the final solution. The transparent\u00a0pricing of KNIME Server\u00a0is fair \u2013 particularly the unlimited number of consumers with access to KNIME WebPortal with Server Large. This was essential for giving executives access to the reports. Lastly, the partner program meant that\u00a0iCode, a KNIME Partner, could provide the required expertise and support due to their in-depth knowledge of data solutions in the government context.\n\u21d2\u00a0Download this Success Story as a PDF here.\n"
    },
    {
        "id": 77,
        "url": "https://www.knime.com/success-story/how-todo1-services-mitigated-transactional-fraud-using-knime",
        "title": "How TODO1 Services mitigated transactional fraud using KNIME | KNIME",
        "company": "TODO1",
        "content": ""
    },
    {
        "id": 78,
        "url": "https://www.knime.com/success-story/how-todo1-services-mitigated-transactional-fraud-using-knime",
        "title": "How TODO1 Services mitigated transactional fraud using KNIME | KNIME",
        "company": "TODO1",
        "content": ""
    },
    {
        "id": 79,
        "url": "https://www.knime.com/success-story/how-truata-anonymized-and-automated-credit-card-data-perform-self-service-analytics",
        "title": "How Truata anonymized and automated credit card data perform self-service analytics",
        "company": "Truata",
        "content": "\nThe goal in this project was to anonymize and automate credit card data and enable financial services companies to perform their own \u201cself-service\u201d analytics while ensuring privacy is maintained.\nRequirements: A tool that is open source, extendible, and that offers\u00a0Guided Analytics and blueprints\u00a0as they didn\u2019t want to write their own workflows from scratch. KNIME was a good fitChallenges: Authorization in an integrated solutions architecture has serious authentication challenges!Set-up: Multiple\u00a0KNIME Executors\u00a0and\u00a0KNIME WebPortalResults: Several hundred financial institutions can now access the solution through the\u00a0WebPortal\n"
    },
    {
        "id": 80,
        "url": "https://www.knime.com/success-story/how-truata-anonymized-and-automated-credit-card-data-perform-self-service-analytics",
        "title": "How Truata anonymized and automated credit card data perform self-service analytics",
        "company": "Truata",
        "content": "\nThe goal in this project was to anonymize and automate credit card data and enable financial services companies to perform their own \u201cself-service\u201d analytics while ensuring privacy is maintained.\nRequirements: A tool that is open source, extendible, and that offers\u00a0Guided Analytics and blueprints\u00a0as they didn\u2019t want to write their own workflows from scratch. KNIME was a good fitChallenges: Authorization in an integrated solutions architecture has serious authentication challenges!Set-up: Multiple\u00a0KNIME Executors\u00a0and\u00a0KNIME WebPortalResults: Several hundred financial institutions can now access the solution through the\u00a0WebPortal\n"
    },
    {
        "id": 81,
        "url": "https://www.knime.com/success-story/how-knime-helps-identify-new-drug-candidates-covid-19",
        "title": "How KNIME Was Used to Accelerate Remdesivir Approval for COVID",
        "company": "Uppsala University",
        "content": "A year of pandemic: Identifying novel candidate molecules with COVID-19 as use case\nThe current timeline for a new drug to get regulatory approval ranges between 12 to 15 years. To accelerate the process of novel drug discovery, Uppsala University, Sweden\u2019s leading public research university, added drug repurposing strategies to its pipeline to find drugs effective for COVID-19 treatment. One of the drugs identified using this approach was remdesivir which continues to be actively used to treat COVID-19 patients with the highest risk of becoming seriously ill.\nDrug repurposing (also known as drug repositioning) is a re-evaluation of an already existing drug to test its potential in the treatment of a novel disease. This approach significantly reduces research and development costs and the drug development timeline as all the existing compounds in the drug have already been tested for safety in humans, eliminating the need for Phase 1 clinical trials.\nWhile drug repurposing is an effective method to speed up drug development, it comes with a unique set of challenges due to its data-intensive nature. To start with, researchers need to combine a wide variety of data entities such as tissue expression data, genes, drug-target interactions, disease datasets, and phenotype data to understand the effect a drug produces in the body. Combining data from multiple databases is key to increasing the diversity of the physico-chemical properties of the final compound sets. Additionally, working with small molecule data also poses several challenges such as an abstract chemical representation and the heterogeneity of data formats in the public domain.\nAll of this complex and diverse chemical data then requires large-scale data analysis to reveal hidden patterns for early stage drug development. This involves advanced data analytics techniques such as text mining, network analyses, machine (deep) learning models to predict drug-target-disease relationship, and structure- (protein-) based modeling methods.\nWith its ability to easily integrate data from multiple sources and its advanced analytics capabilities along with the advantages of automation flexibility, re-usability, and transparency, KNIME was the perfect fit for Uppsala University\u2019s requirements to evaluate COVID-19 drug candidates.\nUppsala University's strategy was based on the molecular similarity principle: structurally similar molecules tend to possess similar biological activities. To this end, they built a workflow in KNIME to perform ligand-based in silico drug repurposing.\n\nProgrammatic data access to life sciences databases\nWith KNIME, researchers at the university could programmatically connect with various publicly available chemical databases such as\u00a0UniProt,\u00a0Protein Data Bank (PDB),\u00a0ChEMBL,\u00a0Guide-To-Pharmacology (IUPHAR),\u00a0PubChem, and\u00a0DrugBank\u00a0to access structural- and bioactivity-ligand data associated with protein targets that are involved in COVID-19.\nInstead of manual querying the databases, they simply had to execute a workflow in KNIME that specified an API request, retrieved data from web services, and extracted relevant information from the received files. In the first instance, protein target identifiers of the Open Targets platform were mapped to their corresponding UniProt IDs. These retrieved UniProt IDs served as a starting point to retrieve protein\u2013ligand structural data from PDB, as well as ligand bioactivity data from ChEMBL, IUPHAR, and PubChem.\n\nStandardizing molecular Sstructures\nA prerequisite for merging ligand data from diverse sources is to standardize molecular structures. To ensure unified chemical data representation, researchers only had to execute another workflow in KNIME. The workflow took care of the complex steps of removing compound stereochemistry, stripping salts by forwarding a predefined set of different salts/salts mixtures, and listing all stripped salt components in a clear output table. Next, the workflow neutralized charges and checked for possible atomic clashes, filtered data by checking specific elements, and generated the necessary InChI, InChiKey, and Canonical smiles formats.\nSubstructure searches to identify candidates for drug repurposing\nOnce the molecular structures were standardized, the research team used machine learning models in KNIME to identify enriched molecular (sub)structures in order to perform substructure searches of the datasets of available drugs for finding new - potentially active - drug candidates.\nTo achieve this, first, the molecular structures were reduced to their Bemis-Murcko scaffolds.Then, using the generated scaffolds and associated UniProt IDs as an input, KNIME helped them calculate the molecular distances for the retained scaffolds. The clustering model in KNIME created hierarchical clusters to group the scaffolds. The output of the model included UniProt IDs, associated scaffolds, and cluster IDs. For each target, researchers used KNIME to loop over distinct clusters of associated scaffolds to create a maximum common substructure.\nThe generated substructures (in SMARTS) were then used as queries to find hits in DrugBank. The structures from DrugBank were standardized and filtered to perform substructure searches. The identified hits were provided in a table with molecule names, associated targets, SMARTS keys, and chemical structures along with highlighted substructures in SVG format.\nResearchers at Uppsala University were able to automate the entire process using KNIME. Since a KNIME workflow is easily reproducible, they could adapt it to individual project needs.\nRemdesivir and other drug candidates for COVID-19\nThe application programming interface and the integration of cheminformatics libraries and external software in KNIME enabled Uppsala University to tackle the problems in a clear, efficient, and reproducible way.\nSubstructure searches with KNIME helped identify 7836 compounds from DrugBank and 36,521 compounds from the CAS data set. Out of those hits, 135 compounds were retrieved from both DrugBank and the CAS data set. Some of the identified drugs include remdesivir, fludarabine, riboprine, rupintrivir, indinavir, darunavir, and telaprevir. Several of the identified hits underwent clinical trials with remdesivir receiving emergency approval for COVID-19 treatment from regulatory authorities across different countries.\n"
    },
    {
        "id": 82,
        "url": "https://www.knime.com/success-story/how-knime-helps-identify-new-drug-candidates-covid-19",
        "title": "How KNIME Was Used to Accelerate Remdesivir Approval for COVID",
        "company": "Uppsala University",
        "content": "A year of pandemic: Identifying novel candidate molecules with COVID-19 as use case\nThe current timeline for a new drug to get regulatory approval ranges between 12 to 15 years. To accelerate the process of novel drug discovery, Uppsala University, Sweden\u2019s leading public research university, added drug repurposing strategies to its pipeline to find drugs effective for COVID-19 treatment. One of the drugs identified using this approach was remdesivir which continues to be actively used to treat COVID-19 patients with the highest risk of becoming seriously ill.\nDrug repurposing (also known as drug repositioning) is a re-evaluation of an already existing drug to test its potential in the treatment of a novel disease. This approach significantly reduces research and development costs and the drug development timeline as all the existing compounds in the drug have already been tested for safety in humans, eliminating the need for Phase 1 clinical trials.\nWhile drug repurposing is an effective method to speed up drug development, it comes with a unique set of challenges due to its data-intensive nature. To start with, researchers need to combine a wide variety of data entities such as tissue expression data, genes, drug-target interactions, disease datasets, and phenotype data to understand the effect a drug produces in the body. Combining data from multiple databases is key to increasing the diversity of the physico-chemical properties of the final compound sets. Additionally, working with small molecule data also poses several challenges such as an abstract chemical representation and the heterogeneity of data formats in the public domain.\nAll of this complex and diverse chemical data then requires large-scale data analysis to reveal hidden patterns for early stage drug development. This involves advanced data analytics techniques such as text mining, network analyses, machine (deep) learning models to predict drug-target-disease relationship, and structure- (protein-) based modeling methods.\nWith its ability to easily integrate data from multiple sources and its advanced analytics capabilities along with the advantages of automation flexibility, re-usability, and transparency, KNIME was the perfect fit for Uppsala University\u2019s requirements to evaluate COVID-19 drug candidates.\nUppsala University's strategy was based on the molecular similarity principle: structurally similar molecules tend to possess similar biological activities. To this end, they built a workflow in KNIME to perform ligand-based in silico drug repurposing.\n\nProgrammatic data access to life sciences databases\nWith KNIME, researchers at the university could programmatically connect with various publicly available chemical databases such as\u00a0UniProt,\u00a0Protein Data Bank (PDB),\u00a0ChEMBL,\u00a0Guide-To-Pharmacology (IUPHAR),\u00a0PubChem, and\u00a0DrugBank\u00a0to access structural- and bioactivity-ligand data associated with protein targets that are involved in COVID-19.\nInstead of manual querying the databases, they simply had to execute a workflow in KNIME that specified an API request, retrieved data from web services, and extracted relevant information from the received files. In the first instance, protein target identifiers of the Open Targets platform were mapped to their corresponding UniProt IDs. These retrieved UniProt IDs served as a starting point to retrieve protein\u2013ligand structural data from PDB, as well as ligand bioactivity data from ChEMBL, IUPHAR, and PubChem.\n\nStandardizing molecular Sstructures\nA prerequisite for merging ligand data from diverse sources is to standardize molecular structures. To ensure unified chemical data representation, researchers only had to execute another workflow in KNIME. The workflow took care of the complex steps of removing compound stereochemistry, stripping salts by forwarding a predefined set of different salts/salts mixtures, and listing all stripped salt components in a clear output table. Next, the workflow neutralized charges and checked for possible atomic clashes, filtered data by checking specific elements, and generated the necessary InChI, InChiKey, and Canonical smiles formats.\nSubstructure searches to identify candidates for drug repurposing\nOnce the molecular structures were standardized, the research team used machine learning models in KNIME to identify enriched molecular (sub)structures in order to perform substructure searches of the datasets of available drugs for finding new - potentially active - drug candidates.\nTo achieve this, first, the molecular structures were reduced to their Bemis-Murcko scaffolds.Then, using the generated scaffolds and associated UniProt IDs as an input, KNIME helped them calculate the molecular distances for the retained scaffolds. The clustering model in KNIME created hierarchical clusters to group the scaffolds. The output of the model included UniProt IDs, associated scaffolds, and cluster IDs. For each target, researchers used KNIME to loop over distinct clusters of associated scaffolds to create a maximum common substructure.\nThe generated substructures (in SMARTS) were then used as queries to find hits in DrugBank. The structures from DrugBank were standardized and filtered to perform substructure searches. The identified hits were provided in a table with molecule names, associated targets, SMARTS keys, and chemical structures along with highlighted substructures in SVG format.\nResearchers at Uppsala University were able to automate the entire process using KNIME. Since a KNIME workflow is easily reproducible, they could adapt it to individual project needs.\nRemdesivir and other drug candidates for COVID-19\nThe application programming interface and the integration of cheminformatics libraries and external software in KNIME enabled Uppsala University to tackle the problems in a clear, efficient, and reproducible way.\nSubstructure searches with KNIME helped identify 7836 compounds from DrugBank and 36,521 compounds from the CAS data set. Out of those hits, 135 compounds were retrieved from both DrugBank and the CAS data set. Some of the identified drugs include remdesivir, fludarabine, riboprine, rupintrivir, indinavir, darunavir, and telaprevir. Several of the identified hits underwent clinical trials with remdesivir receiving emergency approval for COVID-19 treatment from regulatory authorities across different countries.\n"
    },
    {
        "id": 83,
        "url": "https://www.knime.com/success-story/how-wave-lifesciences-made-data-science-accessible-domain-experts-create-real",
        "title": "How Wave Lifesciences made data science accessible to domain experts",
        "company": "Wave Life Sciences",
        "content": "A modern platform that covers the entire data science process\nAn important question in this setting is how to quickly and reliably develop, test, and deploy software that uses modern data science tools and answers key business questions. The answer lies in finding a platform that provides tools for the entire data science process: from data access and transformation, to visualization and predictive analytics, through to reporting and feedback. For Wave, the solution was KNIME Analytics Platform, later followed by KNIME Server and KNIME WebPortal. KNIME provides access not only to robust data science tools, but also a means for productionizing data science solutions for broad use and delivering these to domain experts via the browser. Wave was able to apply this in different life science domains including cheminformatics and bioinformatics - as seen in the following examples.\n\nMix and match technologies for creating a KNIME WebPortal GUI that analyzes qPCR data\nA qPCR workflow for mRNA measurements is a Guided Analytics application for scientists that analyzes 384-well bioassay data. Guided Analytics enables scientists to interact with and explore the data through interactive web pages where a KNIME workflow is running under the hood. \u201cThis is one of the nicest features of KNIME WebPortal. Users get to interact as they move with the data and move through the workflow\u201d says Kenneth Longo, Vice President - Discovery Data Science, Wave Life Sciences. \"Scientists get a sense of what is going on in their experiment. It\u2019s very easy to interact with users at this point to check things that might have gone wrong - which is often a make or break moment\u201d he continues. This close collaboration with scientists can save a lot of time and resources as problems can be detected early on and appropriate decisions can be made.\nThe workflow is triggered through interactions on the web pages and contains \u201cready-made\u201d features of KNIME including components with interactive JavaScript graphs, context properties extraction, and controlled metadata through flow variables. In addition, due to the open nature of KNIME, Wave combines this with advanced statistical features and custom visualizations in R, as well as connections to various databases, data sources, and data APIs - some in AWS and some local. And all of this in one platform. The workflow has allowed Wave to process over 200,000 observations in a period of 24 months.\nUsing KNIME as a data science and chemistry collaboration framework\nWave develops oligonucleotide therapeutics. The precise design and optimization of those oligonucleotides enables the development of molecules with greater efficacy and safety. With KNIME and its dedicated\u00a0chemistry extensions, Wave can explore chemical features of these large molecules, do substructure searches, and select and categorize molecules of interest. KNIME has become an integral tool for interactions and collaborations between functional groups at Wave. Through KNIME\u2019s strength in cheminformatics, combined with the ability to easily create web applications, it serves as a powerful collaborative framework for data science and chemistry groups.\n\n\nCreating a bioinformatics gene-species alignment network\nEvaluating the homology between DNA or RNA sequences is a common task in bioinformatics. Detecting the presence of homologous relationships between relatively short oligonucleotide sequences and the genomes or transcriptomes of different animal species can be a useful predictor of intended (or unintended) gene targets. Command-line tools such as bowtie are commonly used to perform computationally efficient sequence alignments against large genomes. With KNIME it\u2019s possible to integrate these command-line tools and enhance the analysis with KNIME components in one reproducible workflow. This built-in functionality includes dedicated nodes for interactive network analysis and visualization, made available on the KNIME WebPortal. For example, users can interactively select clinically-relevant species of interest and investigate the resulting gene-species network of sequence alignments.\nGiving domain experts access to data and saving significant amounts of time\nSince using KNIME Software, Wave has been able to give domain experts the ability to access, view, process, and interact with data. It has empowered them in their research to get a better feeling and understanding of their data \u2013 thereby allowing them to find errors early on. Significant amounts of time have been saved because data scientists are no longer spending time hand-processing the data, and ideas can be evaluated quickly. Lastly, human error has been considerably reduced through the workflow-based automation of these processes.\nWhy KNIME?\nKNIME Software\u00a0is the perfect end-to-end data software development tool \u2013 enabling the connection of multiple data sources and domains, the ability to build workflows with a simple, visual drag and drop method, and do automated reporting. One of the strongest benefits of KNIME is the flexibility it offers to mix and match different technologies and features. Being able to augment KNIME workflows by integrating\u00a0R\u00a0or\u00a0Python\u00a0code is a huge plus. Deployment of\u00a0KNIME in the AWS cloud\u00a0enables a powerful and flexible resource architecture.\nGetting started with KNIME is easy for new users and has a short learning curve. This is due to the intuitive nature of the KNIME visual interface and the fact that zero coding is required (but possible if needed). KNIME hastens prototype development; the journey from idea to proof-of-concept is short, which is essential in the dynamic and fast-moving biopharma landscape. Lastly, because KNIME works well within a modern software environment, there is less work and risk for IT to integrate it into pre-existing software stacks.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 84,
        "url": "https://www.knime.com/success-story/how-wave-lifesciences-made-data-science-accessible-domain-experts-create-real",
        "title": "How Wave Lifesciences made data science accessible to domain experts",
        "company": "Wave Life Sciences",
        "content": "A modern platform that covers the entire data science process\nAn important question in this setting is how to quickly and reliably develop, test, and deploy software that uses modern data science tools and answers key business questions. The answer lies in finding a platform that provides tools for the entire data science process: from data access and transformation, to visualization and predictive analytics, through to reporting and feedback. For Wave, the solution was KNIME Analytics Platform, later followed by KNIME Server and KNIME WebPortal. KNIME provides access not only to robust data science tools, but also a means for productionizing data science solutions for broad use and delivering these to domain experts via the browser. Wave was able to apply this in different life science domains including cheminformatics and bioinformatics - as seen in the following examples.\n\nMix and match technologies for creating a KNIME WebPortal GUI that analyzes qPCR data\nA qPCR workflow for mRNA measurements is a Guided Analytics application for scientists that analyzes 384-well bioassay data. Guided Analytics enables scientists to interact with and explore the data through interactive web pages where a KNIME workflow is running under the hood. \u201cThis is one of the nicest features of KNIME WebPortal. Users get to interact as they move with the data and move through the workflow\u201d says Kenneth Longo, Vice President - Discovery Data Science, Wave Life Sciences. \"Scientists get a sense of what is going on in their experiment. It\u2019s very easy to interact with users at this point to check things that might have gone wrong - which is often a make or break moment\u201d he continues. This close collaboration with scientists can save a lot of time and resources as problems can be detected early on and appropriate decisions can be made.\nThe workflow is triggered through interactions on the web pages and contains \u201cready-made\u201d features of KNIME including components with interactive JavaScript graphs, context properties extraction, and controlled metadata through flow variables. In addition, due to the open nature of KNIME, Wave combines this with advanced statistical features and custom visualizations in R, as well as connections to various databases, data sources, and data APIs - some in AWS and some local. And all of this in one platform. The workflow has allowed Wave to process over 200,000 observations in a period of 24 months.\nUsing KNIME as a data science and chemistry collaboration framework\nWave develops oligonucleotide therapeutics. The precise design and optimization of those oligonucleotides enables the development of molecules with greater efficacy and safety. With KNIME and its dedicated\u00a0chemistry extensions, Wave can explore chemical features of these large molecules, do substructure searches, and select and categorize molecules of interest. KNIME has become an integral tool for interactions and collaborations between functional groups at Wave. Through KNIME\u2019s strength in cheminformatics, combined with the ability to easily create web applications, it serves as a powerful collaborative framework for data science and chemistry groups.\n\n\nCreating a bioinformatics gene-species alignment network\nEvaluating the homology between DNA or RNA sequences is a common task in bioinformatics. Detecting the presence of homologous relationships between relatively short oligonucleotide sequences and the genomes or transcriptomes of different animal species can be a useful predictor of intended (or unintended) gene targets. Command-line tools such as bowtie are commonly used to perform computationally efficient sequence alignments against large genomes. With KNIME it\u2019s possible to integrate these command-line tools and enhance the analysis with KNIME components in one reproducible workflow. This built-in functionality includes dedicated nodes for interactive network analysis and visualization, made available on the KNIME WebPortal. For example, users can interactively select clinically-relevant species of interest and investigate the resulting gene-species network of sequence alignments.\nGiving domain experts access to data and saving significant amounts of time\nSince using KNIME Software, Wave has been able to give domain experts the ability to access, view, process, and interact with data. It has empowered them in their research to get a better feeling and understanding of their data \u2013 thereby allowing them to find errors early on. Significant amounts of time have been saved because data scientists are no longer spending time hand-processing the data, and ideas can be evaluated quickly. Lastly, human error has been considerably reduced through the workflow-based automation of these processes.\nWhy KNIME?\nKNIME Software\u00a0is the perfect end-to-end data software development tool \u2013 enabling the connection of multiple data sources and domains, the ability to build workflows with a simple, visual drag and drop method, and do automated reporting. One of the strongest benefits of KNIME is the flexibility it offers to mix and match different technologies and features. Being able to augment KNIME workflows by integrating\u00a0R\u00a0or\u00a0Python\u00a0code is a huge plus. Deployment of\u00a0KNIME in the AWS cloud\u00a0enables a powerful and flexible resource architecture.\nGetting started with KNIME is easy for new users and has a short learning curve. This is due to the intuitive nature of the KNIME visual interface and the fact that zero coding is required (but possible if needed). KNIME hastens prototype development; the journey from idea to proof-of-concept is short, which is essential in the dynamic and fast-moving biopharma landscape. Lastly, because KNIME works well within a modern software environment, there is less work and risk for IT to integrate it into pre-existing software stacks.\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 85,
        "url": "https://www.knime.com/success-story/how-webbankir-automated-loan-application-decision-making-using-knime",
        "title": "How Webbankir automated loan application decision making using KNIME | KNIME",
        "company": "Webbankir",
        "content": "Getting loans approved & sent faster with highly automated, accurate decision making process\nWebbankir clients submit an online loan application and, if approved, receive their money directly on their card without visiting an office or banking institution. This means the decision making process must be fast, highly automated, and accurate. Furthermore, the process considers data from both external data providers, as well as the client's history.\nAn application to build and validate machine learning models was required to create an automated online loan application tool. The tool needed to be easy to work with, use visual programming methods i.e. no coding, include a wide range of built-in models and algorithms, and easily connect to various data sources (specifically database management systems such as Amazon Redshift and PostgreSQL), as well as text and Excel files. It also needed to integrate with Python and R.\nOne visual environment for model development & validation\nKNIME was chosen because it not only ticked all the required boxes, but getting up and running with the software was simple. Webbankir was also able to successfully develop and validate models because KNIME had all the required model types in the core nodes or in the extensions. This helped solve one of the problems in the previous system, which was transferring ready-made models from development into production. In that system, the models were encoded in a language that didn\u2019t support machine learning tools - meaning the implementation took a long time and often contained errors that were only detectable after lengthy testing. This resulted in the implementation of the model in the actual decision making process taking between one and two months.\nThe goal of this project was two-fold. Firstly, build a complete solution management system that included the entire path from model development to implementation - and enable any model class to be used. Secondly, reduce the time needed for model implementation to a maximum of seven days to guarantee an accurate transfer of the model from the development environment to the production environment and allow for automated testing.\nThe project included integrating 1C (the existing decision-making system), with KNIME Server and creating a new decision-making service based on a combination of KNIME Analytics Platform and KNIME Server. This new service would replace the decision flow for clients completing their first loan application with Webbankir. It would also be part of the solution for repeat clients. Additionally, the project included building a test system for automatically testing the functionality of such a service.\nProcessing 200,000 requests per month in under ten seconds per request\nThe decision making service needed to have an API running on a cloud platform with a processing capacity of at least 200,000 requests per month, a peak load of up to 5,000 requests per hour, and an average execution time of no more than 10 seconds. The importance of this project was considerable, because it\u2019s extremely important to be able to quickly and accurately respond to changes in the environment. For example, at one point a new scoring model was added to fine tune the decision making process and decrease the default rate. Previously, this would have taken up to two months. However, with the new process it took only seven days - including model development, transferring into production, and testing. In another example, due to Covid-19, the set of decision making rules was changed five times in one month and each time this happened, took less than two hours to complete.\nThe working solution\nThe project started with defining the type of future service (API) and the sequence of how the functionality should be implemented. The integration points with the existing system were also defined, as well as the input and output vector datasets. In terms of construction, the following key steps were completed:\nIntegrate with the existing/legacy decision making systemCreate an MVP \u2013 in this case an API service with default values of output vector fieldsImplement models and calculate indicators used in decision making, and run the calculation in test mode when the output vector was fixed but did not participate in the decisionImplement the full decision logic in test modeStart using the service's responses in the actual decision making processCreate a system for automated testing of the serviceThe decision making service is run\u00a0using KNIME on Amazon Web Services (AWS), which was identified as the easiest way to try out the technology. AWS provided an\u00a0Amazon Machine Image for KNIME Analytics Platform and Server\u00a0instance with scalable performance, which was an important consideration due to a) the required processing capacity and b) the need to respond quickly to changes in the external environment. Furthermore, given the sensitivities of the data, AWS enables KNIME to be containerized and keeps data secure. This pre-packaged and pre-defined instance is a cost-effective solution.\nIn terms of specific KNIME functionality, the integration with the main system consisted of using POST requests to the API service. The\u00a0KNIME Server Connector, KNIME Server Executor,\u00a0KNIME Python Scripting Extensions\u00a0were used, as well as the Palladian,\u00a0Logistic Regression,\u00a0Gradient Boosting,\u00a0PMML,\u00a0Workflow Control\u00a0nodes, and more.\nIncreasing the share of fully automated decisions from 75% to 85%\nPreviously, the time taken to implement new models was one to two months. Using KNIME Analytics Platform, this now takes one to seven days. The new service has also completely freed up one full time software developer who can now work on other important tasks and initiatives. It has reduced decision making time by 30%, and increased the share of fully automated decisions from 70% to 85%. Currently, the developed service takes part in decision-making on more than 95% of the application flow.\nWhy KNIME?\nThe initial plan was to build this service using Python, however, there were difficulties in transferring the developed models. Furthermore, data scientists were unable to work independently because software developers were always required, therefore consuming additional business resources. One of the strongest arguments for choosing KNIME was the openness and ability to read in and work with many data types and integrate with other machine learning tools, as well as automate the decisions being made.\nIn terms of software capabilities, KNIME allows almost any type of machine learning model to be used. It can also complete almost any kind of required data transformation needed for a given project. The visual drag and drop way of building a workflow, not only speeds up the work on models and processes, but it also enables non-coders to work freely and independently. The built-in Python code injection capabilities allow users to implement conversions (if necessary) that are implemented in complex or non-obvious ways using standard KNIME nodes. The scalability of KNIME Server enables the desired results in terms of load and processing speed to be achieved by drawing only on the resources that are required. KNIME Server also allows the team to focus on machine learning related aspects, because they do not need to be occupied by any performance or reliability concerns.\nIn terms of business considerations, KNIME enables employees from the risk team to implement decision making logic completely independently. They can develop and validate the models, as well as transfer these models to the service, and change the decision making rules. The models are transferred using PMML files, which ensures correct and fast migration. This approach is efficient and enables the team to quickly and easily put the developed functionality into production. When needed, the KNIME team provides support and guidance.\nThis Success Story is available here as a downloadable PDF.\n"
    },
    {
        "id": 86,
        "url": "https://www.knime.com/success-story/how-webbankir-automated-loan-application-decision-making-using-knime",
        "title": "How Webbankir automated loan application decision making using KNIME | KNIME",
        "company": "Webbankir",
        "content": "Getting loans approved & sent faster with highly automated, accurate decision making process\nWebbankir clients submit an online loan application and, if approved, receive their money directly on their card without visiting an office or banking institution. This means the decision making process must be fast, highly automated, and accurate. Furthermore, the process considers data from both external data providers, as well as the client's history.\nAn application to build and validate machine learning models was required to create an automated online loan application tool. The tool needed to be easy to work with, use visual programming methods i.e. no coding, include a wide range of built-in models and algorithms, and easily connect to various data sources (specifically database management systems such as Amazon Redshift and PostgreSQL), as well as text and Excel files. It also needed to integrate with Python and R.\nOne visual environment for model development & validation\nKNIME was chosen because it not only ticked all the required boxes, but getting up and running with the software was simple. Webbankir was also able to successfully develop and validate models because KNIME had all the required model types in the core nodes or in the extensions. This helped solve one of the problems in the previous system, which was transferring ready-made models from development into production. In that system, the models were encoded in a language that didn\u2019t support machine learning tools - meaning the implementation took a long time and often contained errors that were only detectable after lengthy testing. This resulted in the implementation of the model in the actual decision making process taking between one and two months.\nThe goal of this project was two-fold. Firstly, build a complete solution management system that included the entire path from model development to implementation - and enable any model class to be used. Secondly, reduce the time needed for model implementation to a maximum of seven days to guarantee an accurate transfer of the model from the development environment to the production environment and allow for automated testing.\nThe project included integrating 1C (the existing decision-making system), with KNIME Server and creating a new decision-making service based on a combination of KNIME Analytics Platform and KNIME Server. This new service would replace the decision flow for clients completing their first loan application with Webbankir. It would also be part of the solution for repeat clients. Additionally, the project included building a test system for automatically testing the functionality of such a service.\nProcessing 200,000 requests per month in under ten seconds per request\nThe decision making service needed to have an API running on a cloud platform with a processing capacity of at least 200,000 requests per month, a peak load of up to 5,000 requests per hour, and an average execution time of no more than 10 seconds. The importance of this project was considerable, because it\u2019s extremely important to be able to quickly and accurately respond to changes in the environment. For example, at one point a new scoring model was added to fine tune the decision making process and decrease the default rate. Previously, this would have taken up to two months. However, with the new process it took only seven days - including model development, transferring into production, and testing. In another example, due to Covid-19, the set of decision making rules was changed five times in one month and each time this happened, took less than two hours to complete.\nThe working solution\nThe project started with defining the type of future service (API) and the sequence of how the functionality should be implemented. The integration points with the existing system were also defined, as well as the input and output vector datasets. In terms of construction, the following key steps were completed:\nIntegrate with the existing/legacy decision making systemCreate an MVP \u2013 in this case an API service with default values of output vector fieldsImplement models and calculate indicators used in decision making, and run the calculation in test mode when the output vector was fixed but did not participate in the decisionImplement the full decision logic in test modeStart using the service's responses in the actual decision making processCreate a system for automated testing of the serviceThe decision making service is run\u00a0using KNIME on Amazon Web Services (AWS), which was identified as the easiest way to try out the technology. AWS provided an\u00a0Amazon Machine Image for KNIME Analytics Platform and Server\u00a0instance with scalable performance, which was an important consideration due to a) the required processing capacity and b) the need to respond quickly to changes in the external environment. Furthermore, given the sensitivities of the data, AWS enables KNIME to be containerized and keeps data secure. This pre-packaged and pre-defined instance is a cost-effective solution.\nIn terms of specific KNIME functionality, the integration with the main system consisted of using POST requests to the API service. The\u00a0KNIME Server Connector, KNIME Server Executor,\u00a0KNIME Python Scripting Extensions\u00a0were used, as well as the Palladian,\u00a0Logistic Regression,\u00a0Gradient Boosting,\u00a0PMML,\u00a0Workflow Control\u00a0nodes, and more.\nIncreasing the share of fully automated decisions from 75% to 85%\nPreviously, the time taken to implement new models was one to two months. Using KNIME Analytics Platform, this now takes one to seven days. The new service has also completely freed up one full time software developer who can now work on other important tasks and initiatives. It has reduced decision making time by 30%, and increased the share of fully automated decisions from 70% to 85%. Currently, the developed service takes part in decision-making on more than 95% of the application flow.\nWhy KNIME?\nThe initial plan was to build this service using Python, however, there were difficulties in transferring the developed models. Furthermore, data scientists were unable to work independently because software developers were always required, therefore consuming additional business resources. One of the strongest arguments for choosing KNIME was the openness and ability to read in and work with many data types and integrate with other machine learning tools, as well as automate the decisions being made.\nIn terms of software capabilities, KNIME allows almost any type of machine learning model to be used. It can also complete almost any kind of required data transformation needed for a given project. The visual drag and drop way of building a workflow, not only speeds up the work on models and processes, but it also enables non-coders to work freely and independently. The built-in Python code injection capabilities allow users to implement conversions (if necessary) that are implemented in complex or non-obvious ways using standard KNIME nodes. The scalability of KNIME Server enables the desired results in terms of load and processing speed to be achieved by drawing only on the resources that are required. KNIME Server also allows the team to focus on machine learning related aspects, because they do not need to be occupied by any performance or reliability concerns.\nIn terms of business considerations, KNIME enables employees from the risk team to implement decision making logic completely independently. They can develop and validate the models, as well as transfer these models to the service, and change the decision making rules. The models are transferred using PMML files, which ensures correct and fast migration. This approach is efficient and enables the team to quickly and easily put the developed functionality into production. When needed, the KNIME team provides support and guidance.\nThis Success Story is available here as a downloadable PDF.\n"
    },
    {
        "id": 87,
        "url": "https://www.knime.com/success-story/how-wurth-built-recommendation-engine-using-knime",
        "title": "How W\u00fcrth built a recommendation engine using KNIME",
        "company": "W\u00fcrth Group",
        "content": "The challenge: Create automatic, personalized email campaigns\nRecently, W\u00fcrth launched an e-commerce business alongside their traditional bricks and mortar sales channel. To increase customer engagement and number of online purchases, personalized offers were needed that could be put in front of potential customers. The challenge was to automatically create personalized content based on users\u2019 behaviors, preferences, and habits (particularly in the online shop), and to send them a regular email newsletter. The objective of these campaigns was to build a stronger relationship with customers, by promoting relevant information and offers.\n\nThe solution: A big data recommendation engine\nEffective campaigns depend on how well data are collected and measured. The first step was to analyze the information from the online shop, as well as other relevant sources such as the data warehouse and CRM, to create relevant content. Then, together with\u00a0KNIME Partner Miriade, a Big Data Recommendation Engine was developed. This was done using a mathematical collaborative filtering algorithm. Given a matrix in which the rows correspond to the users and the columns to the products, the algorithm completes all empty intersections by providing what the customer might like, which is based on purchases of similar customers.\nThe algorithm is based on the available data. The more accurate the data, the more precise the algorithm. To achieve this, the data that were previously stored in different databases, were collected in a single Apache Hadoop system, cleaned, and made ready for algorithmic processing. The cleaned data were then categorized based on predetermined rules, which assign specific values according to business needs. Once the data were prepared, the algorithm identified, and rules established, the analysis was done. The main steps included:\nInstall the cluster where the data is processed. This is usually the moment the KNIME Server is installed.Secure the infrastructure.Ingest data from selected sources. This is the most delicate phase as data is taken from the data warehouse to the HDFS Hadoop distribution using\u00a0Spark and KNIME nodes. Hadoop enables data to be separated from the table and considered as metadata. Here, a flat table is used, in which the relevant data are saved in Parquet for fast elaboration.Conduct predictive analytics. The starting point is the model creation, which is designed as a KNIME workflow, to get the complete rating for user-item association. Once the technical infrastructure is done and the data flow is stabilized, the analysis is performed with the selected algorithm and rating.All workflows - for both data ingestion and model generation - are scheduled using the\u00a0KNIME Server\u00a0Scheduler. All workflows include variables, which ensures all processes are automated and self-governed.\u00a0KNIME WebPortal\u00a0is used to create visual dashboards - including Business Intelligence KPIs and plots.\nResults\nThe entire project took 4-5 months to complete - from the very first meeting with the customer to discuss the objectives through to the full implementation. The result is automatically generated, personalized marketing campaigns that promote W\u00fcrth products. With KNIME, the process is automated and scheduled, ensuring an accurate and efficient campaign.\nWhy KNIME?\nKNIME\u00a0makes it easy to share work and workflows between users and stakeholders, which made it possible for Miriade to create the solution and later share it with W\u00fcrth. This was one of the main reasons KNIME was chosen. The workbench and\u00a0visual programming\u00a0make it very easy to get started with data science because the data flow is shown in a clear and intuitive way \u2013 even for first time users. KNIME offers\u00a0machine learning nodes\u00a0for all phases of the lifecycle: from data ingestion and manipulation, through to model training, visualization, and deployment. Working with KNIME and Hadoop was a seamless experience, and everything was controllable from within the KNIME workbench. The combination of KNIME and Hadoop make it very easy to manage the large amount of data as well as the expected amounts of data in the future.\nThis Success Story is available here as a PDF.\n"
    },
    {
        "id": 88,
        "url": "https://www.knime.com/success-story/how-wurth-built-recommendation-engine-using-knime",
        "title": "How W\u00fcrth built a recommendation engine using KNIME",
        "company": "W\u00fcrth Group",
        "content": "The challenge: Create automatic, personalized email campaigns\nRecently, W\u00fcrth launched an e-commerce business alongside their traditional bricks and mortar sales channel. To increase customer engagement and number of online purchases, personalized offers were needed that could be put in front of potential customers. The challenge was to automatically create personalized content based on users\u2019 behaviors, preferences, and habits (particularly in the online shop), and to send them a regular email newsletter. The objective of these campaigns was to build a stronger relationship with customers, by promoting relevant information and offers.\n\nThe solution: A big data recommendation engine\nEffective campaigns depend on how well data are collected and measured. The first step was to analyze the information from the online shop, as well as other relevant sources such as the data warehouse and CRM, to create relevant content. Then, together with\u00a0KNIME Partner Miriade, a Big Data Recommendation Engine was developed. This was done using a mathematical collaborative filtering algorithm. Given a matrix in which the rows correspond to the users and the columns to the products, the algorithm completes all empty intersections by providing what the customer might like, which is based on purchases of similar customers.\nThe algorithm is based on the available data. The more accurate the data, the more precise the algorithm. To achieve this, the data that were previously stored in different databases, were collected in a single Apache Hadoop system, cleaned, and made ready for algorithmic processing. The cleaned data were then categorized based on predetermined rules, which assign specific values according to business needs. Once the data were prepared, the algorithm identified, and rules established, the analysis was done. The main steps included:\nInstall the cluster where the data is processed. This is usually the moment the KNIME Server is installed.Secure the infrastructure.Ingest data from selected sources. This is the most delicate phase as data is taken from the data warehouse to the HDFS Hadoop distribution using\u00a0Spark and KNIME nodes. Hadoop enables data to be separated from the table and considered as metadata. Here, a flat table is used, in which the relevant data are saved in Parquet for fast elaboration.Conduct predictive analytics. The starting point is the model creation, which is designed as a KNIME workflow, to get the complete rating for user-item association. Once the technical infrastructure is done and the data flow is stabilized, the analysis is performed with the selected algorithm and rating.All workflows - for both data ingestion and model generation - are scheduled using the\u00a0KNIME Server\u00a0Scheduler. All workflows include variables, which ensures all processes are automated and self-governed.\u00a0KNIME WebPortal\u00a0is used to create visual dashboards - including Business Intelligence KPIs and plots.\nResults\nThe entire project took 4-5 months to complete - from the very first meeting with the customer to discuss the objectives through to the full implementation. The result is automatically generated, personalized marketing campaigns that promote W\u00fcrth products. With KNIME, the process is automated and scheduled, ensuring an accurate and efficient campaign.\nWhy KNIME?\nKNIME\u00a0makes it easy to share work and workflows between users and stakeholders, which made it possible for Miriade to create the solution and later share it with W\u00fcrth. This was one of the main reasons KNIME was chosen. The workbench and\u00a0visual programming\u00a0make it very easy to get started with data science because the data flow is shown in a clear and intuitive way \u2013 even for first time users. KNIME offers\u00a0machine learning nodes\u00a0for all phases of the lifecycle: from data ingestion and manipulation, through to model training, visualization, and deployment. Working with KNIME and Hadoop was a seamless experience, and everything was controllable from within the KNIME workbench. The combination of KNIME and Hadoop make it very easy to manage the large amount of data as well as the expected amounts of data in the future.\nThis Success Story is available here as a PDF.\n"
    },
    {
        "id": 89,
        "url": "https://www.knime.com/success-story/how-wurth-detects-environmental-hotspots-using-knime",
        "title": "How W\u00fcrth detects environmental hotspots using KNIME",
        "company": "W\u00fcrth Group",
        "content": "Objective: Develop products sustainably\nOne of the collective objectives is to actively shape the future and develop sustainably by following the concept of the circular economy. Specifically, one of their goals is to detect environmental hotspots in the screw product portfolio. W\u00fcrth has made steps in achieving this by transforming parts of their product portfolio to sustainable and circular products. Other objectives include:\nIncreasing data transparency of products with regards to carbon footprintStrengthening and expanding the role as a driver for sustainable innovationThe problem: Time-intensive, manual data blendingIdentifying environmental hotspots comes with several challenges and requires expertise from multiple domains such as Life Cycle Assessment (LCA), data engineering and analysis, as well as all technical fields involved along the value and supply chains of the analyzed products. On the product portfolio level, such analyses often involve millions of data points and multiple internal and external data sources.\nBecause of these challenges, W\u00fcrth worked with the Department of Life Cycle Engineering at the\u00a0Fraunhofer Institute for Building Physics IBP\u00a0\u2013\u00a0a research unit with more than 30 years\u2019 experience in applied LCA and sustainability analysis. Fraunhofer IBP built a reusable and scalable solution for W\u00fcrth\u2019s end users: sustainability professionals tasked with detecting environmental hotspots and ensuring circular products.\nFollowing the Sustainability Data Science Life Cycle (S-DSLC) approach, W\u00fcrth and Fraunhofer IBP started the project by inspecting one product category: Standardized metric screws. These make up one of the most relevant parts of the W\u00fcrth product portfolio. And with about 30,000 distinct products of the most common technical standards, it was a huge challenge to tackle.\nTo understand the environmental hotspots of screws, it's necessary to understand their data. Fraunhofer IBP sustainability experts had to prepare the various data for LCA. With millions of data points, it simply wasn\u2019t economically feasible to prepare data the conventional way.\nThey were also faced with a common but serious challenge in data preparation: Blending different data that have no natural link (i.e., no matching columns in the different data tables). This so-called \u201cmapping problem\u201d is common to any LCA project as corporate data on the analyzed product must be enriched with data from external LCA databases. For example, adding representative data for the carbon footprint of a sourced material to the bill of materials of the analyzed product. Such mapping lists are usually prepared manually involving experts from all required domains, which is a time-intensive process that must be automated to scale up LCA on the portfolio level.\n\nThe solution: Mapping algorithm for blending core data sources\nBased on the\u00a0S-DSLC\u00a0and the automation capabilities of KNIME Analytics Platform, Fraunhofer IBP developed a mapping algorithm that creates matching columns for blending the three core data sources:\nThe product list (bill of materials) with all product specifications provided by W\u00fcrth\u2019s ERP systemData on how the different screws are produced, stored in \u2018manufacturing modules\u2019 with quantified inputs and outputs for material provision, energy use, and other environmental dataLCA-background data from Fraunhofer IBP covering emissions and environmental impacts happening in the supply chains of the sourced materials and energyThe mapping algorithm was a decision tree implemented in KNIME Analytics Platform and constituted the heart of Fraunhofer IBP\u2019s modular and scalable solution to LCA of large product portfolios.\nAll core modules of the solution were implemented as components or metanodes in KNIME. This provided the flexibility to automate all processes: from loading, cleaning, and preparing all data, blending the product list with certain manufacturing models and LCA-background data, and defining the production route per screw\u00a0\u2013\u00a0through to performing all required LCA computations, visualizing results for interpretation by sustainability experts, and preparing the results for deployment at W\u00fcrth.\nResults: 24,000 CO2\u00a0footprints analyzed in under two minutes\nData science approaches and efficient tools for their implementation made all the difference in this project. Not only did this enable efficient understanding of the data and the data preparation process, but it also laid the groundwork for automation and continuous deployment of the developed solution.\nThe project took approximately six months to implement and involved stakeholders from various domains such as sustainability management, product management, manufacturing, cost engineering, strategic procurement and controlling, as well as upper management. This investment was worth it. With one click, 24,000 CO2\u00a0footprints could be analyzed in less than two minutes. This wouldn\u2019t have been feasible with traditional methods, as these are made for manually defining models and production plans. Without data science approaches and tools, this model would have taken years to build.\nThese results allowed identification of hotspots at both the product portfolio and process levels through high-quality estimates on the product level. The modeling also included ten typical production routes, each including a unique set of processing steps (aka modules). The deep insights generated include:\nContribution of screws from different production CO2 routes to the CO2\u00a0footprint of the overall portfolioMinimum and maximum CO2\u00a0footprint for key production countriesMedian CO2\u00a0footprint per production routeContribution from modules to the CO2\u00a0footprint per screwThe approach also identified gaps in data size and quality. These were identified either in the ERP database or in the relevant LCA data, and through this process were able to be repaired. This was identified as another project win.\nOutlook\nThe next step for W\u00fcrth is to transfer all results and insights into business practice, focusing on three main aspects:\nIncrease measurement and transparency regarding the environmental impact of production processes. Furthermore, identify factors such as production processes or locations to close the circle (of the circular economy). These can then be used as drivers for further transformation.Increase communication and product data transparency to fulfill either specific customer requirements, market requirements, or both.Improve sustainability data and constitute a first framework for evaluation.Why KNIME?There are several advantages to undertaking this approach with KNIME software. Primarily, the process is scalable through the automation of KNIME workflows. This enables W\u00fcrth and Fraunhofer IBP to remove manual work and enable effective collaboration as everyone can spend their time on more meaningful and creative tasks. Secondly, the model is easy to update due to the modular concept. Users can easily exchange modules, product list elements, and lines in the mapping algorithm without touching other parts of the model. Thirdly, because KNIME workflows are reusable and shareable, this approach can be transferred and applied to other products.\n\u21d2\u00a0Download this Success Story as a PDF here.\n"
    },
    {
        "id": 90,
        "url": "https://www.knime.com/success-story/how-wurth-detects-environmental-hotspots-using-knime",
        "title": "How W\u00fcrth detects environmental hotspots using KNIME",
        "company": "W\u00fcrth Group",
        "content": "Objective: Develop products sustainably\nOne of the collective objectives is to actively shape the future and develop sustainably by following the concept of the circular economy. Specifically, one of their goals is to detect environmental hotspots in the screw product portfolio. W\u00fcrth has made steps in achieving this by transforming parts of their product portfolio to sustainable and circular products. Other objectives include:\nIncreasing data transparency of products with regards to carbon footprintStrengthening and expanding the role as a driver for sustainable innovationThe problem: Time-intensive, manual data blendingIdentifying environmental hotspots comes with several challenges and requires expertise from multiple domains such as Life Cycle Assessment (LCA), data engineering and analysis, as well as all technical fields involved along the value and supply chains of the analyzed products. On the product portfolio level, such analyses often involve millions of data points and multiple internal and external data sources.\nBecause of these challenges, W\u00fcrth worked with the Department of Life Cycle Engineering at the\u00a0Fraunhofer Institute for Building Physics IBP\u00a0\u2013\u00a0a research unit with more than 30 years\u2019 experience in applied LCA and sustainability analysis. Fraunhofer IBP built a reusable and scalable solution for W\u00fcrth\u2019s end users: sustainability professionals tasked with detecting environmental hotspots and ensuring circular products.\nFollowing the Sustainability Data Science Life Cycle (S-DSLC) approach, W\u00fcrth and Fraunhofer IBP started the project by inspecting one product category: Standardized metric screws. These make up one of the most relevant parts of the W\u00fcrth product portfolio. And with about 30,000 distinct products of the most common technical standards, it was a huge challenge to tackle.\nTo understand the environmental hotspots of screws, it's necessary to understand their data. Fraunhofer IBP sustainability experts had to prepare the various data for LCA. With millions of data points, it simply wasn\u2019t economically feasible to prepare data the conventional way.\nThey were also faced with a common but serious challenge in data preparation: Blending different data that have no natural link (i.e., no matching columns in the different data tables). This so-called \u201cmapping problem\u201d is common to any LCA project as corporate data on the analyzed product must be enriched with data from external LCA databases. For example, adding representative data for the carbon footprint of a sourced material to the bill of materials of the analyzed product. Such mapping lists are usually prepared manually involving experts from all required domains, which is a time-intensive process that must be automated to scale up LCA on the portfolio level.\n\nThe solution: Mapping algorithm for blending core data sources\nBased on the\u00a0S-DSLC\u00a0and the automation capabilities of KNIME Analytics Platform, Fraunhofer IBP developed a mapping algorithm that creates matching columns for blending the three core data sources:\nThe product list (bill of materials) with all product specifications provided by W\u00fcrth\u2019s ERP systemData on how the different screws are produced, stored in \u2018manufacturing modules\u2019 with quantified inputs and outputs for material provision, energy use, and other environmental dataLCA-background data from Fraunhofer IBP covering emissions and environmental impacts happening in the supply chains of the sourced materials and energyThe mapping algorithm was a decision tree implemented in KNIME Analytics Platform and constituted the heart of Fraunhofer IBP\u2019s modular and scalable solution to LCA of large product portfolios.\nAll core modules of the solution were implemented as components or metanodes in KNIME. This provided the flexibility to automate all processes: from loading, cleaning, and preparing all data, blending the product list with certain manufacturing models and LCA-background data, and defining the production route per screw\u00a0\u2013\u00a0through to performing all required LCA computations, visualizing results for interpretation by sustainability experts, and preparing the results for deployment at W\u00fcrth.\nResults: 24,000 CO2\u00a0footprints analyzed in under two minutes\nData science approaches and efficient tools for their implementation made all the difference in this project. Not only did this enable efficient understanding of the data and the data preparation process, but it also laid the groundwork for automation and continuous deployment of the developed solution.\nThe project took approximately six months to implement and involved stakeholders from various domains such as sustainability management, product management, manufacturing, cost engineering, strategic procurement and controlling, as well as upper management. This investment was worth it. With one click, 24,000 CO2\u00a0footprints could be analyzed in less than two minutes. This wouldn\u2019t have been feasible with traditional methods, as these are made for manually defining models and production plans. Without data science approaches and tools, this model would have taken years to build.\nThese results allowed identification of hotspots at both the product portfolio and process levels through high-quality estimates on the product level. The modeling also included ten typical production routes, each including a unique set of processing steps (aka modules). The deep insights generated include:\nContribution of screws from different production CO2 routes to the CO2\u00a0footprint of the overall portfolioMinimum and maximum CO2\u00a0footprint for key production countriesMedian CO2\u00a0footprint per production routeContribution from modules to the CO2\u00a0footprint per screwThe approach also identified gaps in data size and quality. These were identified either in the ERP database or in the relevant LCA data, and through this process were able to be repaired. This was identified as another project win.\nOutlook\nThe next step for W\u00fcrth is to transfer all results and insights into business practice, focusing on three main aspects:\nIncrease measurement and transparency regarding the environmental impact of production processes. Furthermore, identify factors such as production processes or locations to close the circle (of the circular economy). These can then be used as drivers for further transformation.Increase communication and product data transparency to fulfill either specific customer requirements, market requirements, or both.Improve sustainability data and constitute a first framework for evaluation.Why KNIME?There are several advantages to undertaking this approach with KNIME software. Primarily, the process is scalable through the automation of KNIME workflows. This enables W\u00fcrth and Fraunhofer IBP to remove manual work and enable effective collaboration as everyone can spend their time on more meaningful and creative tasks. Secondly, the model is easy to update due to the modular concept. Users can easily exchange modules, product list elements, and lines in the mapping algorithm without touching other parts of the model. Thirdly, because KNIME workflows are reusable and shareable, this approach can be transferred and applied to other products.\n\u21d2\u00a0Download this Success Story as a PDF here.\n"
    },
    {
        "id": 91,
        "url": "https://www.knime.com/success-story/how-z5-inventory-prevents-medical-supply-waste-using-knime",
        "title": "How Z5 Inventory prevents medical supply waste using KNIME for predictive analytics",
        "company": "Z5 Inventory",
        "content": "Closing the never-ending waste cycle\nIn the US alone, $5 billion worth of medical supplies are thrown away each year. This epidemic of waste in health care facilities raises the cost of care and takes up space in landfills \u2013 both of which have significant social and environmental consequences. This waste can be attributed to poor inventory management. Without accurate inventory tracking and analysis, stock piles up and sits, unused, until it expires.\nThis project, run by Z5 Inventory, consisted of two phases. Phase one: clean out all the excess products (those not purchased recently or above the Periodic Automatic Replenishment levels) from 30 health care facilities and send them to a Z5-run warehouse. Phase two: use predictive analytics to reallocate these products to health care facilities in need.\nThe goal was to help clients reduce product overstock, gain shelf space for newer products, improve staff efficiency, reduce inventory supply costs, and reduce waste in the health care supply chain to as close to zero as possible. If this process were to be implemented in all US health care facilities, billions could be saved annually. This would ultimately save valuable taxpayer money, reduce landfill levels, and give at-risk communities supplies they might not have otherwise had access to.\n\nForecasting demand Using KNIME and R\nAfter determining the project objectives with key stakeholders (Z5 and client C-Suite), data scientists drafted out the components of the project (i.e. demand forecast, reallocation, best cycle selector, etc.), and how they should interact.\nThe first step was processing the data for analysis, checking for and handling errors such as missing values, invalid entries, and date range errors. Then, using the\u00a0KNIME Integration with R, data scientists could begin the demand forecasting (based on the purchase history of the health care facility), and create a list of products for each facility. R was used to split excess products into different destination facilities based on a ranking. This ranking was calculated using purchase frequency, average purchase quantity, standard deviation of purchase quantity, as well as the predicted quantity on the wish list. Additionally, since the goal was to reallocate products to health care facilities as much as possible, a \u201cbest cycle selector\u201d was created to ensure that for a given product, its use was maximized in the destination facility.\nThroughout the process, data scientists checked back in with key stakeholders, presenting interim results, and making necessary changes to the KNIME workflow. Robustness of models are continually evaluated based on customer feedback. Until now, there have been no complaints of over-shipment, which indicates that the model is successful in helping to solve the challenge of over-stocked health care facilities.\n\n\n$1.3 million in savings\nThis project was carried out in the Mid-Atlantic region of one of the largest health care networks in the US, where 32 health care facilities participated. USD 1.3 million dollars worth of medical and surgical product was moved to prevent expiration and disposal within the project execution. This is a phenomenal achievement. It indicates that, if all US health care providers employed a similar strategy, nearly all the USD 5 billion that goes to waste would actually be used.\nIdeally, by utilizing this process, the central warehouse that handles distribution to health care facilities will become perfectly efficient. To be precise, it would be empty. The current projection is being able to distribute all product within the following month. In a perfect world, waste in the health care supply chain would be reduced to zero. Realistically, given the number of uncontrollable variables in the industry such as clinician preference, supplier and manufacturer changes, and the increasing rate of industry consolidation, this isn\u2019t possible. However, what can be controlled is reducing products going to waste on shelves and being thrown away.\nWhy KNIME?\nKNIME nodes are visual and self-documenting, which saves a significant amount of time and makes results easier to understand for the non-coder. Sometimes, they can even reproduce the results without knowing how to code. Previously, when coding directly in\u00a0R\u00a0or\u00a0Python, a lot of time was spent on documentation to ensure the scripts were easy to read and results were reproducible. This is now time which can be spent modifying and improving the project processes.\nBecause the data scientists all come from a statistics background and code in R daily, they remain happy because KNIME gives them the flexibility to still work in R whilst remaining in one uniform platform. They can therefore take advantage of the best of both worlds.\u00a0KNIME Analytics Platform\u00a0is a great tool with many powerful capabilities such as comprehensive ETL nodes, an easy-to-use drag-and-drop interface, and integrations with many other popular data science and data mining tools. It simply makes doing analytics and pre-processing tasks simpler and faster.\n"
    },
    {
        "id": 92,
        "url": "https://www.knime.com/success-story/how-z5-inventory-prevents-medical-supply-waste-using-knime",
        "title": "How Z5 Inventory prevents medical supply waste using KNIME for predictive analytics",
        "company": "Z5 Inventory",
        "content": "Closing the never-ending waste cycle\nIn the US alone, $5 billion worth of medical supplies are thrown away each year. This epidemic of waste in health care facilities raises the cost of care and takes up space in landfills \u2013 both of which have significant social and environmental consequences. This waste can be attributed to poor inventory management. Without accurate inventory tracking and analysis, stock piles up and sits, unused, until it expires.\nThis project, run by Z5 Inventory, consisted of two phases. Phase one: clean out all the excess products (those not purchased recently or above the Periodic Automatic Replenishment levels) from 30 health care facilities and send them to a Z5-run warehouse. Phase two: use predictive analytics to reallocate these products to health care facilities in need.\nThe goal was to help clients reduce product overstock, gain shelf space for newer products, improve staff efficiency, reduce inventory supply costs, and reduce waste in the health care supply chain to as close to zero as possible. If this process were to be implemented in all US health care facilities, billions could be saved annually. This would ultimately save valuable taxpayer money, reduce landfill levels, and give at-risk communities supplies they might not have otherwise had access to.\n\nForecasting demand Using KNIME and R\nAfter determining the project objectives with key stakeholders (Z5 and client C-Suite), data scientists drafted out the components of the project (i.e. demand forecast, reallocation, best cycle selector, etc.), and how they should interact.\nThe first step was processing the data for analysis, checking for and handling errors such as missing values, invalid entries, and date range errors. Then, using the\u00a0KNIME Integration with R, data scientists could begin the demand forecasting (based on the purchase history of the health care facility), and create a list of products for each facility. R was used to split excess products into different destination facilities based on a ranking. This ranking was calculated using purchase frequency, average purchase quantity, standard deviation of purchase quantity, as well as the predicted quantity on the wish list. Additionally, since the goal was to reallocate products to health care facilities as much as possible, a \u201cbest cycle selector\u201d was created to ensure that for a given product, its use was maximized in the destination facility.\nThroughout the process, data scientists checked back in with key stakeholders, presenting interim results, and making necessary changes to the KNIME workflow. Robustness of models are continually evaluated based on customer feedback. Until now, there have been no complaints of over-shipment, which indicates that the model is successful in helping to solve the challenge of over-stocked health care facilities.\n\n\n$1.3 million in savings\nThis project was carried out in the Mid-Atlantic region of one of the largest health care networks in the US, where 32 health care facilities participated. USD 1.3 million dollars worth of medical and surgical product was moved to prevent expiration and disposal within the project execution. This is a phenomenal achievement. It indicates that, if all US health care providers employed a similar strategy, nearly all the USD 5 billion that goes to waste would actually be used.\nIdeally, by utilizing this process, the central warehouse that handles distribution to health care facilities will become perfectly efficient. To be precise, it would be empty. The current projection is being able to distribute all product within the following month. In a perfect world, waste in the health care supply chain would be reduced to zero. Realistically, given the number of uncontrollable variables in the industry such as clinician preference, supplier and manufacturer changes, and the increasing rate of industry consolidation, this isn\u2019t possible. However, what can be controlled is reducing products going to waste on shelves and being thrown away.\nWhy KNIME?\nKNIME nodes are visual and self-documenting, which saves a significant amount of time and makes results easier to understand for the non-coder. Sometimes, they can even reproduce the results without knowing how to code. Previously, when coding directly in\u00a0R\u00a0or\u00a0Python, a lot of time was spent on documentation to ensure the scripts were easy to read and results were reproducible. This is now time which can be spent modifying and improving the project processes.\nBecause the data scientists all come from a statistics background and code in R daily, they remain happy because KNIME gives them the flexibility to still work in R whilst remaining in one uniform platform. They can therefore take advantage of the best of both worlds.\u00a0KNIME Analytics Platform\u00a0is a great tool with many powerful capabilities such as comprehensive ETL nodes, an easy-to-use drag-and-drop interface, and integrations with many other popular data science and data mining tools. It simply makes doing analytics and pre-processing tasks simpler and faster.\n"
    },
    {
        "id": 93,
        "url": "https://www.knime.com/success-story/how-zf-group-improved-end-line-testing-using-knime",
        "title": "How the ZF Group improved end-of-line testing using KNIME | KNIME",
        "company": "ZF",
        "content": "The challenge of becoming data-driven\nThe last checkpoint before the product leaves the factory is end-of-line (EOL) testing. These final tests are key to increasing safety and reducing warranty claims, but typically cost manufacturers 4-6% of their revenue.\nZF is a global technology company supplying systems for passenger cars, commercial vehicles and industrial technology. They employ over 157,500 employees worldwide and in 2021 reported sales of \u20ac38.3 billion. They produce complex mechatronic and electronic products, such as transmissions or e-axles comprise multiple parts. For electronic parts 100% tests are required at three steps in the production value chain. The effect this amount of testing has on cost is huge.\nThe people behind the Test Systems Product Line at ZF are specialists for the validation and development of test facilities. Gert Jeckel, Senior Manager Data Analytics at ZF Test Systems, and his team identified the value of using advanced analytics to optimize EOL testing and gain significant savings in time and money.\nRedundant, expensive EOL testing\nElectronic component production is part of ZF\u2019s value chain. After an electronic board has been produced in the assembly line, it undergoes three 100% electrical checks while the product receives interfaces like plugs or housing. In-circuit testing checks all the internal electronic functions. Next the board is subjected to a run-in test, where it is exposed to extreme ambient temperatures: -40, +20, and +120 degree Celsius. After final assembly, which involves placing the final covering, plugs, and cables, the board undergoes its third 100% check.\nQuality engineers physically carry out 1000 measurements on each and every board. This is not only costly and time consuming, it also has a huge carbon footprint impact because of the energy needed for the extreme temperatures used in conducting these tests. The data analytics team\u2019s objective was to optimize the process by reducing the most expensive part of the process: physical testing.\n \n\n\nPhysical testing reduced by 95%\nLearning from the results of in-circuit testing, a machine learning model is now used to test the parts \u201cvirtually\u201d. The ML model predicts which parts have anomalies, i.e., are likely to fail, and sends only those parts on to physically undergo the stress tests at extreme temperatures. This solution has reduced physical testing by 95%.\nIntegration of the anomaly detection model to EOL testing also adds value to ZF\u2019s existing traceability data. The results of the anomaly detection model are visualized through a browser- based data app. Quality engineers can check the decisions the model has made and inject their expertise by adding comments, explaining, for example, why a particular decision was made.\nVirtual testing enables 7-digit savings\nThe new solution of performing virtual testing with KNIME enables significant savings in test equipment costs, manpower, and energy spend.\nTest Systems can now avoid physical product testing and no longer need to purchase physical testing equipment. They have achieved 7-digit savings on investments in stress-test equipment.\nThe extremely high energy costs of the stress tests, which are performed at -40\u00b0C, +20, and +120\u00b0C have been reduced dramatically. Test Systems expect annual savings in energy in the 7-digit range. This effort marks a milestone in ZF\u2019s path to achieving their environmental sustainability goals.\nWhy KNIME?\nWhen the data analytics team set out to reduce physical EOL testing, they knew they would need a data analytics tool that enables collaboration.Agile collaboration\nQuality engineers, IT, and data scientists easily inject their knowledge into the new solution, their involvement improving results and raising successive production quality.\n\u201cKNIME\u2019s visual programming environment is the perfect solution to enabling interdisciplinary teamwork,\u201d says Jeckel. \u201cThe analytics platform has enabled agile cooperation of electronic testing expertise, IT integration, and data science expertise to define proper data science solutions. This has been key in transforming existing processes.\u201dA scalable solution\nAfter the virtual testing system had proven successful in a single location, ZF wanted to be able to scale the solution to additional factories. Easy deployment of the solution via a browser- based data app means that the solution can be shared easily with multiple locations without infrastructure expertise.Easy Python integration\nKNIME\u2019s Python integration was strategically important to Test Systems\u2019 solution. The team uses Python to enhance their solution with custom-designed functionality, maximizing the customer experience. \u201cKNIME is a full system providing tons of lines of quality code. It really helps to know that you can concentrate on your business, but add any custom pieces through the Python integration.\u201d said Thomas Nithin, data scientist on the project team.Open for (more) integration\nExtensibility and flexibility of the analytics platform was a key requirement for ZF Test Systems. KNIME provides access to 300+ data sources and integrations to all relevant tools and environments, enabling ZF to provide future-proof solutions that customers can connect to their 2nd, or 3rd production line and even implement minor process changes by themselves.\n\u201cCustomers try to avoid being locked into a provider\u2019s ecosystem; KNIME\u2019s large partner network helps by reducing this risk,\u201d said Jeckel.\nAs a cooperative working platform for IT, data science, and business, the ability to deploy on-premise or in the cloud \u2013 all within the visual programming environment, Gert Jeckel confirmed that \u201cKNIME is a perfect fit for our internal and external customers. From prototyping right through to deployment, the end-to-end coverage enables consistent end- to-end work.\u201d\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    },
    {
        "id": 94,
        "url": "https://www.knime.com/success-story/how-zf-group-improved-end-line-testing-using-knime",
        "title": "How the ZF Group improved end-of-line testing using KNIME | KNIME",
        "company": "ZF",
        "content": "The challenge of becoming data-driven\nThe last checkpoint before the product leaves the factory is end-of-line (EOL) testing. These final tests are key to increasing safety and reducing warranty claims, but typically cost manufacturers 4-6% of their revenue.\nZF is a global technology company supplying systems for passenger cars, commercial vehicles and industrial technology. They employ over 157,500 employees worldwide and in 2021 reported sales of \u20ac38.3 billion. They produce complex mechatronic and electronic products, such as transmissions or e-axles comprise multiple parts. For electronic parts 100% tests are required at three steps in the production value chain. The effect this amount of testing has on cost is huge.\nThe people behind the Test Systems Product Line at ZF are specialists for the validation and development of test facilities. Gert Jeckel, Senior Manager Data Analytics at ZF Test Systems, and his team identified the value of using advanced analytics to optimize EOL testing and gain significant savings in time and money.\nRedundant, expensive EOL testing\nElectronic component production is part of ZF\u2019s value chain. After an electronic board has been produced in the assembly line, it undergoes three 100% electrical checks while the product receives interfaces like plugs or housing. In-circuit testing checks all the internal electronic functions. Next the board is subjected to a run-in test, where it is exposed to extreme ambient temperatures: -40, +20, and +120 degree Celsius. After final assembly, which involves placing the final covering, plugs, and cables, the board undergoes its third 100% check.\nQuality engineers physically carry out 1000 measurements on each and every board. This is not only costly and time consuming, it also has a huge carbon footprint impact because of the energy needed for the extreme temperatures used in conducting these tests. The data analytics team\u2019s objective was to optimize the process by reducing the most expensive part of the process: physical testing.\n \n\n\nPhysical testing reduced by 95%\nLearning from the results of in-circuit testing, a machine learning model is now used to test the parts \u201cvirtually\u201d. The ML model predicts which parts have anomalies, i.e., are likely to fail, and sends only those parts on to physically undergo the stress tests at extreme temperatures. This solution has reduced physical testing by 95%.\nIntegration of the anomaly detection model to EOL testing also adds value to ZF\u2019s existing traceability data. The results of the anomaly detection model are visualized through a browser- based data app. Quality engineers can check the decisions the model has made and inject their expertise by adding comments, explaining, for example, why a particular decision was made.\nVirtual testing enables 7-digit savings\nThe new solution of performing virtual testing with KNIME enables significant savings in test equipment costs, manpower, and energy spend.\nTest Systems can now avoid physical product testing and no longer need to purchase physical testing equipment. They have achieved 7-digit savings on investments in stress-test equipment.\nThe extremely high energy costs of the stress tests, which are performed at -40\u00b0C, +20, and +120\u00b0C have been reduced dramatically. Test Systems expect annual savings in energy in the 7-digit range. This effort marks a milestone in ZF\u2019s path to achieving their environmental sustainability goals.\nWhy KNIME?\nWhen the data analytics team set out to reduce physical EOL testing, they knew they would need a data analytics tool that enables collaboration.Agile collaboration\nQuality engineers, IT, and data scientists easily inject their knowledge into the new solution, their involvement improving results and raising successive production quality.\n\u201cKNIME\u2019s visual programming environment is the perfect solution to enabling interdisciplinary teamwork,\u201d says Jeckel. \u201cThe analytics platform has enabled agile cooperation of electronic testing expertise, IT integration, and data science expertise to define proper data science solutions. This has been key in transforming existing processes.\u201dA scalable solution\nAfter the virtual testing system had proven successful in a single location, ZF wanted to be able to scale the solution to additional factories. Easy deployment of the solution via a browser- based data app means that the solution can be shared easily with multiple locations without infrastructure expertise.Easy Python integration\nKNIME\u2019s Python integration was strategically important to Test Systems\u2019 solution. The team uses Python to enhance their solution with custom-designed functionality, maximizing the customer experience. \u201cKNIME is a full system providing tons of lines of quality code. It really helps to know that you can concentrate on your business, but add any custom pieces through the Python integration.\u201d said Thomas Nithin, data scientist on the project team.Open for (more) integration\nExtensibility and flexibility of the analytics platform was a key requirement for ZF Test Systems. KNIME provides access to 300+ data sources and integrations to all relevant tools and environments, enabling ZF to provide future-proof solutions that customers can connect to their 2nd, or 3rd production line and even implement minor process changes by themselves.\n\u201cCustomers try to avoid being locked into a provider\u2019s ecosystem; KNIME\u2019s large partner network helps by reducing this risk,\u201d said Jeckel.\nAs a cooperative working platform for IT, data science, and business, the ability to deploy on-premise or in the cloud \u2013 all within the visual programming environment, Gert Jeckel confirmed that \u201cKNIME is a perfect fit for our internal and external customers. From prototyping right through to deployment, the end-to-end coverage enables consistent end- to-end work.\u201d\nThis Success Story is available\u00a0here\u00a0as a PDF.\n"
    }
]